{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Izzet Emre Kucukkaya\n",
    "# CMPE597 HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dca4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a758e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a70276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "dataset = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "X_train = dataset[0][0]\n",
    "Y_train = dataset[0][1]\n",
    "X_test = dataset[1][0]\n",
    "Y_test = dataset[1][1]\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6914933",
   "metadata": {},
   "source": [
    "### Using Own Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c957e3",
   "metadata": {},
   "source": [
    "### Using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "923d432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchNetwork(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PytorchNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(PytorchNetwork, self).__init__()\n",
    "\n",
    "      self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n",
    "      self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "      self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=5, stride=1)\n",
    "      self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "      self.fc1 = nn.Linear(128, 128)\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "      x = self.conv1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.pool1(x)\n",
    "    \n",
    "      x = self.conv2(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.pool2(x)\n",
    "      \n",
    "      x = torch.flatten(x, 1)\n",
    "\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      output = F.log_softmax(x, dim=1)\n",
    "      return output\n",
    "\n",
    "model = PytorchNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86fe1630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 0.129008\n",
      "==>>> epoch: 0, batch index: 200, train loss: 0.109498\n",
      "==>>> epoch: 0, batch index: 300, train loss: 0.096006\n",
      "==>>> epoch: 0, batch index: 400, train loss: 0.072754\n",
      "==>>> epoch: 0, batch index: 500, train loss: 0.087649\n",
      "==>>> epoch: 0, batch index: 600, train loss: 0.098350\n",
      "==>>> epoch: 0, batch index: 100, test loss: 0.073826, acc: 0.975\n",
      "==>>> epoch: 1, batch index: 100, train loss: 0.091941\n",
      "==>>> epoch: 1, batch index: 200, train loss: 0.104930\n",
      "==>>> epoch: 1, batch index: 300, train loss: 0.093259\n",
      "==>>> epoch: 1, batch index: 400, train loss: 0.095452\n",
      "==>>> epoch: 1, batch index: 500, train loss: 0.100227\n",
      "==>>> epoch: 1, batch index: 600, train loss: 0.080403\n",
      "==>>> epoch: 1, batch index: 100, test loss: 0.070209, acc: 0.977\n",
      "==>>> epoch: 2, batch index: 100, train loss: 0.088849\n",
      "==>>> epoch: 2, batch index: 200, train loss: 0.072697\n",
      "==>>> epoch: 2, batch index: 300, train loss: 0.085524\n",
      "==>>> epoch: 2, batch index: 400, train loss: 0.112429\n",
      "==>>> epoch: 2, batch index: 500, train loss: 0.082953\n",
      "==>>> epoch: 2, batch index: 600, train loss: 0.101598\n",
      "==>>> epoch: 2, batch index: 100, test loss: 0.065635, acc: 0.978\n",
      "==>>> epoch: 3, batch index: 100, train loss: 0.077192\n",
      "==>>> epoch: 3, batch index: 200, train loss: 0.078729\n",
      "==>>> epoch: 3, batch index: 300, train loss: 0.080039\n",
      "==>>> epoch: 3, batch index: 400, train loss: 0.087928\n",
      "==>>> epoch: 3, batch index: 500, train loss: 0.074796\n",
      "==>>> epoch: 3, batch index: 600, train loss: 0.083700\n",
      "==>>> epoch: 3, batch index: 100, test loss: 0.069051, acc: 0.977\n",
      "==>>> epoch: 4, batch index: 100, train loss: 0.065359\n",
      "==>>> epoch: 4, batch index: 200, train loss: 0.081848\n",
      "==>>> epoch: 4, batch index: 300, train loss: 0.075520\n",
      "==>>> epoch: 4, batch index: 400, train loss: 0.067175\n",
      "==>>> epoch: 4, batch index: 500, train loss: 0.096925\n",
      "==>>> epoch: 4, batch index: 600, train loss: 0.064978\n",
      "==>>> epoch: 4, batch index: 100, test loss: 0.058177, acc: 0.980\n",
      "==>>> epoch: 5, batch index: 100, train loss: 0.089064\n",
      "==>>> epoch: 5, batch index: 200, train loss: 0.080548\n",
      "==>>> epoch: 5, batch index: 300, train loss: 0.081449\n",
      "==>>> epoch: 5, batch index: 400, train loss: 0.076526\n",
      "==>>> epoch: 5, batch index: 500, train loss: 0.062024\n",
      "==>>> epoch: 5, batch index: 600, train loss: 0.075650\n",
      "==>>> epoch: 5, batch index: 100, test loss: 0.057566, acc: 0.980\n",
      "==>>> epoch: 6, batch index: 100, train loss: 0.073265\n",
      "==>>> epoch: 6, batch index: 200, train loss: 0.074718\n",
      "==>>> epoch: 6, batch index: 300, train loss: 0.070189\n",
      "==>>> epoch: 6, batch index: 400, train loss: 0.080540\n",
      "==>>> epoch: 6, batch index: 500, train loss: 0.061678\n",
      "==>>> epoch: 6, batch index: 600, train loss: 0.081572\n",
      "==>>> epoch: 6, batch index: 100, test loss: 0.055998, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 100, train loss: 0.063632\n",
      "==>>> epoch: 7, batch index: 200, train loss: 0.068590\n",
      "==>>> epoch: 7, batch index: 300, train loss: 0.069287\n",
      "==>>> epoch: 7, batch index: 400, train loss: 0.064445\n",
      "==>>> epoch: 7, batch index: 500, train loss: 0.063108\n",
      "==>>> epoch: 7, batch index: 600, train loss: 0.069687\n",
      "==>>> epoch: 7, batch index: 100, test loss: 0.052661, acc: 0.982\n",
      "==>>> epoch: 8, batch index: 100, train loss: 0.063788\n",
      "==>>> epoch: 8, batch index: 200, train loss: 0.074677\n",
      "==>>> epoch: 8, batch index: 300, train loss: 0.066432\n",
      "==>>> epoch: 8, batch index: 400, train loss: 0.061477\n",
      "==>>> epoch: 8, batch index: 500, train loss: 0.066443\n",
      "==>>> epoch: 8, batch index: 600, train loss: 0.065124\n",
      "==>>> epoch: 8, batch index: 100, test loss: 0.052279, acc: 0.982\n",
      "==>>> epoch: 9, batch index: 100, train loss: 0.079960\n",
      "==>>> epoch: 9, batch index: 200, train loss: 0.067082\n",
      "==>>> epoch: 9, batch index: 300, train loss: 0.055960\n",
      "==>>> epoch: 9, batch index: 400, train loss: 0.063502\n",
      "==>>> epoch: 9, batch index: 500, train loss: 0.060853\n",
      "==>>> epoch: 9, batch index: 600, train loss: 0.060842\n",
      "==>>> epoch: 9, batch index: 100, test loss: 0.048014, acc: 0.980\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    # trainning\n",
    "    ave_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d9e36",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./Data/X_test.npy\")\n",
    "X_train = np.load(\"./Data/X_train.npy\")\n",
    "Y_test = np.load(\"./Data/Y_test.npy\")\n",
    "Y_train = np.load(\"./Data/Y_train.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
