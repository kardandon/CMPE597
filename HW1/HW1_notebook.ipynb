{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Izzet Emre Kucukkaya\n",
    "# CMPE597 HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dca4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a758e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d0a70276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4882, -0.4294,\n",
      "          -0.4294, -0.4294, -0.0059,  0.0333,  0.1863, -0.3980,  0.1510,\n",
      "           0.5000,  0.4686, -0.0020, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.3824, -0.3588, -0.1314,  0.1039,  0.1667,  0.4922,\n",
      "           0.4922,  0.4922,  0.4922,  0.4922,  0.3824,  0.1745,  0.4922,\n",
      "           0.4490,  0.2647, -0.2490, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.3078,  0.4333,  0.4922,  0.4922,  0.4922,  0.4922,  0.4922,\n",
      "           0.4922,  0.4922,  0.4922,  0.4843, -0.1353, -0.1784, -0.1784,\n",
      "          -0.2804, -0.3471, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.4294,  0.3588,  0.4922,  0.4922,  0.4922,  0.4922,  0.4922,\n",
      "           0.2765,  0.2137,  0.4686,  0.4451, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.1863,  0.1118, -0.0804,  0.4922,  0.4922,  0.3039,\n",
      "          -0.4569, -0.5000, -0.3314,  0.1039, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.4451, -0.4961,  0.1039,  0.4922, -0.1471,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000,  0.0451,  0.4922,  0.2451,\n",
      "          -0.4922, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.4569,  0.2451,  0.4922,\n",
      "          -0.2255, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.3627,  0.4451,\n",
      "           0.3824,  0.1275, -0.0765, -0.4961, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.1824,\n",
      "           0.4412,  0.4922,  0.4922, -0.0333, -0.4020, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.3235,  0.2294,  0.4922,  0.4922,  0.0882, -0.3941, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.4373, -0.1353,  0.4882,  0.4922,  0.2333, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000,  0.4765,  0.4922,  0.4765, -0.2490,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.3196,  0.0098,  0.2176,  0.4922,  0.4922,  0.3118, -0.4922,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.3471,  0.0804,\n",
      "           0.3980,  0.4922,  0.4922,  0.4922,  0.4804,  0.2137, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.4059, -0.0529,  0.3667,  0.4922,\n",
      "           0.4922,  0.4922,  0.4922,  0.2882, -0.1941, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.4098, -0.2412,  0.3353,  0.4922,  0.4922,  0.4922,\n",
      "           0.4922,  0.2765, -0.1824, -0.4922, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4294,\n",
      "           0.1706,  0.3588,  0.4922,  0.4922,  0.4922,  0.4922,  0.2647,\n",
      "          -0.1863, -0.4647, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.2843,  0.1745,  0.3863,\n",
      "           0.4922,  0.4922,  0.4922,  0.4922,  0.4569,  0.0216, -0.4569,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000,  0.0333,  0.4922,  0.4922,\n",
      "           0.4922,  0.3314,  0.0294,  0.0176, -0.4373, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000]]]), 5)\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "dataset = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "X_train = dataset[0][0]\n",
    "Y_train = dataset[0][1]\n",
    "X_test = dataset[1][0]\n",
    "Y_test = dataset[1][1]\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "print(train_set[0])\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6914933",
   "metadata": {},
   "source": [
    "### Using Own Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c957e3",
   "metadata": {},
   "source": [
    "### Using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "923d432e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchNetwork(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PytorchNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(PytorchNetwork, self).__init__()\n",
    "\n",
    "      self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n",
    "      self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "      self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=5, stride=1)\n",
    "      self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "      self.fc1 = nn.Linear(128, 128)\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "      x = self.conv1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.pool1(x)\n",
    "    \n",
    "      x = self.conv2(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.pool2(x)\n",
    "      \n",
    "      x = torch.flatten(x, 1)\n",
    "\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      output = F.log_softmax(x, dim=1)\n",
    "      return output\n",
    "\n",
    "model = PytorchNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "86fe1630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 230.522127\n",
      "==>>> epoch: 0, batch index: 200, train loss: 460.497815\n",
      "==>>> epoch: 0, batch index: 300, train loss: 689.874042\n",
      "==>>> epoch: 0, batch index: 400, train loss: 918.725889\n",
      "==>>> epoch: 0, batch index: 500, train loss: 1146.534586\n",
      "==>>> epoch: 0, batch index: 600, train loss: 1372.758989\n",
      "==>>> epoch: 0, batch index: 100, test loss: 224.866524, acc: 0.321\n",
      "==>>> epoch: 1, batch index: 100, train loss: 222.851202\n",
      "==>>> epoch: 1, batch index: 200, train loss: 436.715191\n",
      "==>>> epoch: 1, batch index: 300, train loss: 620.101203\n",
      "==>>> epoch: 1, batch index: 400, train loss: 733.676869\n",
      "==>>> epoch: 1, batch index: 500, train loss: 802.265101\n",
      "==>>> epoch: 1, batch index: 600, train loss: 854.414838\n",
      "==>>> epoch: 1, batch index: 100, test loss: 43.777157, acc: 0.882\n",
      "==>>> epoch: 2, batch index: 100, train loss: 43.183321\n",
      "==>>> epoch: 2, batch index: 200, train loss: 83.051962\n",
      "==>>> epoch: 2, batch index: 300, train loss: 118.775426\n",
      "==>>> epoch: 2, batch index: 400, train loss: 152.424040\n",
      "==>>> epoch: 2, batch index: 500, train loss: 183.798923\n",
      "==>>> epoch: 2, batch index: 600, train loss: 212.175075\n",
      "==>>> epoch: 2, batch index: 100, test loss: 25.368330, acc: 0.923\n",
      "==>>> epoch: 3, batch index: 100, train loss: 28.559030\n",
      "==>>> epoch: 3, batch index: 200, train loss: 54.422868\n",
      "==>>> epoch: 3, batch index: 300, train loss: 80.617385\n",
      "==>>> epoch: 3, batch index: 400, train loss: 105.926318\n",
      "==>>> epoch: 3, batch index: 500, train loss: 129.476122\n",
      "==>>> epoch: 3, batch index: 600, train loss: 151.968807\n",
      "==>>> epoch: 3, batch index: 100, test loss: 20.516412, acc: 0.937\n",
      "==>>> epoch: 4, batch index: 100, train loss: 21.682435\n",
      "==>>> epoch: 4, batch index: 200, train loss: 44.065329\n",
      "==>>> epoch: 4, batch index: 300, train loss: 65.404517\n",
      "==>>> epoch: 4, batch index: 400, train loss: 85.579980\n",
      "==>>> epoch: 4, batch index: 500, train loss: 106.510151\n",
      "==>>> epoch: 4, batch index: 600, train loss: 125.337830\n",
      "==>>> epoch: 4, batch index: 100, test loss: 16.903480, acc: 0.948\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    # trainning\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, total_loss))\n",
    "    # testing\n",
    "    correct_cnt = 0\n",
    "    total_cnt = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        total_loss += loss.item()\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, total_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d9e36",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "6059ebc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d11e66fcd0>]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = np.asarray(np.load(\"./Data/X_test.npy\"), dtype=np.float32)\n",
    "X_train = np.asarray(np.load(\"./Data/X_train.npy\"), dtype=np.float32)\n",
    "Y_test = np.asarray(np.load(\"./Data/Y_test.npy\"))\n",
    "Y_train = np.asarray(np.load(\"./Data/Y_train.npy\"))\n",
    "plt.plot(X_test[Y_test == 0, 0], X_test[Y_test == 0, 1], \"*r\")\n",
    "plt.plot(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], \"*b\")\n",
    "plt.plot(X_train[Y_train == 0, 0], X_train[Y_train == 0, 1], \"or\")\n",
    "plt.plot(X_train[Y_train == 1, 0], X_train[Y_train == 1, 1], \"ob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d902ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-1.1891905,  1.0173422], dtype=float32), 0), (array([1.8614649, 1.2298379], dtype=float32), 0), (array([0.440304 , 0.9436088], dtype=float32), 1), (array([ 0.7329011, -1.3278403], dtype=float32), 0), (array([-0.9693477 , -0.72879255], dtype=float32), 1), (array([-1.0259897, -1.2520034], dtype=float32), 0), (array([ 0.83688706, -0.54252464], dtype=float32), 1), (array([-1.949864 , -1.1116453], dtype=float32), 0), (array([ 0.42107823, -0.9507564 ], dtype=float32), 1), (array([-0.6471025 , -0.13248588], dtype=float32), 1), (array([-0.80688876, -0.5903789 ], dtype=float32), 1), (array([-0.17615908,  1.3725386 ], dtype=float32), 1), (array([-0.07870086,  0.75478023], dtype=float32), 1), (array([ 0.37287754, -0.656734  ], dtype=float32), 1), (array([1.9841359 , 0.11500192], dtype=float32), 0), (array([ 1.2425642, -0.3106096], dtype=float32), 1), (array([-0.22543794,  1.3778895 ], dtype=float32), 0), (array([-0.07644515,  0.37673712], dtype=float32), 1), (array([0.91728514, 0.94192255], dtype=float32), 1), (array([ 1.4185266 , -0.24612582], dtype=float32), 1), (array([ 1.6738425 , -0.04001506], dtype=float32), 0), (array([-0.5703266,  0.9027618], dtype=float32), 1), (array([ 0.32910967, -0.8848952 ], dtype=float32), 1), (array([-0.62365556,  0.05975249], dtype=float32), 1), (array([ 0.04200134, -1.1378378 ], dtype=float32), 1), (array([-0.540831 , -1.4558374], dtype=float32), 0), (array([-0.05018494, -0.7222681 ], dtype=float32), 1), (array([0.23855336, 1.6462349 ], dtype=float32), 0), (array([-0.2502601,  0.8024203], dtype=float32), 1), (array([0.30984256, 0.4717467 ], dtype=float32), 1), (array([ 0.29067776, -1.5711447 ], dtype=float32), 0), (array([ 0.13336253, -0.8985866 ], dtype=float32), 1), (array([0.9213388, 0.5940241], dtype=float32), 1), (array([ 0.9045283, -1.2785239], dtype=float32), 0), (array([-0.76288056, -0.07023891], dtype=float32), 1), (array([-0.7807143,  1.322182 ], dtype=float32), 0), (array([0.925639 , 1.7782485], dtype=float32), 0), (array([ 0.45065784, -0.97869027], dtype=float32), 1), (array([ 0.5891893, -1.8301537], dtype=float32), 0), (array([-1.8506076, -1.322199 ], dtype=float32), 0)]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=[(x,y) for x, y in zip(X_train, Y_train)],\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=[(x,y) for x, y in zip(X_test, Y_test)],\n",
    "                batch_size=len(Y_test),\n",
    "                shuffle=False)\n",
    "print([(x,y) for x, y in zip(X_test, Y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b7b0810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchNetworkQ2(\n",
      "  (fc1): Linear(in_features=2, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PytorchNetworkQ2(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(PytorchNetworkQ2, self).__init__()\n",
    "    \n",
    "      self.fc1 = nn.Linear(2, 8)\n",
    "      self.fc2 = nn.Linear(8, 8)\n",
    "      self.fc3 = nn.Linear(8, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc2(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc3(x)\n",
    "        \n",
    "      return torch.sigmoid(x)\n",
    "\n",
    "model2 = PytorchNetworkQ2()\n",
    "print(model2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "585102f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 6, train loss: 4.208117\n",
      "==>>> epoch: 0, batch index: 1, test loss: 0.690713, acc: 0.650\n",
      "==>>> epoch: 1, batch index: 6, train loss: 4.183156\n",
      "==>>> epoch: 1, batch index: 1, test loss: 0.692096, acc: 0.525\n",
      "==>>> epoch: 2, batch index: 6, train loss: 4.158533\n",
      "==>>> epoch: 2, batch index: 1, test loss: 0.693545, acc: 0.475\n",
      "==>>> epoch: 3, batch index: 6, train loss: 4.137401\n",
      "==>>> epoch: 3, batch index: 1, test loss: 0.694924, acc: 0.425\n",
      "==>>> epoch: 4, batch index: 6, train loss: 4.115012\n",
      "==>>> epoch: 4, batch index: 1, test loss: 0.696307, acc: 0.400\n",
      "==>>> epoch: 5, batch index: 6, train loss: 4.098738\n",
      "==>>> epoch: 5, batch index: 1, test loss: 0.697612, acc: 0.400\n",
      "==>>> epoch: 6, batch index: 6, train loss: 4.081828\n",
      "==>>> epoch: 6, batch index: 1, test loss: 0.698789, acc: 0.400\n",
      "==>>> epoch: 7, batch index: 6, train loss: 4.069247\n",
      "==>>> epoch: 7, batch index: 1, test loss: 0.699705, acc: 0.400\n",
      "==>>> epoch: 8, batch index: 6, train loss: 4.052100\n",
      "==>>> epoch: 8, batch index: 1, test loss: 0.700584, acc: 0.400\n",
      "==>>> epoch: 9, batch index: 6, train loss: 4.041756\n",
      "==>>> epoch: 9, batch index: 1, test loss: 0.701174, acc: 0.400\n",
      "==>>> epoch: 10, batch index: 6, train loss: 4.031125\n",
      "==>>> epoch: 10, batch index: 1, test loss: 0.701430, acc: 0.400\n",
      "==>>> epoch: 11, batch index: 6, train loss: 4.014823\n",
      "==>>> epoch: 11, batch index: 1, test loss: 0.701449, acc: 0.400\n",
      "==>>> epoch: 12, batch index: 6, train loss: 3.998304\n",
      "==>>> epoch: 12, batch index: 1, test loss: 0.701150, acc: 0.400\n",
      "==>>> epoch: 13, batch index: 6, train loss: 3.986378\n",
      "==>>> epoch: 13, batch index: 1, test loss: 0.700588, acc: 0.400\n",
      "==>>> epoch: 14, batch index: 6, train loss: 3.971588\n",
      "==>>> epoch: 14, batch index: 1, test loss: 0.699963, acc: 0.400\n",
      "==>>> epoch: 15, batch index: 6, train loss: 3.955735\n",
      "==>>> epoch: 15, batch index: 1, test loss: 0.699322, acc: 0.400\n",
      "==>>> epoch: 16, batch index: 6, train loss: 3.937916\n",
      "==>>> epoch: 16, batch index: 1, test loss: 0.697760, acc: 0.400\n",
      "==>>> epoch: 17, batch index: 6, train loss: 3.919969\n",
      "==>>> epoch: 17, batch index: 1, test loss: 0.696811, acc: 0.400\n",
      "==>>> epoch: 18, batch index: 6, train loss: 3.901895\n",
      "==>>> epoch: 18, batch index: 1, test loss: 0.695191, acc: 0.400\n",
      "==>>> epoch: 19, batch index: 6, train loss: 3.877387\n",
      "==>>> epoch: 19, batch index: 1, test loss: 0.693219, acc: 0.400\n",
      "==>>> epoch: 20, batch index: 6, train loss: 3.863228\n",
      "==>>> epoch: 20, batch index: 1, test loss: 0.691256, acc: 0.400\n",
      "==>>> epoch: 21, batch index: 6, train loss: 3.838796\n",
      "==>>> epoch: 21, batch index: 1, test loss: 0.688063, acc: 0.400\n",
      "==>>> epoch: 22, batch index: 6, train loss: 3.818837\n",
      "==>>> epoch: 22, batch index: 1, test loss: 0.686236, acc: 0.400\n",
      "==>>> epoch: 23, batch index: 6, train loss: 3.799106\n",
      "==>>> epoch: 23, batch index: 1, test loss: 0.684318, acc: 0.400\n",
      "==>>> epoch: 24, batch index: 6, train loss: 3.786491\n",
      "==>>> epoch: 24, batch index: 1, test loss: 0.681482, acc: 0.400\n",
      "==>>> epoch: 25, batch index: 6, train loss: 3.752560\n",
      "==>>> epoch: 25, batch index: 1, test loss: 0.678630, acc: 0.450\n",
      "==>>> epoch: 26, batch index: 6, train loss: 3.724707\n",
      "==>>> epoch: 26, batch index: 1, test loss: 0.675475, acc: 0.525\n",
      "==>>> epoch: 27, batch index: 6, train loss: 3.703704\n",
      "==>>> epoch: 27, batch index: 1, test loss: 0.670482, acc: 0.600\n",
      "==>>> epoch: 28, batch index: 6, train loss: 3.676641\n",
      "==>>> epoch: 28, batch index: 1, test loss: 0.667248, acc: 0.625\n",
      "==>>> epoch: 29, batch index: 6, train loss: 3.673944\n",
      "==>>> epoch: 29, batch index: 1, test loss: 0.661479, acc: 0.650\n",
      "==>>> epoch: 30, batch index: 6, train loss: 3.635694\n",
      "==>>> epoch: 30, batch index: 1, test loss: 0.658251, acc: 0.675\n",
      "==>>> epoch: 31, batch index: 6, train loss: 3.606948\n",
      "==>>> epoch: 31, batch index: 1, test loss: 0.653857, acc: 0.675\n",
      "==>>> epoch: 32, batch index: 6, train loss: 3.585021\n",
      "==>>> epoch: 32, batch index: 1, test loss: 0.647170, acc: 0.700\n",
      "==>>> epoch: 33, batch index: 6, train loss: 3.548795\n",
      "==>>> epoch: 33, batch index: 1, test loss: 0.643347, acc: 0.675\n",
      "==>>> epoch: 34, batch index: 6, train loss: 3.520857\n",
      "==>>> epoch: 34, batch index: 1, test loss: 0.639933, acc: 0.700\n",
      "==>>> epoch: 35, batch index: 6, train loss: 3.489820\n",
      "==>>> epoch: 35, batch index: 1, test loss: 0.636237, acc: 0.725\n",
      "==>>> epoch: 36, batch index: 6, train loss: 3.466130\n",
      "==>>> epoch: 36, batch index: 1, test loss: 0.630942, acc: 0.750\n",
      "==>>> epoch: 37, batch index: 6, train loss: 3.439821\n",
      "==>>> epoch: 37, batch index: 1, test loss: 0.622051, acc: 0.750\n",
      "==>>> epoch: 38, batch index: 6, train loss: 3.405900\n",
      "==>>> epoch: 38, batch index: 1, test loss: 0.614069, acc: 0.750\n",
      "==>>> epoch: 39, batch index: 6, train loss: 3.370357\n",
      "==>>> epoch: 39, batch index: 1, test loss: 0.607537, acc: 0.750\n",
      "==>>> epoch: 40, batch index: 6, train loss: 3.320695\n",
      "==>>> epoch: 40, batch index: 1, test loss: 0.603451, acc: 0.800\n",
      "==>>> epoch: 41, batch index: 6, train loss: 3.294727\n",
      "==>>> epoch: 41, batch index: 1, test loss: 0.596874, acc: 0.800\n",
      "==>>> epoch: 42, batch index: 6, train loss: 3.247047\n",
      "==>>> epoch: 42, batch index: 1, test loss: 0.588173, acc: 0.800\n",
      "==>>> epoch: 43, batch index: 6, train loss: 3.211334\n",
      "==>>> epoch: 43, batch index: 1, test loss: 0.578961, acc: 0.825\n",
      "==>>> epoch: 44, batch index: 6, train loss: 3.193984\n",
      "==>>> epoch: 44, batch index: 1, test loss: 0.572878, acc: 0.800\n",
      "==>>> epoch: 45, batch index: 6, train loss: 3.139313\n",
      "==>>> epoch: 45, batch index: 1, test loss: 0.568060, acc: 0.800\n",
      "==>>> epoch: 46, batch index: 6, train loss: 3.123723\n",
      "==>>> epoch: 46, batch index: 1, test loss: 0.559206, acc: 0.825\n",
      "==>>> epoch: 47, batch index: 6, train loss: 3.063194\n",
      "==>>> epoch: 47, batch index: 1, test loss: 0.561057, acc: 0.800\n",
      "==>>> epoch: 48, batch index: 6, train loss: 3.013380\n",
      "==>>> epoch: 48, batch index: 1, test loss: 0.548813, acc: 0.825\n",
      "==>>> epoch: 49, batch index: 6, train loss: 2.992054\n",
      "==>>> epoch: 49, batch index: 1, test loss: 0.540793, acc: 0.900\n",
      "==>>> epoch: 50, batch index: 6, train loss: 2.975209\n",
      "==>>> epoch: 50, batch index: 1, test loss: 0.535413, acc: 0.900\n",
      "==>>> epoch: 51, batch index: 6, train loss: 2.947964\n",
      "==>>> epoch: 51, batch index: 1, test loss: 0.534563, acc: 0.850\n",
      "==>>> epoch: 52, batch index: 6, train loss: 2.916012\n",
      "==>>> epoch: 52, batch index: 1, test loss: 0.528463, acc: 0.875\n",
      "==>>> epoch: 53, batch index: 6, train loss: 2.916820\n",
      "==>>> epoch: 53, batch index: 1, test loss: 0.524762, acc: 0.875\n",
      "==>>> epoch: 54, batch index: 6, train loss: 2.850448\n",
      "==>>> epoch: 54, batch index: 1, test loss: 0.515556, acc: 0.875\n",
      "==>>> epoch: 55, batch index: 6, train loss: 2.868966\n",
      "==>>> epoch: 55, batch index: 1, test loss: 0.523915, acc: 0.825\n",
      "==>>> epoch: 56, batch index: 6, train loss: 2.818610\n",
      "==>>> epoch: 56, batch index: 1, test loss: 0.513185, acc: 0.875\n",
      "==>>> epoch: 57, batch index: 6, train loss: 2.788396\n",
      "==>>> epoch: 57, batch index: 1, test loss: 0.508516, acc: 0.875\n",
      "==>>> epoch: 58, batch index: 6, train loss: 2.770591\n",
      "==>>> epoch: 58, batch index: 1, test loss: 0.512091, acc: 0.850\n",
      "==>>> epoch: 59, batch index: 6, train loss: 2.760235\n",
      "==>>> epoch: 59, batch index: 1, test loss: 0.508671, acc: 0.825\n",
      "==>>> epoch: 60, batch index: 6, train loss: 2.753102\n",
      "==>>> epoch: 60, batch index: 1, test loss: 0.502293, acc: 0.875\n",
      "==>>> epoch: 61, batch index: 6, train loss: 2.750177\n",
      "==>>> epoch: 61, batch index: 1, test loss: 0.496101, acc: 0.875\n",
      "==>>> epoch: 62, batch index: 6, train loss: 2.715967\n",
      "==>>> epoch: 62, batch index: 1, test loss: 0.492330, acc: 0.875\n",
      "==>>> epoch: 63, batch index: 6, train loss: 2.704301\n",
      "==>>> epoch: 63, batch index: 1, test loss: 0.494514, acc: 0.875\n",
      "==>>> epoch: 64, batch index: 6, train loss: 2.672266\n",
      "==>>> epoch: 64, batch index: 1, test loss: 0.490674, acc: 0.875\n",
      "==>>> epoch: 65, batch index: 6, train loss: 2.687192\n",
      "==>>> epoch: 65, batch index: 1, test loss: 0.485221, acc: 0.875\n",
      "==>>> epoch: 66, batch index: 6, train loss: 2.676458\n",
      "==>>> epoch: 66, batch index: 1, test loss: 0.485502, acc: 0.875\n",
      "==>>> epoch: 67, batch index: 6, train loss: 2.625071\n",
      "==>>> epoch: 67, batch index: 1, test loss: 0.481777, acc: 0.875\n",
      "==>>> epoch: 68, batch index: 6, train loss: 2.630583\n",
      "==>>> epoch: 68, batch index: 1, test loss: 0.486228, acc: 0.875\n",
      "==>>> epoch: 69, batch index: 6, train loss: 2.604307\n",
      "==>>> epoch: 69, batch index: 1, test loss: 0.483062, acc: 0.875\n",
      "==>>> epoch: 70, batch index: 6, train loss: 2.609644\n",
      "==>>> epoch: 70, batch index: 1, test loss: 0.485738, acc: 0.850\n",
      "==>>> epoch: 71, batch index: 6, train loss: 2.620775\n",
      "==>>> epoch: 71, batch index: 1, test loss: 0.479021, acc: 0.875\n",
      "==>>> epoch: 72, batch index: 6, train loss: 2.597156\n",
      "==>>> epoch: 72, batch index: 1, test loss: 0.478137, acc: 0.875\n",
      "==>>> epoch: 73, batch index: 6, train loss: 2.614167\n",
      "==>>> epoch: 73, batch index: 1, test loss: 0.481242, acc: 0.875\n",
      "==>>> epoch: 74, batch index: 6, train loss: 2.560158\n",
      "==>>> epoch: 74, batch index: 1, test loss: 0.482532, acc: 0.875\n",
      "==>>> epoch: 75, batch index: 6, train loss: 2.559733\n",
      "==>>> epoch: 75, batch index: 1, test loss: 0.479346, acc: 0.875\n",
      "==>>> epoch: 76, batch index: 6, train loss: 2.574249\n",
      "==>>> epoch: 76, batch index: 1, test loss: 0.477259, acc: 0.875\n",
      "==>>> epoch: 77, batch index: 6, train loss: 2.541173\n",
      "==>>> epoch: 77, batch index: 1, test loss: 0.480049, acc: 0.875\n",
      "==>>> epoch: 78, batch index: 6, train loss: 2.533872\n",
      "==>>> epoch: 78, batch index: 1, test loss: 0.480796, acc: 0.850\n",
      "==>>> epoch: 79, batch index: 6, train loss: 2.519550\n",
      "==>>> epoch: 79, batch index: 1, test loss: 0.473785, acc: 0.875\n",
      "==>>> epoch: 80, batch index: 6, train loss: 2.508523\n",
      "==>>> epoch: 80, batch index: 1, test loss: 0.470478, acc: 0.900\n",
      "==>>> epoch: 81, batch index: 6, train loss: 2.547458\n",
      "==>>> epoch: 81, batch index: 1, test loss: 0.476378, acc: 0.875\n",
      "==>>> epoch: 82, batch index: 6, train loss: 2.518964\n",
      "==>>> epoch: 82, batch index: 1, test loss: 0.468820, acc: 0.875\n",
      "==>>> epoch: 83, batch index: 6, train loss: 2.526523\n",
      "==>>> epoch: 83, batch index: 1, test loss: 0.469340, acc: 0.875\n",
      "==>>> epoch: 84, batch index: 6, train loss: 2.498254\n",
      "==>>> epoch: 84, batch index: 1, test loss: 0.466543, acc: 0.900\n",
      "==>>> epoch: 85, batch index: 6, train loss: 2.521024\n",
      "==>>> epoch: 85, batch index: 1, test loss: 0.467900, acc: 0.850\n",
      "==>>> epoch: 86, batch index: 6, train loss: 2.501601\n",
      "==>>> epoch: 86, batch index: 1, test loss: 0.460552, acc: 0.900\n",
      "==>>> epoch: 87, batch index: 6, train loss: 2.496204\n",
      "==>>> epoch: 87, batch index: 1, test loss: 0.463386, acc: 0.900\n",
      "==>>> epoch: 88, batch index: 6, train loss: 2.466940\n",
      "==>>> epoch: 88, batch index: 1, test loss: 0.465506, acc: 0.875\n",
      "==>>> epoch: 89, batch index: 6, train loss: 2.511537\n",
      "==>>> epoch: 89, batch index: 1, test loss: 0.483038, acc: 0.825\n",
      "==>>> epoch: 90, batch index: 6, train loss: 2.481157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 90, batch index: 1, test loss: 0.470429, acc: 0.875\n",
      "==>>> epoch: 91, batch index: 6, train loss: 2.463511\n",
      "==>>> epoch: 91, batch index: 1, test loss: 0.464017, acc: 0.875\n",
      "==>>> epoch: 92, batch index: 6, train loss: 2.490494\n",
      "==>>> epoch: 92, batch index: 1, test loss: 0.463359, acc: 0.875\n",
      "==>>> epoch: 93, batch index: 6, train loss: 2.503933\n",
      "==>>> epoch: 93, batch index: 1, test loss: 0.459314, acc: 0.900\n",
      "==>>> epoch: 94, batch index: 6, train loss: 2.462811\n",
      "==>>> epoch: 94, batch index: 1, test loss: 0.466858, acc: 0.850\n",
      "==>>> epoch: 95, batch index: 6, train loss: 2.500282\n",
      "==>>> epoch: 95, batch index: 1, test loss: 0.474563, acc: 0.850\n",
      "==>>> epoch: 96, batch index: 6, train loss: 2.489934\n",
      "==>>> epoch: 96, batch index: 1, test loss: 0.465425, acc: 0.850\n",
      "==>>> epoch: 97, batch index: 6, train loss: 2.509119\n",
      "==>>> epoch: 97, batch index: 1, test loss: 0.468828, acc: 0.850\n",
      "==>>> epoch: 98, batch index: 6, train loss: 2.454793\n",
      "==>>> epoch: 98, batch index: 1, test loss: 0.471953, acc: 0.850\n",
      "==>>> epoch: 99, batch index: 6, train loss: 2.445278\n",
      "==>>> epoch: 99, batch index: 1, test loss: 0.473237, acc: 0.875\n",
      "==>>> epoch: 100, batch index: 6, train loss: 2.428327\n",
      "==>>> epoch: 100, batch index: 1, test loss: 0.466195, acc: 0.850\n",
      "==>>> epoch: 101, batch index: 6, train loss: 2.475633\n",
      "==>>> epoch: 101, batch index: 1, test loss: 0.462752, acc: 0.875\n",
      "==>>> epoch: 102, batch index: 6, train loss: 2.415955\n",
      "==>>> epoch: 102, batch index: 1, test loss: 0.454733, acc: 0.900\n",
      "==>>> epoch: 103, batch index: 6, train loss: 2.440023\n",
      "==>>> epoch: 103, batch index: 1, test loss: 0.460394, acc: 0.900\n",
      "==>>> epoch: 104, batch index: 6, train loss: 2.436565\n",
      "==>>> epoch: 104, batch index: 1, test loss: 0.465216, acc: 0.850\n",
      "==>>> epoch: 105, batch index: 6, train loss: 2.415977\n",
      "==>>> epoch: 105, batch index: 1, test loss: 0.459890, acc: 0.900\n",
      "==>>> epoch: 106, batch index: 6, train loss: 2.431749\n",
      "==>>> epoch: 106, batch index: 1, test loss: 0.457609, acc: 0.875\n",
      "==>>> epoch: 107, batch index: 6, train loss: 2.429863\n",
      "==>>> epoch: 107, batch index: 1, test loss: 0.464610, acc: 0.875\n",
      "==>>> epoch: 108, batch index: 6, train loss: 2.400169\n",
      "==>>> epoch: 108, batch index: 1, test loss: 0.471036, acc: 0.875\n",
      "==>>> epoch: 109, batch index: 6, train loss: 2.401371\n",
      "==>>> epoch: 109, batch index: 1, test loss: 0.465743, acc: 0.875\n",
      "==>>> epoch: 110, batch index: 6, train loss: 2.416470\n",
      "==>>> epoch: 110, batch index: 1, test loss: 0.478387, acc: 0.850\n",
      "==>>> epoch: 111, batch index: 6, train loss: 2.411646\n",
      "==>>> epoch: 111, batch index: 1, test loss: 0.464666, acc: 0.850\n",
      "==>>> epoch: 112, batch index: 6, train loss: 2.450951\n",
      "==>>> epoch: 112, batch index: 1, test loss: 0.462730, acc: 0.825\n",
      "==>>> epoch: 113, batch index: 6, train loss: 2.415705\n",
      "==>>> epoch: 113, batch index: 1, test loss: 0.474021, acc: 0.850\n",
      "==>>> epoch: 114, batch index: 6, train loss: 2.402284\n",
      "==>>> epoch: 114, batch index: 1, test loss: 0.470156, acc: 0.850\n",
      "==>>> epoch: 115, batch index: 6, train loss: 2.399916\n",
      "==>>> epoch: 115, batch index: 1, test loss: 0.475483, acc: 0.850\n",
      "==>>> epoch: 116, batch index: 6, train loss: 2.399924\n",
      "==>>> epoch: 116, batch index: 1, test loss: 0.458559, acc: 0.875\n",
      "==>>> epoch: 117, batch index: 6, train loss: 2.422813\n",
      "==>>> epoch: 117, batch index: 1, test loss: 0.461415, acc: 0.850\n",
      "==>>> epoch: 118, batch index: 6, train loss: 2.397387\n",
      "==>>> epoch: 118, batch index: 1, test loss: 0.464945, acc: 0.875\n",
      "==>>> epoch: 119, batch index: 6, train loss: 2.412738\n",
      "==>>> epoch: 119, batch index: 1, test loss: 0.460988, acc: 0.875\n",
      "==>>> epoch: 120, batch index: 6, train loss: 2.401652\n",
      "==>>> epoch: 120, batch index: 1, test loss: 0.458702, acc: 0.875\n",
      "==>>> epoch: 121, batch index: 6, train loss: 2.422379\n",
      "==>>> epoch: 121, batch index: 1, test loss: 0.447242, acc: 0.900\n",
      "==>>> epoch: 122, batch index: 6, train loss: 2.394501\n",
      "==>>> epoch: 122, batch index: 1, test loss: 0.447634, acc: 0.900\n",
      "==>>> epoch: 123, batch index: 6, train loss: 2.459638\n",
      "==>>> epoch: 123, batch index: 1, test loss: 0.462124, acc: 0.875\n",
      "==>>> epoch: 124, batch index: 6, train loss: 2.370575\n",
      "==>>> epoch: 124, batch index: 1, test loss: 0.457913, acc: 0.850\n",
      "==>>> epoch: 125, batch index: 6, train loss: 2.393930\n",
      "==>>> epoch: 125, batch index: 1, test loss: 0.470870, acc: 0.850\n",
      "==>>> epoch: 126, batch index: 6, train loss: 2.383200\n",
      "==>>> epoch: 126, batch index: 1, test loss: 0.462518, acc: 0.850\n",
      "==>>> epoch: 127, batch index: 6, train loss: 2.405129\n",
      "==>>> epoch: 127, batch index: 1, test loss: 0.457534, acc: 0.875\n",
      "==>>> epoch: 128, batch index: 6, train loss: 2.394906\n",
      "==>>> epoch: 128, batch index: 1, test loss: 0.458636, acc: 0.850\n",
      "==>>> epoch: 129, batch index: 6, train loss: 2.381018\n",
      "==>>> epoch: 129, batch index: 1, test loss: 0.461231, acc: 0.875\n",
      "==>>> epoch: 130, batch index: 6, train loss: 2.392640\n",
      "==>>> epoch: 130, batch index: 1, test loss: 0.471684, acc: 0.875\n",
      "==>>> epoch: 131, batch index: 6, train loss: 2.379485\n",
      "==>>> epoch: 131, batch index: 1, test loss: 0.465438, acc: 0.850\n",
      "==>>> epoch: 132, batch index: 6, train loss: 2.417633\n",
      "==>>> epoch: 132, batch index: 1, test loss: 0.461205, acc: 0.875\n",
      "==>>> epoch: 133, batch index: 6, train loss: 2.378259\n",
      "==>>> epoch: 133, batch index: 1, test loss: 0.465824, acc: 0.875\n",
      "==>>> epoch: 134, batch index: 6, train loss: 2.470126\n",
      "==>>> epoch: 134, batch index: 1, test loss: 0.467928, acc: 0.850\n",
      "==>>> epoch: 135, batch index: 6, train loss: 2.389348\n",
      "==>>> epoch: 135, batch index: 1, test loss: 0.453954, acc: 0.875\n",
      "==>>> epoch: 136, batch index: 6, train loss: 2.367195\n",
      "==>>> epoch: 136, batch index: 1, test loss: 0.465173, acc: 0.850\n",
      "==>>> epoch: 137, batch index: 6, train loss: 2.367226\n",
      "==>>> epoch: 137, batch index: 1, test loss: 0.458335, acc: 0.850\n",
      "==>>> epoch: 138, batch index: 6, train loss: 2.384087\n",
      "==>>> epoch: 138, batch index: 1, test loss: 0.456303, acc: 0.875\n",
      "==>>> epoch: 139, batch index: 6, train loss: 2.378134\n",
      "==>>> epoch: 139, batch index: 1, test loss: 0.460856, acc: 0.875\n",
      "==>>> epoch: 140, batch index: 6, train loss: 2.396177\n",
      "==>>> epoch: 140, batch index: 1, test loss: 0.459310, acc: 0.875\n",
      "==>>> epoch: 141, batch index: 6, train loss: 2.398651\n",
      "==>>> epoch: 141, batch index: 1, test loss: 0.464613, acc: 0.875\n",
      "==>>> epoch: 142, batch index: 6, train loss: 2.411672\n",
      "==>>> epoch: 142, batch index: 1, test loss: 0.461164, acc: 0.850\n",
      "==>>> epoch: 143, batch index: 6, train loss: 2.375036\n",
      "==>>> epoch: 143, batch index: 1, test loss: 0.456822, acc: 0.875\n",
      "==>>> epoch: 144, batch index: 6, train loss: 2.403807\n",
      "==>>> epoch: 144, batch index: 1, test loss: 0.462845, acc: 0.875\n",
      "==>>> epoch: 145, batch index: 6, train loss: 2.336786\n",
      "==>>> epoch: 145, batch index: 1, test loss: 0.458644, acc: 0.850\n",
      "==>>> epoch: 146, batch index: 6, train loss: 2.381314\n",
      "==>>> epoch: 146, batch index: 1, test loss: 0.466043, acc: 0.825\n",
      "==>>> epoch: 147, batch index: 6, train loss: 2.395297\n",
      "==>>> epoch: 147, batch index: 1, test loss: 0.461087, acc: 0.875\n",
      "==>>> epoch: 148, batch index: 6, train loss: 2.417578\n",
      "==>>> epoch: 148, batch index: 1, test loss: 0.460034, acc: 0.875\n",
      "==>>> epoch: 149, batch index: 6, train loss: 2.382513\n",
      "==>>> epoch: 149, batch index: 1, test loss: 0.460099, acc: 0.875\n",
      "==>>> epoch: 150, batch index: 6, train loss: 2.383219\n",
      "==>>> epoch: 150, batch index: 1, test loss: 0.462418, acc: 0.875\n",
      "==>>> epoch: 151, batch index: 6, train loss: 2.350450\n",
      "==>>> epoch: 151, batch index: 1, test loss: 0.452593, acc: 0.875\n",
      "==>>> epoch: 152, batch index: 6, train loss: 2.358565\n",
      "==>>> epoch: 152, batch index: 1, test loss: 0.456377, acc: 0.875\n",
      "==>>> epoch: 153, batch index: 6, train loss: 2.345188\n",
      "==>>> epoch: 153, batch index: 1, test loss: 0.453444, acc: 0.850\n",
      "==>>> epoch: 154, batch index: 6, train loss: 2.358684\n",
      "==>>> epoch: 154, batch index: 1, test loss: 0.450519, acc: 0.850\n",
      "==>>> epoch: 155, batch index: 6, train loss: 2.388528\n",
      "==>>> epoch: 155, batch index: 1, test loss: 0.466010, acc: 0.850\n",
      "==>>> epoch: 156, batch index: 6, train loss: 2.366795\n",
      "==>>> epoch: 156, batch index: 1, test loss: 0.464761, acc: 0.850\n",
      "==>>> epoch: 157, batch index: 6, train loss: 2.347030\n",
      "==>>> epoch: 157, batch index: 1, test loss: 0.457583, acc: 0.850\n",
      "==>>> epoch: 158, batch index: 6, train loss: 2.385213\n",
      "==>>> epoch: 158, batch index: 1, test loss: 0.455867, acc: 0.875\n",
      "==>>> epoch: 159, batch index: 6, train loss: 2.344391\n",
      "==>>> epoch: 159, batch index: 1, test loss: 0.455032, acc: 0.850\n",
      "==>>> epoch: 160, batch index: 6, train loss: 2.368277\n",
      "==>>> epoch: 160, batch index: 1, test loss: 0.463650, acc: 0.875\n",
      "==>>> epoch: 161, batch index: 6, train loss: 2.364478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 161, batch index: 1, test loss: 0.452365, acc: 0.900\n",
      "==>>> epoch: 162, batch index: 6, train loss: 2.370288\n",
      "==>>> epoch: 162, batch index: 1, test loss: 0.457684, acc: 0.875\n",
      "==>>> epoch: 163, batch index: 6, train loss: 2.360450\n",
      "==>>> epoch: 163, batch index: 1, test loss: 0.462209, acc: 0.875\n",
      "==>>> epoch: 164, batch index: 6, train loss: 2.351854\n",
      "==>>> epoch: 164, batch index: 1, test loss: 0.466084, acc: 0.850\n",
      "==>>> epoch: 165, batch index: 6, train loss: 2.354943\n",
      "==>>> epoch: 165, batch index: 1, test loss: 0.465097, acc: 0.875\n",
      "==>>> epoch: 166, batch index: 6, train loss: 2.340397\n",
      "==>>> epoch: 166, batch index: 1, test loss: 0.456544, acc: 0.875\n",
      "==>>> epoch: 167, batch index: 6, train loss: 2.365439\n",
      "==>>> epoch: 167, batch index: 1, test loss: 0.457471, acc: 0.875\n",
      "==>>> epoch: 168, batch index: 6, train loss: 2.355801\n",
      "==>>> epoch: 168, batch index: 1, test loss: 0.460426, acc: 0.875\n",
      "==>>> epoch: 169, batch index: 6, train loss: 2.345636\n",
      "==>>> epoch: 169, batch index: 1, test loss: 0.466590, acc: 0.850\n",
      "==>>> epoch: 170, batch index: 6, train loss: 2.357972\n",
      "==>>> epoch: 170, batch index: 1, test loss: 0.452811, acc: 0.850\n",
      "==>>> epoch: 171, batch index: 6, train loss: 2.440011\n",
      "==>>> epoch: 171, batch index: 1, test loss: 0.461207, acc: 0.875\n",
      "==>>> epoch: 172, batch index: 6, train loss: 2.330523\n",
      "==>>> epoch: 172, batch index: 1, test loss: 0.459383, acc: 0.875\n",
      "==>>> epoch: 173, batch index: 6, train loss: 2.335451\n",
      "==>>> epoch: 173, batch index: 1, test loss: 0.459420, acc: 0.850\n",
      "==>>> epoch: 174, batch index: 6, train loss: 2.375980\n",
      "==>>> epoch: 174, batch index: 1, test loss: 0.468057, acc: 0.850\n",
      "==>>> epoch: 175, batch index: 6, train loss: 2.328596\n",
      "==>>> epoch: 175, batch index: 1, test loss: 0.472544, acc: 0.825\n",
      "==>>> epoch: 176, batch index: 6, train loss: 2.357595\n",
      "==>>> epoch: 176, batch index: 1, test loss: 0.461755, acc: 0.875\n",
      "==>>> epoch: 177, batch index: 6, train loss: 2.372718\n",
      "==>>> epoch: 177, batch index: 1, test loss: 0.454506, acc: 0.875\n",
      "==>>> epoch: 178, batch index: 6, train loss: 2.357633\n",
      "==>>> epoch: 178, batch index: 1, test loss: 0.450579, acc: 0.875\n",
      "==>>> epoch: 179, batch index: 6, train loss: 2.365086\n",
      "==>>> epoch: 179, batch index: 1, test loss: 0.454063, acc: 0.850\n",
      "==>>> epoch: 180, batch index: 6, train loss: 2.369978\n",
      "==>>> epoch: 180, batch index: 1, test loss: 0.457749, acc: 0.875\n",
      "==>>> epoch: 181, batch index: 6, train loss: 2.338382\n",
      "==>>> epoch: 181, batch index: 1, test loss: 0.462250, acc: 0.875\n",
      "==>>> epoch: 182, batch index: 6, train loss: 2.348466\n",
      "==>>> epoch: 182, batch index: 1, test loss: 0.453910, acc: 0.875\n",
      "==>>> epoch: 183, batch index: 6, train loss: 2.335368\n",
      "==>>> epoch: 183, batch index: 1, test loss: 0.459162, acc: 0.875\n",
      "==>>> epoch: 184, batch index: 6, train loss: 2.349738\n",
      "==>>> epoch: 184, batch index: 1, test loss: 0.446540, acc: 0.900\n",
      "==>>> epoch: 185, batch index: 6, train loss: 2.335100\n",
      "==>>> epoch: 185, batch index: 1, test loss: 0.455841, acc: 0.875\n",
      "==>>> epoch: 186, batch index: 6, train loss: 2.329885\n",
      "==>>> epoch: 186, batch index: 1, test loss: 0.461931, acc: 0.850\n",
      "==>>> epoch: 187, batch index: 6, train loss: 2.333882\n",
      "==>>> epoch: 187, batch index: 1, test loss: 0.466540, acc: 0.825\n",
      "==>>> epoch: 188, batch index: 6, train loss: 2.365634\n",
      "==>>> epoch: 188, batch index: 1, test loss: 0.462742, acc: 0.850\n",
      "==>>> epoch: 189, batch index: 6, train loss: 2.342032\n",
      "==>>> epoch: 189, batch index: 1, test loss: 0.452144, acc: 0.875\n",
      "==>>> epoch: 190, batch index: 6, train loss: 2.347594\n",
      "==>>> epoch: 190, batch index: 1, test loss: 0.457364, acc: 0.825\n",
      "==>>> epoch: 191, batch index: 6, train loss: 2.361778\n",
      "==>>> epoch: 191, batch index: 1, test loss: 0.460618, acc: 0.850\n",
      "==>>> epoch: 192, batch index: 6, train loss: 2.346897\n",
      "==>>> epoch: 192, batch index: 1, test loss: 0.456116, acc: 0.850\n",
      "==>>> epoch: 193, batch index: 6, train loss: 2.320600\n",
      "==>>> epoch: 193, batch index: 1, test loss: 0.460808, acc: 0.850\n",
      "==>>> epoch: 194, batch index: 6, train loss: 2.344332\n",
      "==>>> epoch: 194, batch index: 1, test loss: 0.468424, acc: 0.850\n",
      "==>>> epoch: 195, batch index: 6, train loss: 2.347860\n",
      "==>>> epoch: 195, batch index: 1, test loss: 0.466277, acc: 0.875\n",
      "==>>> epoch: 196, batch index: 6, train loss: 2.330900\n",
      "==>>> epoch: 196, batch index: 1, test loss: 0.459389, acc: 0.875\n",
      "==>>> epoch: 197, batch index: 6, train loss: 2.333227\n",
      "==>>> epoch: 197, batch index: 1, test loss: 0.465018, acc: 0.850\n",
      "==>>> epoch: 198, batch index: 6, train loss: 2.340717\n",
      "==>>> epoch: 198, batch index: 1, test loss: 0.451241, acc: 0.850\n",
      "==>>> epoch: 199, batch index: 6, train loss: 2.358161\n",
      "==>>> epoch: 199, batch index: 1, test loss: 0.456298, acc: 0.875\n",
      "==>>> epoch: 200, batch index: 6, train loss: 2.304081\n",
      "==>>> epoch: 200, batch index: 1, test loss: 0.461426, acc: 0.825\n",
      "==>>> epoch: 201, batch index: 6, train loss: 2.345463\n",
      "==>>> epoch: 201, batch index: 1, test loss: 0.454021, acc: 0.875\n",
      "==>>> epoch: 202, batch index: 6, train loss: 2.322199\n",
      "==>>> epoch: 202, batch index: 1, test loss: 0.455033, acc: 0.850\n",
      "==>>> epoch: 203, batch index: 6, train loss: 2.351539\n",
      "==>>> epoch: 203, batch index: 1, test loss: 0.462987, acc: 0.825\n",
      "==>>> epoch: 204, batch index: 6, train loss: 2.350851\n",
      "==>>> epoch: 204, batch index: 1, test loss: 0.467968, acc: 0.825\n",
      "==>>> epoch: 205, batch index: 6, train loss: 2.346988\n",
      "==>>> epoch: 205, batch index: 1, test loss: 0.461545, acc: 0.850\n",
      "==>>> epoch: 206, batch index: 6, train loss: 2.319817\n",
      "==>>> epoch: 206, batch index: 1, test loss: 0.459485, acc: 0.850\n",
      "==>>> epoch: 207, batch index: 6, train loss: 2.329956\n",
      "==>>> epoch: 207, batch index: 1, test loss: 0.462754, acc: 0.850\n",
      "==>>> epoch: 208, batch index: 6, train loss: 2.325832\n",
      "==>>> epoch: 208, batch index: 1, test loss: 0.463181, acc: 0.850\n",
      "==>>> epoch: 209, batch index: 6, train loss: 2.361353\n",
      "==>>> epoch: 209, batch index: 1, test loss: 0.465943, acc: 0.850\n",
      "==>>> epoch: 210, batch index: 6, train loss: 2.335359\n",
      "==>>> epoch: 210, batch index: 1, test loss: 0.457280, acc: 0.825\n",
      "==>>> epoch: 211, batch index: 6, train loss: 2.375974\n",
      "==>>> epoch: 211, batch index: 1, test loss: 0.489240, acc: 0.825\n",
      "==>>> epoch: 212, batch index: 6, train loss: 2.368172\n",
      "==>>> epoch: 212, batch index: 1, test loss: 0.459187, acc: 0.850\n",
      "==>>> epoch: 213, batch index: 6, train loss: 2.303801\n",
      "==>>> epoch: 213, batch index: 1, test loss: 0.462118, acc: 0.850\n",
      "==>>> epoch: 214, batch index: 6, train loss: 2.339087\n",
      "==>>> epoch: 214, batch index: 1, test loss: 0.462193, acc: 0.850\n",
      "==>>> epoch: 215, batch index: 6, train loss: 2.313772\n",
      "==>>> epoch: 215, batch index: 1, test loss: 0.465187, acc: 0.875\n",
      "==>>> epoch: 216, batch index: 6, train loss: 2.315045\n",
      "==>>> epoch: 216, batch index: 1, test loss: 0.457991, acc: 0.825\n",
      "==>>> epoch: 217, batch index: 6, train loss: 2.323600\n",
      "==>>> epoch: 217, batch index: 1, test loss: 0.452636, acc: 0.825\n",
      "==>>> epoch: 218, batch index: 6, train loss: 2.367024\n",
      "==>>> epoch: 218, batch index: 1, test loss: 0.457732, acc: 0.875\n",
      "==>>> epoch: 219, batch index: 6, train loss: 2.316681\n",
      "==>>> epoch: 219, batch index: 1, test loss: 0.462847, acc: 0.850\n",
      "==>>> epoch: 220, batch index: 6, train loss: 2.304281\n",
      "==>>> epoch: 220, batch index: 1, test loss: 0.457672, acc: 0.825\n",
      "==>>> epoch: 221, batch index: 6, train loss: 2.326787\n",
      "==>>> epoch: 221, batch index: 1, test loss: 0.458582, acc: 0.825\n",
      "==>>> epoch: 222, batch index: 6, train loss: 2.324518\n",
      "==>>> epoch: 222, batch index: 1, test loss: 0.460622, acc: 0.825\n",
      "==>>> epoch: 223, batch index: 6, train loss: 2.341035\n",
      "==>>> epoch: 223, batch index: 1, test loss: 0.463330, acc: 0.825\n",
      "==>>> epoch: 224, batch index: 6, train loss: 2.326162\n",
      "==>>> epoch: 224, batch index: 1, test loss: 0.463915, acc: 0.850\n",
      "==>>> epoch: 225, batch index: 6, train loss: 2.328215\n",
      "==>>> epoch: 225, batch index: 1, test loss: 0.458598, acc: 0.850\n",
      "==>>> epoch: 226, batch index: 6, train loss: 2.325358\n",
      "==>>> epoch: 226, batch index: 1, test loss: 0.459057, acc: 0.850\n",
      "==>>> epoch: 227, batch index: 6, train loss: 2.347704\n",
      "==>>> epoch: 227, batch index: 1, test loss: 0.462409, acc: 0.850\n",
      "==>>> epoch: 228, batch index: 6, train loss: 2.302845\n",
      "==>>> epoch: 228, batch index: 1, test loss: 0.461827, acc: 0.850\n",
      "==>>> epoch: 229, batch index: 6, train loss: 2.308739\n",
      "==>>> epoch: 229, batch index: 1, test loss: 0.458406, acc: 0.875\n",
      "==>>> epoch: 230, batch index: 6, train loss: 2.326668\n",
      "==>>> epoch: 230, batch index: 1, test loss: 0.455174, acc: 0.875\n",
      "==>>> epoch: 231, batch index: 6, train loss: 2.360093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 231, batch index: 1, test loss: 0.449884, acc: 0.875\n",
      "==>>> epoch: 232, batch index: 6, train loss: 2.293663\n",
      "==>>> epoch: 232, batch index: 1, test loss: 0.455549, acc: 0.850\n",
      "==>>> epoch: 233, batch index: 6, train loss: 2.331991\n",
      "==>>> epoch: 233, batch index: 1, test loss: 0.462644, acc: 0.850\n",
      "==>>> epoch: 234, batch index: 6, train loss: 2.324659\n",
      "==>>> epoch: 234, batch index: 1, test loss: 0.463658, acc: 0.850\n",
      "==>>> epoch: 235, batch index: 6, train loss: 2.348086\n",
      "==>>> epoch: 235, batch index: 1, test loss: 0.471840, acc: 0.850\n",
      "==>>> epoch: 236, batch index: 6, train loss: 2.331333\n",
      "==>>> epoch: 236, batch index: 1, test loss: 0.463943, acc: 0.850\n",
      "==>>> epoch: 237, batch index: 6, train loss: 2.302994\n",
      "==>>> epoch: 237, batch index: 1, test loss: 0.472986, acc: 0.850\n",
      "==>>> epoch: 238, batch index: 6, train loss: 2.334502\n",
      "==>>> epoch: 238, batch index: 1, test loss: 0.456905, acc: 0.875\n",
      "==>>> epoch: 239, batch index: 6, train loss: 2.318149\n",
      "==>>> epoch: 239, batch index: 1, test loss: 0.461133, acc: 0.850\n",
      "==>>> epoch: 240, batch index: 6, train loss: 2.325691\n",
      "==>>> epoch: 240, batch index: 1, test loss: 0.460212, acc: 0.875\n",
      "==>>> epoch: 241, batch index: 6, train loss: 2.320407\n",
      "==>>> epoch: 241, batch index: 1, test loss: 0.463726, acc: 0.850\n",
      "==>>> epoch: 242, batch index: 6, train loss: 2.303261\n",
      "==>>> epoch: 242, batch index: 1, test loss: 0.462774, acc: 0.825\n",
      "==>>> epoch: 243, batch index: 6, train loss: 2.339448\n",
      "==>>> epoch: 243, batch index: 1, test loss: 0.463170, acc: 0.850\n",
      "==>>> epoch: 244, batch index: 6, train loss: 2.304625\n",
      "==>>> epoch: 244, batch index: 1, test loss: 0.456604, acc: 0.875\n",
      "==>>> epoch: 245, batch index: 6, train loss: 2.326522\n",
      "==>>> epoch: 245, batch index: 1, test loss: 0.449095, acc: 0.875\n",
      "==>>> epoch: 246, batch index: 6, train loss: 2.303633\n",
      "==>>> epoch: 246, batch index: 1, test loss: 0.457333, acc: 0.875\n",
      "==>>> epoch: 247, batch index: 6, train loss: 2.293646\n",
      "==>>> epoch: 247, batch index: 1, test loss: 0.469764, acc: 0.850\n",
      "==>>> epoch: 248, batch index: 6, train loss: 2.295059\n",
      "==>>> epoch: 248, batch index: 1, test loss: 0.461957, acc: 0.850\n",
      "==>>> epoch: 249, batch index: 6, train loss: 2.306470\n",
      "==>>> epoch: 249, batch index: 1, test loss: 0.467490, acc: 0.850\n",
      "==>>> epoch: 250, batch index: 6, train loss: 2.322989\n",
      "==>>> epoch: 250, batch index: 1, test loss: 0.465228, acc: 0.850\n",
      "==>>> epoch: 251, batch index: 6, train loss: 2.297727\n",
      "==>>> epoch: 251, batch index: 1, test loss: 0.466155, acc: 0.850\n",
      "==>>> epoch: 252, batch index: 6, train loss: 2.325546\n",
      "==>>> epoch: 252, batch index: 1, test loss: 0.468198, acc: 0.825\n",
      "==>>> epoch: 253, batch index: 6, train loss: 2.324956\n",
      "==>>> epoch: 253, batch index: 1, test loss: 0.458349, acc: 0.850\n",
      "==>>> epoch: 254, batch index: 6, train loss: 2.317429\n",
      "==>>> epoch: 254, batch index: 1, test loss: 0.467951, acc: 0.850\n",
      "==>>> epoch: 255, batch index: 6, train loss: 2.339933\n",
      "==>>> epoch: 255, batch index: 1, test loss: 0.474009, acc: 0.825\n",
      "==>>> epoch: 256, batch index: 6, train loss: 2.343294\n",
      "==>>> epoch: 256, batch index: 1, test loss: 0.487169, acc: 0.825\n",
      "==>>> epoch: 257, batch index: 6, train loss: 2.357696\n",
      "==>>> epoch: 257, batch index: 1, test loss: 0.464465, acc: 0.850\n",
      "==>>> epoch: 258, batch index: 6, train loss: 2.319394\n",
      "==>>> epoch: 258, batch index: 1, test loss: 0.456493, acc: 0.875\n",
      "==>>> epoch: 259, batch index: 6, train loss: 2.293202\n",
      "==>>> epoch: 259, batch index: 1, test loss: 0.463235, acc: 0.825\n",
      "==>>> epoch: 260, batch index: 6, train loss: 2.343816\n",
      "==>>> epoch: 260, batch index: 1, test loss: 0.473556, acc: 0.825\n",
      "==>>> epoch: 261, batch index: 6, train loss: 2.287924\n",
      "==>>> epoch: 261, batch index: 1, test loss: 0.470982, acc: 0.800\n",
      "==>>> epoch: 262, batch index: 6, train loss: 2.303983\n",
      "==>>> epoch: 262, batch index: 1, test loss: 0.459972, acc: 0.850\n",
      "==>>> epoch: 263, batch index: 6, train loss: 2.297503\n",
      "==>>> epoch: 263, batch index: 1, test loss: 0.453283, acc: 0.850\n",
      "==>>> epoch: 264, batch index: 6, train loss: 2.282014\n",
      "==>>> epoch: 264, batch index: 1, test loss: 0.456904, acc: 0.875\n",
      "==>>> epoch: 265, batch index: 6, train loss: 2.279267\n",
      "==>>> epoch: 265, batch index: 1, test loss: 0.471542, acc: 0.850\n",
      "==>>> epoch: 266, batch index: 6, train loss: 2.311688\n",
      "==>>> epoch: 266, batch index: 1, test loss: 0.461186, acc: 0.850\n",
      "==>>> epoch: 267, batch index: 6, train loss: 2.328447\n",
      "==>>> epoch: 267, batch index: 1, test loss: 0.466507, acc: 0.825\n",
      "==>>> epoch: 268, batch index: 6, train loss: 2.309400\n",
      "==>>> epoch: 268, batch index: 1, test loss: 0.458881, acc: 0.850\n",
      "==>>> epoch: 269, batch index: 6, train loss: 2.347492\n",
      "==>>> epoch: 269, batch index: 1, test loss: 0.458533, acc: 0.850\n",
      "==>>> epoch: 270, batch index: 6, train loss: 2.309002\n",
      "==>>> epoch: 270, batch index: 1, test loss: 0.461440, acc: 0.850\n",
      "==>>> epoch: 271, batch index: 6, train loss: 2.348384\n",
      "==>>> epoch: 271, batch index: 1, test loss: 0.466787, acc: 0.850\n",
      "==>>> epoch: 272, batch index: 6, train loss: 2.303455\n",
      "==>>> epoch: 272, batch index: 1, test loss: 0.466331, acc: 0.850\n",
      "==>>> epoch: 273, batch index: 6, train loss: 2.315374\n",
      "==>>> epoch: 273, batch index: 1, test loss: 0.458715, acc: 0.875\n",
      "==>>> epoch: 274, batch index: 6, train loss: 2.301598\n",
      "==>>> epoch: 274, batch index: 1, test loss: 0.471323, acc: 0.825\n",
      "==>>> epoch: 275, batch index: 6, train loss: 2.303201\n",
      "==>>> epoch: 275, batch index: 1, test loss: 0.461993, acc: 0.850\n",
      "==>>> epoch: 276, batch index: 6, train loss: 2.296168\n",
      "==>>> epoch: 276, batch index: 1, test loss: 0.475174, acc: 0.825\n",
      "==>>> epoch: 277, batch index: 6, train loss: 2.297067\n",
      "==>>> epoch: 277, batch index: 1, test loss: 0.468745, acc: 0.850\n",
      "==>>> epoch: 278, batch index: 6, train loss: 2.314643\n",
      "==>>> epoch: 278, batch index: 1, test loss: 0.460584, acc: 0.825\n",
      "==>>> epoch: 279, batch index: 6, train loss: 2.301596\n",
      "==>>> epoch: 279, batch index: 1, test loss: 0.456782, acc: 0.875\n",
      "==>>> epoch: 280, batch index: 6, train loss: 2.306330\n",
      "==>>> epoch: 280, batch index: 1, test loss: 0.465703, acc: 0.825\n",
      "==>>> epoch: 281, batch index: 6, train loss: 2.273636\n",
      "==>>> epoch: 281, batch index: 1, test loss: 0.460312, acc: 0.850\n",
      "==>>> epoch: 282, batch index: 6, train loss: 2.304897\n",
      "==>>> epoch: 282, batch index: 1, test loss: 0.461713, acc: 0.825\n",
      "==>>> epoch: 283, batch index: 6, train loss: 2.269691\n",
      "==>>> epoch: 283, batch index: 1, test loss: 0.459956, acc: 0.875\n",
      "==>>> epoch: 284, batch index: 6, train loss: 2.293193\n",
      "==>>> epoch: 284, batch index: 1, test loss: 0.458807, acc: 0.825\n",
      "==>>> epoch: 285, batch index: 6, train loss: 2.280506\n",
      "==>>> epoch: 285, batch index: 1, test loss: 0.465831, acc: 0.850\n",
      "==>>> epoch: 286, batch index: 6, train loss: 2.301239\n",
      "==>>> epoch: 286, batch index: 1, test loss: 0.476535, acc: 0.850\n",
      "==>>> epoch: 287, batch index: 6, train loss: 2.319387\n",
      "==>>> epoch: 287, batch index: 1, test loss: 0.463204, acc: 0.850\n",
      "==>>> epoch: 288, batch index: 6, train loss: 2.302768\n",
      "==>>> epoch: 288, batch index: 1, test loss: 0.466233, acc: 0.825\n",
      "==>>> epoch: 289, batch index: 6, train loss: 2.294957\n",
      "==>>> epoch: 289, batch index: 1, test loss: 0.459088, acc: 0.875\n",
      "==>>> epoch: 290, batch index: 6, train loss: 2.287438\n",
      "==>>> epoch: 290, batch index: 1, test loss: 0.465801, acc: 0.825\n",
      "==>>> epoch: 291, batch index: 6, train loss: 2.271433\n",
      "==>>> epoch: 291, batch index: 1, test loss: 0.463783, acc: 0.875\n",
      "==>>> epoch: 292, batch index: 6, train loss: 2.300143\n",
      "==>>> epoch: 292, batch index: 1, test loss: 0.463335, acc: 0.875\n",
      "==>>> epoch: 293, batch index: 6, train loss: 2.262215\n",
      "==>>> epoch: 293, batch index: 1, test loss: 0.462342, acc: 0.850\n",
      "==>>> epoch: 294, batch index: 6, train loss: 2.278757\n",
      "==>>> epoch: 294, batch index: 1, test loss: 0.459830, acc: 0.850\n",
      "==>>> epoch: 295, batch index: 6, train loss: 2.300307\n",
      "==>>> epoch: 295, batch index: 1, test loss: 0.465978, acc: 0.825\n",
      "==>>> epoch: 296, batch index: 6, train loss: 2.291479\n",
      "==>>> epoch: 296, batch index: 1, test loss: 0.453605, acc: 0.875\n",
      "==>>> epoch: 297, batch index: 6, train loss: 2.296552\n",
      "==>>> epoch: 297, batch index: 1, test loss: 0.443301, acc: 0.875\n",
      "==>>> epoch: 298, batch index: 6, train loss: 2.265908\n",
      "==>>> epoch: 298, batch index: 1, test loss: 0.454716, acc: 0.850\n",
      "==>>> epoch: 299, batch index: 6, train loss: 2.262758\n",
      "==>>> epoch: 299, batch index: 1, test loss: 0.455373, acc: 0.875\n",
      "==>>> epoch: 300, batch index: 6, train loss: 2.254212\n",
      "==>>> epoch: 300, batch index: 1, test loss: 0.450395, acc: 0.875\n",
      "==>>> epoch: 301, batch index: 6, train loss: 2.287419\n",
      "==>>> epoch: 301, batch index: 1, test loss: 0.450724, acc: 0.875\n",
      "==>>> epoch: 302, batch index: 6, train loss: 2.273264\n",
      "==>>> epoch: 302, batch index: 1, test loss: 0.456561, acc: 0.825\n",
      "==>>> epoch: 303, batch index: 6, train loss: 2.263119\n",
      "==>>> epoch: 303, batch index: 1, test loss: 0.453668, acc: 0.875\n",
      "==>>> epoch: 304, batch index: 6, train loss: 2.301700\n",
      "==>>> epoch: 304, batch index: 1, test loss: 0.451338, acc: 0.875\n",
      "==>>> epoch: 305, batch index: 6, train loss: 2.285544\n",
      "==>>> epoch: 305, batch index: 1, test loss: 0.451755, acc: 0.875\n",
      "==>>> epoch: 306, batch index: 6, train loss: 2.271496\n",
      "==>>> epoch: 306, batch index: 1, test loss: 0.449033, acc: 0.850\n",
      "==>>> epoch: 307, batch index: 6, train loss: 2.251323\n",
      "==>>> epoch: 307, batch index: 1, test loss: 0.441880, acc: 0.875\n",
      "==>>> epoch: 308, batch index: 6, train loss: 2.243160\n",
      "==>>> epoch: 308, batch index: 1, test loss: 0.447953, acc: 0.875\n",
      "==>>> epoch: 309, batch index: 6, train loss: 2.260927\n",
      "==>>> epoch: 309, batch index: 1, test loss: 0.441384, acc: 0.875\n",
      "==>>> epoch: 310, batch index: 6, train loss: 2.288926\n",
      "==>>> epoch: 310, batch index: 1, test loss: 0.439869, acc: 0.875\n",
      "==>>> epoch: 311, batch index: 6, train loss: 2.286231\n",
      "==>>> epoch: 311, batch index: 1, test loss: 0.442532, acc: 0.875\n",
      "==>>> epoch: 312, batch index: 6, train loss: 2.236149\n",
      "==>>> epoch: 312, batch index: 1, test loss: 0.432518, acc: 0.875\n",
      "==>>> epoch: 313, batch index: 6, train loss: 2.250016\n",
      "==>>> epoch: 313, batch index: 1, test loss: 0.427408, acc: 0.875\n",
      "==>>> epoch: 314, batch index: 6, train loss: 2.236745\n",
      "==>>> epoch: 314, batch index: 1, test loss: 0.438879, acc: 0.875\n",
      "==>>> epoch: 315, batch index: 6, train loss: 2.229272\n",
      "==>>> epoch: 315, batch index: 1, test loss: 0.432817, acc: 0.875\n",
      "==>>> epoch: 316, batch index: 6, train loss: 2.259934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 316, batch index: 1, test loss: 0.447374, acc: 0.850\n",
      "==>>> epoch: 317, batch index: 6, train loss: 2.263046\n",
      "==>>> epoch: 317, batch index: 1, test loss: 0.443331, acc: 0.875\n",
      "==>>> epoch: 318, batch index: 6, train loss: 2.241668\n",
      "==>>> epoch: 318, batch index: 1, test loss: 0.431931, acc: 0.900\n",
      "==>>> epoch: 319, batch index: 6, train loss: 2.265267\n",
      "==>>> epoch: 319, batch index: 1, test loss: 0.432289, acc: 0.900\n",
      "==>>> epoch: 320, batch index: 6, train loss: 2.234302\n",
      "==>>> epoch: 320, batch index: 1, test loss: 0.431735, acc: 0.875\n",
      "==>>> epoch: 321, batch index: 6, train loss: 2.274040\n",
      "==>>> epoch: 321, batch index: 1, test loss: 0.423035, acc: 0.875\n",
      "==>>> epoch: 322, batch index: 6, train loss: 2.275863\n",
      "==>>> epoch: 322, batch index: 1, test loss: 0.425761, acc: 0.900\n",
      "==>>> epoch: 323, batch index: 6, train loss: 2.241346\n",
      "==>>> epoch: 323, batch index: 1, test loss: 0.432539, acc: 0.875\n",
      "==>>> epoch: 324, batch index: 6, train loss: 2.239926\n",
      "==>>> epoch: 324, batch index: 1, test loss: 0.436853, acc: 0.900\n",
      "==>>> epoch: 325, batch index: 6, train loss: 2.227426\n",
      "==>>> epoch: 325, batch index: 1, test loss: 0.431411, acc: 0.900\n",
      "==>>> epoch: 326, batch index: 6, train loss: 2.257191\n",
      "==>>> epoch: 326, batch index: 1, test loss: 0.427200, acc: 0.900\n",
      "==>>> epoch: 327, batch index: 6, train loss: 2.247227\n",
      "==>>> epoch: 327, batch index: 1, test loss: 0.438053, acc: 0.875\n",
      "==>>> epoch: 328, batch index: 6, train loss: 2.234364\n",
      "==>>> epoch: 328, batch index: 1, test loss: 0.422316, acc: 0.900\n",
      "==>>> epoch: 329, batch index: 6, train loss: 2.264005\n",
      "==>>> epoch: 329, batch index: 1, test loss: 0.415988, acc: 0.900\n",
      "==>>> epoch: 330, batch index: 6, train loss: 2.316196\n",
      "==>>> epoch: 330, batch index: 1, test loss: 0.434140, acc: 0.875\n",
      "==>>> epoch: 331, batch index: 6, train loss: 2.266746\n",
      "==>>> epoch: 331, batch index: 1, test loss: 0.423344, acc: 0.900\n",
      "==>>> epoch: 332, batch index: 6, train loss: 2.252850\n",
      "==>>> epoch: 332, batch index: 1, test loss: 0.432652, acc: 0.875\n",
      "==>>> epoch: 333, batch index: 6, train loss: 2.240737\n",
      "==>>> epoch: 333, batch index: 1, test loss: 0.422560, acc: 0.900\n",
      "==>>> epoch: 334, batch index: 6, train loss: 2.250426\n",
      "==>>> epoch: 334, batch index: 1, test loss: 0.426861, acc: 0.900\n",
      "==>>> epoch: 335, batch index: 6, train loss: 2.222332\n",
      "==>>> epoch: 335, batch index: 1, test loss: 0.433616, acc: 0.875\n",
      "==>>> epoch: 336, batch index: 6, train loss: 2.236085\n",
      "==>>> epoch: 336, batch index: 1, test loss: 0.450263, acc: 0.875\n",
      "==>>> epoch: 337, batch index: 6, train loss: 2.252952\n",
      "==>>> epoch: 337, batch index: 1, test loss: 0.428267, acc: 0.900\n",
      "==>>> epoch: 338, batch index: 6, train loss: 2.247957\n",
      "==>>> epoch: 338, batch index: 1, test loss: 0.425811, acc: 0.900\n",
      "==>>> epoch: 339, batch index: 6, train loss: 2.222569\n",
      "==>>> epoch: 339, batch index: 1, test loss: 0.425603, acc: 0.900\n",
      "==>>> epoch: 340, batch index: 6, train loss: 2.246295\n",
      "==>>> epoch: 340, batch index: 1, test loss: 0.433846, acc: 0.875\n",
      "==>>> epoch: 341, batch index: 6, train loss: 2.239388\n",
      "==>>> epoch: 341, batch index: 1, test loss: 0.415391, acc: 0.925\n",
      "==>>> epoch: 342, batch index: 6, train loss: 2.240666\n",
      "==>>> epoch: 342, batch index: 1, test loss: 0.412492, acc: 0.925\n",
      "==>>> epoch: 343, batch index: 6, train loss: 2.250144\n",
      "==>>> epoch: 343, batch index: 1, test loss: 0.418522, acc: 0.900\n",
      "==>>> epoch: 344, batch index: 6, train loss: 2.260785\n",
      "==>>> epoch: 344, batch index: 1, test loss: 0.430676, acc: 0.875\n",
      "==>>> epoch: 345, batch index: 6, train loss: 2.237749\n",
      "==>>> epoch: 345, batch index: 1, test loss: 0.430868, acc: 0.875\n",
      "==>>> epoch: 346, batch index: 6, train loss: 2.218131\n",
      "==>>> epoch: 346, batch index: 1, test loss: 0.431475, acc: 0.875\n",
      "==>>> epoch: 347, batch index: 6, train loss: 2.264059\n",
      "==>>> epoch: 347, batch index: 1, test loss: 0.425878, acc: 0.900\n",
      "==>>> epoch: 348, batch index: 6, train loss: 2.249811\n",
      "==>>> epoch: 348, batch index: 1, test loss: 0.419772, acc: 0.900\n",
      "==>>> epoch: 349, batch index: 6, train loss: 2.238872\n",
      "==>>> epoch: 349, batch index: 1, test loss: 0.422166, acc: 0.900\n",
      "==>>> epoch: 350, batch index: 6, train loss: 2.250058\n",
      "==>>> epoch: 350, batch index: 1, test loss: 0.422648, acc: 0.900\n",
      "==>>> epoch: 351, batch index: 6, train loss: 2.223232\n",
      "==>>> epoch: 351, batch index: 1, test loss: 0.421769, acc: 0.900\n",
      "==>>> epoch: 352, batch index: 6, train loss: 2.222193\n",
      "==>>> epoch: 352, batch index: 1, test loss: 0.413483, acc: 0.900\n",
      "==>>> epoch: 353, batch index: 6, train loss: 2.241753\n",
      "==>>> epoch: 353, batch index: 1, test loss: 0.416200, acc: 0.900\n",
      "==>>> epoch: 354, batch index: 6, train loss: 2.256188\n",
      "==>>> epoch: 354, batch index: 1, test loss: 0.427030, acc: 0.900\n",
      "==>>> epoch: 355, batch index: 6, train loss: 2.232647\n",
      "==>>> epoch: 355, batch index: 1, test loss: 0.413516, acc: 0.900\n",
      "==>>> epoch: 356, batch index: 6, train loss: 2.245012\n",
      "==>>> epoch: 356, batch index: 1, test loss: 0.431404, acc: 0.875\n",
      "==>>> epoch: 357, batch index: 6, train loss: 2.214398\n",
      "==>>> epoch: 357, batch index: 1, test loss: 0.423485, acc: 0.900\n",
      "==>>> epoch: 358, batch index: 6, train loss: 2.236604\n",
      "==>>> epoch: 358, batch index: 1, test loss: 0.425748, acc: 0.900\n",
      "==>>> epoch: 359, batch index: 6, train loss: 2.239614\n",
      "==>>> epoch: 359, batch index: 1, test loss: 0.417299, acc: 0.900\n",
      "==>>> epoch: 360, batch index: 6, train loss: 2.355002\n",
      "==>>> epoch: 360, batch index: 1, test loss: 0.434893, acc: 0.875\n",
      "==>>> epoch: 361, batch index: 6, train loss: 2.225300\n",
      "==>>> epoch: 361, batch index: 1, test loss: 0.453498, acc: 0.875\n",
      "==>>> epoch: 362, batch index: 6, train loss: 2.266334\n",
      "==>>> epoch: 362, batch index: 1, test loss: 0.419394, acc: 0.900\n",
      "==>>> epoch: 363, batch index: 6, train loss: 2.236574\n",
      "==>>> epoch: 363, batch index: 1, test loss: 0.421196, acc: 0.900\n",
      "==>>> epoch: 364, batch index: 6, train loss: 2.256483\n",
      "==>>> epoch: 364, batch index: 1, test loss: 0.428574, acc: 0.875\n",
      "==>>> epoch: 365, batch index: 6, train loss: 2.225164\n",
      "==>>> epoch: 365, batch index: 1, test loss: 0.448581, acc: 0.875\n",
      "==>>> epoch: 366, batch index: 6, train loss: 2.222713\n",
      "==>>> epoch: 366, batch index: 1, test loss: 0.422835, acc: 0.900\n",
      "==>>> epoch: 367, batch index: 6, train loss: 2.238434\n",
      "==>>> epoch: 367, batch index: 1, test loss: 0.417452, acc: 0.900\n",
      "==>>> epoch: 368, batch index: 6, train loss: 2.251762\n",
      "==>>> epoch: 368, batch index: 1, test loss: 0.423524, acc: 0.900\n",
      "==>>> epoch: 369, batch index: 6, train loss: 2.243055\n",
      "==>>> epoch: 369, batch index: 1, test loss: 0.449065, acc: 0.875\n",
      "==>>> epoch: 370, batch index: 6, train loss: 2.227386\n",
      "==>>> epoch: 370, batch index: 1, test loss: 0.438718, acc: 0.875\n",
      "==>>> epoch: 371, batch index: 6, train loss: 2.223398\n",
      "==>>> epoch: 371, batch index: 1, test loss: 0.416516, acc: 0.925\n",
      "==>>> epoch: 372, batch index: 6, train loss: 2.236044\n",
      "==>>> epoch: 372, batch index: 1, test loss: 0.425917, acc: 0.900\n",
      "==>>> epoch: 373, batch index: 6, train loss: 2.207223\n",
      "==>>> epoch: 373, batch index: 1, test loss: 0.404739, acc: 0.925\n",
      "==>>> epoch: 374, batch index: 6, train loss: 2.275351\n",
      "==>>> epoch: 374, batch index: 1, test loss: 0.430362, acc: 0.875\n",
      "==>>> epoch: 375, batch index: 6, train loss: 2.218636\n",
      "==>>> epoch: 375, batch index: 1, test loss: 0.422627, acc: 0.900\n",
      "==>>> epoch: 376, batch index: 6, train loss: 2.262183\n",
      "==>>> epoch: 376, batch index: 1, test loss: 0.431428, acc: 0.875\n",
      "==>>> epoch: 377, batch index: 6, train loss: 2.275870\n",
      "==>>> epoch: 377, batch index: 1, test loss: 0.442838, acc: 0.875\n",
      "==>>> epoch: 378, batch index: 6, train loss: 2.230588\n",
      "==>>> epoch: 378, batch index: 1, test loss: 0.436397, acc: 0.875\n",
      "==>>> epoch: 379, batch index: 6, train loss: 2.290011\n",
      "==>>> epoch: 379, batch index: 1, test loss: 0.443400, acc: 0.875\n",
      "==>>> epoch: 380, batch index: 6, train loss: 2.276898\n",
      "==>>> epoch: 380, batch index: 1, test loss: 0.440683, acc: 0.875\n",
      "==>>> epoch: 381, batch index: 6, train loss: 2.252143\n",
      "==>>> epoch: 381, batch index: 1, test loss: 0.429094, acc: 0.900\n",
      "==>>> epoch: 382, batch index: 6, train loss: 2.232517\n",
      "==>>> epoch: 382, batch index: 1, test loss: 0.416309, acc: 0.900\n",
      "==>>> epoch: 383, batch index: 6, train loss: 2.252256\n",
      "==>>> epoch: 383, batch index: 1, test loss: 0.427772, acc: 0.875\n",
      "==>>> epoch: 384, batch index: 6, train loss: 2.250528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 384, batch index: 1, test loss: 0.430884, acc: 0.875\n",
      "==>>> epoch: 385, batch index: 6, train loss: 2.243304\n",
      "==>>> epoch: 385, batch index: 1, test loss: 0.418265, acc: 0.900\n",
      "==>>> epoch: 386, batch index: 6, train loss: 2.240826\n",
      "==>>> epoch: 386, batch index: 1, test loss: 0.415761, acc: 0.900\n",
      "==>>> epoch: 387, batch index: 6, train loss: 2.220932\n",
      "==>>> epoch: 387, batch index: 1, test loss: 0.432190, acc: 0.875\n",
      "==>>> epoch: 388, batch index: 6, train loss: 2.195452\n",
      "==>>> epoch: 388, batch index: 1, test loss: 0.398193, acc: 0.900\n",
      "==>>> epoch: 389, batch index: 6, train loss: 2.240951\n",
      "==>>> epoch: 389, batch index: 1, test loss: 0.433430, acc: 0.875\n",
      "==>>> epoch: 390, batch index: 6, train loss: 2.280889\n",
      "==>>> epoch: 390, batch index: 1, test loss: 0.436231, acc: 0.875\n",
      "==>>> epoch: 391, batch index: 6, train loss: 2.215248\n",
      "==>>> epoch: 391, batch index: 1, test loss: 0.435282, acc: 0.875\n",
      "==>>> epoch: 392, batch index: 6, train loss: 2.212585\n",
      "==>>> epoch: 392, batch index: 1, test loss: 0.412223, acc: 0.900\n",
      "==>>> epoch: 393, batch index: 6, train loss: 2.301821\n",
      "==>>> epoch: 393, batch index: 1, test loss: 0.428468, acc: 0.900\n",
      "==>>> epoch: 394, batch index: 6, train loss: 2.230877\n",
      "==>>> epoch: 394, batch index: 1, test loss: 0.439719, acc: 0.875\n",
      "==>>> epoch: 395, batch index: 6, train loss: 2.230378\n",
      "==>>> epoch: 395, batch index: 1, test loss: 0.421028, acc: 0.900\n",
      "==>>> epoch: 396, batch index: 6, train loss: 2.211757\n",
      "==>>> epoch: 396, batch index: 1, test loss: 0.427544, acc: 0.900\n",
      "==>>> epoch: 397, batch index: 6, train loss: 2.217160\n",
      "==>>> epoch: 397, batch index: 1, test loss: 0.430950, acc: 0.875\n",
      "==>>> epoch: 398, batch index: 6, train loss: 2.272612\n",
      "==>>> epoch: 398, batch index: 1, test loss: 0.440418, acc: 0.875\n",
      "==>>> epoch: 399, batch index: 6, train loss: 2.219578\n",
      "==>>> epoch: 399, batch index: 1, test loss: 0.435153, acc: 0.875\n",
      "==>>> epoch: 400, batch index: 6, train loss: 2.208780\n",
      "==>>> epoch: 400, batch index: 1, test loss: 0.430699, acc: 0.875\n",
      "==>>> epoch: 401, batch index: 6, train loss: 2.218686\n",
      "==>>> epoch: 401, batch index: 1, test loss: 0.404149, acc: 0.925\n",
      "==>>> epoch: 402, batch index: 6, train loss: 2.269576\n",
      "==>>> epoch: 402, batch index: 1, test loss: 0.438994, acc: 0.875\n",
      "==>>> epoch: 403, batch index: 6, train loss: 2.247655\n",
      "==>>> epoch: 403, batch index: 1, test loss: 0.420393, acc: 0.900\n",
      "==>>> epoch: 404, batch index: 6, train loss: 2.240722\n",
      "==>>> epoch: 404, batch index: 1, test loss: 0.414128, acc: 0.900\n",
      "==>>> epoch: 405, batch index: 6, train loss: 2.232116\n",
      "==>>> epoch: 405, batch index: 1, test loss: 0.422637, acc: 0.900\n",
      "==>>> epoch: 406, batch index: 6, train loss: 2.263550\n",
      "==>>> epoch: 406, batch index: 1, test loss: 0.430140, acc: 0.875\n",
      "==>>> epoch: 407, batch index: 6, train loss: 2.211447\n",
      "==>>> epoch: 407, batch index: 1, test loss: 0.417282, acc: 0.900\n",
      "==>>> epoch: 408, batch index: 6, train loss: 2.196855\n",
      "==>>> epoch: 408, batch index: 1, test loss: 0.434560, acc: 0.875\n",
      "==>>> epoch: 409, batch index: 6, train loss: 2.223915\n",
      "==>>> epoch: 409, batch index: 1, test loss: 0.420225, acc: 0.900\n",
      "==>>> epoch: 410, batch index: 6, train loss: 2.257883\n",
      "==>>> epoch: 410, batch index: 1, test loss: 0.411774, acc: 0.900\n",
      "==>>> epoch: 411, batch index: 6, train loss: 2.263179\n",
      "==>>> epoch: 411, batch index: 1, test loss: 0.430048, acc: 0.875\n",
      "==>>> epoch: 412, batch index: 6, train loss: 2.229625\n",
      "==>>> epoch: 412, batch index: 1, test loss: 0.448925, acc: 0.875\n",
      "==>>> epoch: 413, batch index: 6, train loss: 2.250445\n",
      "==>>> epoch: 413, batch index: 1, test loss: 0.414554, acc: 0.900\n",
      "==>>> epoch: 414, batch index: 6, train loss: 2.246989\n",
      "==>>> epoch: 414, batch index: 1, test loss: 0.426504, acc: 0.900\n",
      "==>>> epoch: 415, batch index: 6, train loss: 2.211118\n",
      "==>>> epoch: 415, batch index: 1, test loss: 0.420884, acc: 0.900\n",
      "==>>> epoch: 416, batch index: 6, train loss: 2.213945\n",
      "==>>> epoch: 416, batch index: 1, test loss: 0.424547, acc: 0.900\n",
      "==>>> epoch: 417, batch index: 6, train loss: 2.252893\n",
      "==>>> epoch: 417, batch index: 1, test loss: 0.408871, acc: 0.900\n",
      "==>>> epoch: 418, batch index: 6, train loss: 2.227810\n",
      "==>>> epoch: 418, batch index: 1, test loss: 0.433109, acc: 0.875\n",
      "==>>> epoch: 419, batch index: 6, train loss: 2.239692\n",
      "==>>> epoch: 419, batch index: 1, test loss: 0.430667, acc: 0.875\n",
      "==>>> epoch: 420, batch index: 6, train loss: 2.237638\n",
      "==>>> epoch: 420, batch index: 1, test loss: 0.426469, acc: 0.900\n",
      "==>>> epoch: 421, batch index: 6, train loss: 2.220832\n",
      "==>>> epoch: 421, batch index: 1, test loss: 0.446621, acc: 0.875\n",
      "==>>> epoch: 422, batch index: 6, train loss: 2.210244\n",
      "==>>> epoch: 422, batch index: 1, test loss: 0.445535, acc: 0.875\n",
      "==>>> epoch: 423, batch index: 6, train loss: 2.226407\n",
      "==>>> epoch: 423, batch index: 1, test loss: 0.430228, acc: 0.875\n",
      "==>>> epoch: 424, batch index: 6, train loss: 2.221090\n",
      "==>>> epoch: 424, batch index: 1, test loss: 0.421836, acc: 0.900\n",
      "==>>> epoch: 425, batch index: 6, train loss: 2.299085\n",
      "==>>> epoch: 425, batch index: 1, test loss: 0.428576, acc: 0.875\n",
      "==>>> epoch: 426, batch index: 6, train loss: 2.221593\n",
      "==>>> epoch: 426, batch index: 1, test loss: 0.425873, acc: 0.900\n",
      "==>>> epoch: 427, batch index: 6, train loss: 2.203215\n",
      "==>>> epoch: 427, batch index: 1, test loss: 0.427661, acc: 0.875\n",
      "==>>> epoch: 428, batch index: 6, train loss: 2.218467\n",
      "==>>> epoch: 428, batch index: 1, test loss: 0.432747, acc: 0.875\n",
      "==>>> epoch: 429, batch index: 6, train loss: 2.224287\n",
      "==>>> epoch: 429, batch index: 1, test loss: 0.431852, acc: 0.875\n",
      "==>>> epoch: 430, batch index: 6, train loss: 2.208679\n",
      "==>>> epoch: 430, batch index: 1, test loss: 0.406107, acc: 0.925\n",
      "==>>> epoch: 431, batch index: 6, train loss: 2.253389\n",
      "==>>> epoch: 431, batch index: 1, test loss: 0.418420, acc: 0.900\n",
      "==>>> epoch: 432, batch index: 6, train loss: 2.200361\n",
      "==>>> epoch: 432, batch index: 1, test loss: 0.435959, acc: 0.875\n",
      "==>>> epoch: 433, batch index: 6, train loss: 2.192669\n",
      "==>>> epoch: 433, batch index: 1, test loss: 0.420291, acc: 0.900\n",
      "==>>> epoch: 434, batch index: 6, train loss: 2.205626\n",
      "==>>> epoch: 434, batch index: 1, test loss: 0.435988, acc: 0.875\n",
      "==>>> epoch: 435, batch index: 6, train loss: 2.198714\n",
      "==>>> epoch: 435, batch index: 1, test loss: 0.423904, acc: 0.900\n",
      "==>>> epoch: 436, batch index: 6, train loss: 2.217563\n",
      "==>>> epoch: 436, batch index: 1, test loss: 0.424725, acc: 0.900\n",
      "==>>> epoch: 437, batch index: 6, train loss: 2.237800\n",
      "==>>> epoch: 437, batch index: 1, test loss: 0.421943, acc: 0.900\n",
      "==>>> epoch: 438, batch index: 6, train loss: 2.212031\n",
      "==>>> epoch: 438, batch index: 1, test loss: 0.407374, acc: 0.900\n",
      "==>>> epoch: 439, batch index: 6, train loss: 2.194693\n",
      "==>>> epoch: 439, batch index: 1, test loss: 0.426697, acc: 0.900\n",
      "==>>> epoch: 440, batch index: 6, train loss: 2.208188\n",
      "==>>> epoch: 440, batch index: 1, test loss: 0.420541, acc: 0.900\n",
      "==>>> epoch: 441, batch index: 6, train loss: 2.206157\n",
      "==>>> epoch: 441, batch index: 1, test loss: 0.428525, acc: 0.875\n",
      "==>>> epoch: 442, batch index: 6, train loss: 2.265691\n",
      "==>>> epoch: 442, batch index: 1, test loss: 0.431881, acc: 0.875\n",
      "==>>> epoch: 443, batch index: 6, train loss: 2.215225\n",
      "==>>> epoch: 443, batch index: 1, test loss: 0.421947, acc: 0.900\n",
      "==>>> epoch: 444, batch index: 6, train loss: 2.225811\n",
      "==>>> epoch: 444, batch index: 1, test loss: 0.426993, acc: 0.900\n",
      "==>>> epoch: 445, batch index: 6, train loss: 2.184575\n",
      "==>>> epoch: 445, batch index: 1, test loss: 0.420826, acc: 0.900\n",
      "==>>> epoch: 446, batch index: 6, train loss: 2.256945\n",
      "==>>> epoch: 446, batch index: 1, test loss: 0.417371, acc: 0.900\n",
      "==>>> epoch: 447, batch index: 6, train loss: 2.198988\n",
      "==>>> epoch: 447, batch index: 1, test loss: 0.413242, acc: 0.900\n",
      "==>>> epoch: 448, batch index: 6, train loss: 2.240974\n",
      "==>>> epoch: 448, batch index: 1, test loss: 0.403467, acc: 0.900\n",
      "==>>> epoch: 449, batch index: 6, train loss: 2.309355\n",
      "==>>> epoch: 449, batch index: 1, test loss: 0.406063, acc: 0.925\n",
      "==>>> epoch: 450, batch index: 6, train loss: 2.263022\n",
      "==>>> epoch: 450, batch index: 1, test loss: 0.418689, acc: 0.900\n",
      "==>>> epoch: 451, batch index: 6, train loss: 2.228556\n",
      "==>>> epoch: 451, batch index: 1, test loss: 0.432943, acc: 0.875\n",
      "==>>> epoch: 452, batch index: 6, train loss: 2.247856\n",
      "==>>> epoch: 452, batch index: 1, test loss: 0.413668, acc: 0.900\n",
      "==>>> epoch: 453, batch index: 6, train loss: 2.233323\n",
      "==>>> epoch: 453, batch index: 1, test loss: 0.427736, acc: 0.875\n",
      "==>>> epoch: 454, batch index: 6, train loss: 2.249820\n",
      "==>>> epoch: 454, batch index: 1, test loss: 0.441294, acc: 0.875\n",
      "==>>> epoch: 455, batch index: 6, train loss: 2.222167\n",
      "==>>> epoch: 455, batch index: 1, test loss: 0.421588, acc: 0.900\n",
      "==>>> epoch: 456, batch index: 6, train loss: 2.227593\n",
      "==>>> epoch: 456, batch index: 1, test loss: 0.419261, acc: 0.900\n",
      "==>>> epoch: 457, batch index: 6, train loss: 2.186955\n",
      "==>>> epoch: 457, batch index: 1, test loss: 0.402686, acc: 0.925\n",
      "==>>> epoch: 458, batch index: 6, train loss: 2.215322\n",
      "==>>> epoch: 458, batch index: 1, test loss: 0.430270, acc: 0.875\n",
      "==>>> epoch: 459, batch index: 6, train loss: 2.222391\n",
      "==>>> epoch: 459, batch index: 1, test loss: 0.414601, acc: 0.900\n",
      "==>>> epoch: 460, batch index: 6, train loss: 2.245548\n",
      "==>>> epoch: 460, batch index: 1, test loss: 0.410031, acc: 0.900\n",
      "==>>> epoch: 461, batch index: 6, train loss: 2.273834\n",
      "==>>> epoch: 461, batch index: 1, test loss: 0.429641, acc: 0.875\n",
      "==>>> epoch: 462, batch index: 6, train loss: 2.214889\n",
      "==>>> epoch: 462, batch index: 1, test loss: 0.431447, acc: 0.875\n",
      "==>>> epoch: 463, batch index: 6, train loss: 2.266676\n",
      "==>>> epoch: 463, batch index: 1, test loss: 0.425874, acc: 0.900\n",
      "==>>> epoch: 464, batch index: 6, train loss: 2.225084\n",
      "==>>> epoch: 464, batch index: 1, test loss: 0.445348, acc: 0.875\n",
      "==>>> epoch: 465, batch index: 6, train loss: 2.209918\n",
      "==>>> epoch: 465, batch index: 1, test loss: 0.444583, acc: 0.875\n",
      "==>>> epoch: 466, batch index: 6, train loss: 2.204847\n",
      "==>>> epoch: 466, batch index: 1, test loss: 0.429498, acc: 0.875\n",
      "==>>> epoch: 467, batch index: 6, train loss: 2.225478\n",
      "==>>> epoch: 467, batch index: 1, test loss: 0.445993, acc: 0.875\n",
      "==>>> epoch: 468, batch index: 6, train loss: 2.207223\n",
      "==>>> epoch: 468, batch index: 1, test loss: 0.426894, acc: 0.900\n",
      "==>>> epoch: 469, batch index: 6, train loss: 2.189757\n",
      "==>>> epoch: 469, batch index: 1, test loss: 0.409423, acc: 0.900\n",
      "==>>> epoch: 470, batch index: 6, train loss: 2.240766\n",
      "==>>> epoch: 470, batch index: 1, test loss: 0.417710, acc: 0.900\n",
      "==>>> epoch: 471, batch index: 6, train loss: 2.207189\n",
      "==>>> epoch: 471, batch index: 1, test loss: 0.426446, acc: 0.900\n",
      "==>>> epoch: 472, batch index: 6, train loss: 2.215061\n",
      "==>>> epoch: 472, batch index: 1, test loss: 0.430941, acc: 0.875\n",
      "==>>> epoch: 473, batch index: 6, train loss: 2.228424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 473, batch index: 1, test loss: 0.429259, acc: 0.875\n",
      "==>>> epoch: 474, batch index: 6, train loss: 2.232612\n",
      "==>>> epoch: 474, batch index: 1, test loss: 0.425752, acc: 0.900\n",
      "==>>> epoch: 475, batch index: 6, train loss: 2.264345\n",
      "==>>> epoch: 475, batch index: 1, test loss: 0.429890, acc: 0.875\n",
      "==>>> epoch: 476, batch index: 6, train loss: 2.190793\n",
      "==>>> epoch: 476, batch index: 1, test loss: 0.418500, acc: 0.900\n",
      "==>>> epoch: 477, batch index: 6, train loss: 2.195273\n",
      "==>>> epoch: 477, batch index: 1, test loss: 0.446299, acc: 0.875\n",
      "==>>> epoch: 478, batch index: 6, train loss: 2.277557\n",
      "==>>> epoch: 478, batch index: 1, test loss: 0.424788, acc: 0.900\n",
      "==>>> epoch: 479, batch index: 6, train loss: 2.207525\n",
      "==>>> epoch: 479, batch index: 1, test loss: 0.427895, acc: 0.875\n",
      "==>>> epoch: 480, batch index: 6, train loss: 2.220232\n",
      "==>>> epoch: 480, batch index: 1, test loss: 0.415082, acc: 0.900\n",
      "==>>> epoch: 481, batch index: 6, train loss: 2.196293\n",
      "==>>> epoch: 481, batch index: 1, test loss: 0.430718, acc: 0.875\n",
      "==>>> epoch: 482, batch index: 6, train loss: 2.237438\n",
      "==>>> epoch: 482, batch index: 1, test loss: 0.446282, acc: 0.875\n",
      "==>>> epoch: 483, batch index: 6, train loss: 2.224992\n",
      "==>>> epoch: 483, batch index: 1, test loss: 0.409053, acc: 0.900\n",
      "==>>> epoch: 484, batch index: 6, train loss: 2.239168\n",
      "==>>> epoch: 484, batch index: 1, test loss: 0.424909, acc: 0.900\n",
      "==>>> epoch: 485, batch index: 6, train loss: 2.191808\n",
      "==>>> epoch: 485, batch index: 1, test loss: 0.406101, acc: 0.925\n",
      "==>>> epoch: 486, batch index: 6, train loss: 2.214807\n",
      "==>>> epoch: 486, batch index: 1, test loss: 0.433600, acc: 0.875\n",
      "==>>> epoch: 487, batch index: 6, train loss: 2.199420\n",
      "==>>> epoch: 487, batch index: 1, test loss: 0.422928, acc: 0.900\n",
      "==>>> epoch: 488, batch index: 6, train loss: 2.202490\n",
      "==>>> epoch: 488, batch index: 1, test loss: 0.420615, acc: 0.900\n",
      "==>>> epoch: 489, batch index: 6, train loss: 2.192471\n",
      "==>>> epoch: 489, batch index: 1, test loss: 0.425577, acc: 0.875\n",
      "==>>> epoch: 490, batch index: 6, train loss: 2.266571\n",
      "==>>> epoch: 490, batch index: 1, test loss: 0.421864, acc: 0.900\n",
      "==>>> epoch: 491, batch index: 6, train loss: 2.192566\n",
      "==>>> epoch: 491, batch index: 1, test loss: 0.430100, acc: 0.875\n",
      "==>>> epoch: 492, batch index: 6, train loss: 2.239260\n",
      "==>>> epoch: 492, batch index: 1, test loss: 0.401540, acc: 0.925\n",
      "==>>> epoch: 493, batch index: 6, train loss: 2.169446\n",
      "==>>> epoch: 493, batch index: 1, test loss: 0.448584, acc: 0.875\n",
      "==>>> epoch: 494, batch index: 6, train loss: 2.218175\n",
      "==>>> epoch: 494, batch index: 1, test loss: 0.413170, acc: 0.900\n",
      "==>>> epoch: 495, batch index: 6, train loss: 2.222501\n",
      "==>>> epoch: 495, batch index: 1, test loss: 0.423788, acc: 0.900\n",
      "==>>> epoch: 496, batch index: 6, train loss: 2.245728\n",
      "==>>> epoch: 496, batch index: 1, test loss: 0.416806, acc: 0.900\n",
      "==>>> epoch: 497, batch index: 6, train loss: 2.233569\n",
      "==>>> epoch: 497, batch index: 1, test loss: 0.423299, acc: 0.900\n",
      "==>>> epoch: 498, batch index: 6, train loss: 2.204754\n",
      "==>>> epoch: 498, batch index: 1, test loss: 0.427250, acc: 0.900\n",
      "==>>> epoch: 499, batch index: 6, train loss: 2.203540\n",
      "==>>> epoch: 499, batch index: 1, test loss: 0.431778, acc: 0.875\n",
      "==>>> epoch: 500, batch index: 6, train loss: 2.235418\n",
      "==>>> epoch: 500, batch index: 1, test loss: 0.417073, acc: 0.900\n",
      "==>>> epoch: 501, batch index: 6, train loss: 2.266005\n",
      "==>>> epoch: 501, batch index: 1, test loss: 0.426485, acc: 0.900\n",
      "==>>> epoch: 502, batch index: 6, train loss: 2.209036\n",
      "==>>> epoch: 502, batch index: 1, test loss: 0.416626, acc: 0.900\n",
      "==>>> epoch: 503, batch index: 6, train loss: 2.239388\n",
      "==>>> epoch: 503, batch index: 1, test loss: 0.428932, acc: 0.900\n",
      "==>>> epoch: 504, batch index: 6, train loss: 2.205105\n",
      "==>>> epoch: 504, batch index: 1, test loss: 0.430653, acc: 0.875\n",
      "==>>> epoch: 505, batch index: 6, train loss: 2.219509\n",
      "==>>> epoch: 505, batch index: 1, test loss: 0.423367, acc: 0.900\n",
      "==>>> epoch: 506, batch index: 6, train loss: 2.203813\n",
      "==>>> epoch: 506, batch index: 1, test loss: 0.426873, acc: 0.900\n",
      "==>>> epoch: 507, batch index: 6, train loss: 2.224167\n",
      "==>>> epoch: 507, batch index: 1, test loss: 0.453660, acc: 0.850\n",
      "==>>> epoch: 508, batch index: 6, train loss: 2.220392\n",
      "==>>> epoch: 508, batch index: 1, test loss: 0.420380, acc: 0.900\n",
      "==>>> epoch: 509, batch index: 6, train loss: 2.204503\n",
      "==>>> epoch: 509, batch index: 1, test loss: 0.430434, acc: 0.875\n",
      "==>>> epoch: 510, batch index: 6, train loss: 2.241966\n",
      "==>>> epoch: 510, batch index: 1, test loss: 0.427736, acc: 0.875\n",
      "==>>> epoch: 511, batch index: 6, train loss: 2.192682\n",
      "==>>> epoch: 511, batch index: 1, test loss: 0.422831, acc: 0.900\n",
      "==>>> epoch: 512, batch index: 6, train loss: 2.229538\n",
      "==>>> epoch: 512, batch index: 1, test loss: 0.420538, acc: 0.900\n",
      "==>>> epoch: 513, batch index: 6, train loss: 2.288968\n",
      "==>>> epoch: 513, batch index: 1, test loss: 0.420812, acc: 0.900\n",
      "==>>> epoch: 514, batch index: 6, train loss: 2.199158\n",
      "==>>> epoch: 514, batch index: 1, test loss: 0.420329, acc: 0.900\n",
      "==>>> epoch: 515, batch index: 6, train loss: 2.222071\n",
      "==>>> epoch: 515, batch index: 1, test loss: 0.425648, acc: 0.900\n",
      "==>>> epoch: 516, batch index: 6, train loss: 2.196185\n",
      "==>>> epoch: 516, batch index: 1, test loss: 0.443824, acc: 0.875\n",
      "==>>> epoch: 517, batch index: 6, train loss: 2.195871\n",
      "==>>> epoch: 517, batch index: 1, test loss: 0.415736, acc: 0.900\n",
      "==>>> epoch: 518, batch index: 6, train loss: 2.255498\n",
      "==>>> epoch: 518, batch index: 1, test loss: 0.435073, acc: 0.875\n",
      "==>>> epoch: 519, batch index: 6, train loss: 2.191468\n",
      "==>>> epoch: 519, batch index: 1, test loss: 0.414632, acc: 0.900\n",
      "==>>> epoch: 520, batch index: 6, train loss: 2.227919\n",
      "==>>> epoch: 520, batch index: 1, test loss: 0.425228, acc: 0.900\n",
      "==>>> epoch: 521, batch index: 6, train loss: 2.195055\n",
      "==>>> epoch: 521, batch index: 1, test loss: 0.435456, acc: 0.875\n",
      "==>>> epoch: 522, batch index: 6, train loss: 2.213299\n",
      "==>>> epoch: 522, batch index: 1, test loss: 0.423246, acc: 0.900\n",
      "==>>> epoch: 523, batch index: 6, train loss: 2.193032\n",
      "==>>> epoch: 523, batch index: 1, test loss: 0.418584, acc: 0.900\n",
      "==>>> epoch: 524, batch index: 6, train loss: 2.202802\n",
      "==>>> epoch: 524, batch index: 1, test loss: 0.437854, acc: 0.875\n",
      "==>>> epoch: 525, batch index: 6, train loss: 2.229957\n",
      "==>>> epoch: 525, batch index: 1, test loss: 0.416979, acc: 0.900\n",
      "==>>> epoch: 526, batch index: 6, train loss: 2.266788\n",
      "==>>> epoch: 526, batch index: 1, test loss: 0.425968, acc: 0.900\n",
      "==>>> epoch: 527, batch index: 6, train loss: 2.207382\n",
      "==>>> epoch: 527, batch index: 1, test loss: 0.431027, acc: 0.875\n",
      "==>>> epoch: 528, batch index: 6, train loss: 2.206897\n",
      "==>>> epoch: 528, batch index: 1, test loss: 0.433349, acc: 0.875\n",
      "==>>> epoch: 529, batch index: 6, train loss: 2.185238\n",
      "==>>> epoch: 529, batch index: 1, test loss: 0.421710, acc: 0.900\n",
      "==>>> epoch: 530, batch index: 6, train loss: 2.228854\n",
      "==>>> epoch: 530, batch index: 1, test loss: 0.429996, acc: 0.875\n",
      "==>>> epoch: 531, batch index: 6, train loss: 2.217841\n",
      "==>>> epoch: 531, batch index: 1, test loss: 0.448687, acc: 0.850\n",
      "==>>> epoch: 532, batch index: 6, train loss: 2.244234\n",
      "==>>> epoch: 532, batch index: 1, test loss: 0.415648, acc: 0.900\n",
      "==>>> epoch: 533, batch index: 6, train loss: 2.204586\n",
      "==>>> epoch: 533, batch index: 1, test loss: 0.425275, acc: 0.875\n",
      "==>>> epoch: 534, batch index: 6, train loss: 2.219872\n",
      "==>>> epoch: 534, batch index: 1, test loss: 0.423858, acc: 0.900\n",
      "==>>> epoch: 535, batch index: 6, train loss: 2.197486\n",
      "==>>> epoch: 535, batch index: 1, test loss: 0.408588, acc: 0.900\n",
      "==>>> epoch: 536, batch index: 6, train loss: 2.221487\n",
      "==>>> epoch: 536, batch index: 1, test loss: 0.435676, acc: 0.875\n",
      "==>>> epoch: 537, batch index: 6, train loss: 2.190824\n",
      "==>>> epoch: 537, batch index: 1, test loss: 0.437744, acc: 0.875\n",
      "==>>> epoch: 538, batch index: 6, train loss: 2.283631\n",
      "==>>> epoch: 538, batch index: 1, test loss: 0.457144, acc: 0.850\n",
      "==>>> epoch: 539, batch index: 6, train loss: 2.219508\n",
      "==>>> epoch: 539, batch index: 1, test loss: 0.421712, acc: 0.900\n",
      "==>>> epoch: 540, batch index: 6, train loss: 2.205611\n",
      "==>>> epoch: 540, batch index: 1, test loss: 0.425364, acc: 0.900\n",
      "==>>> epoch: 541, batch index: 6, train loss: 2.226501\n",
      "==>>> epoch: 541, batch index: 1, test loss: 0.418997, acc: 0.900\n",
      "==>>> epoch: 542, batch index: 6, train loss: 2.227786\n",
      "==>>> epoch: 542, batch index: 1, test loss: 0.428202, acc: 0.900\n",
      "==>>> epoch: 543, batch index: 6, train loss: 2.193124\n",
      "==>>> epoch: 543, batch index: 1, test loss: 0.414721, acc: 0.900\n",
      "==>>> epoch: 544, batch index: 6, train loss: 2.222449\n",
      "==>>> epoch: 544, batch index: 1, test loss: 0.432613, acc: 0.875\n",
      "==>>> epoch: 545, batch index: 6, train loss: 2.180106\n",
      "==>>> epoch: 545, batch index: 1, test loss: 0.406297, acc: 0.925\n",
      "==>>> epoch: 546, batch index: 6, train loss: 2.197623\n",
      "==>>> epoch: 546, batch index: 1, test loss: 0.433249, acc: 0.875\n",
      "==>>> epoch: 547, batch index: 6, train loss: 2.204925\n",
      "==>>> epoch: 547, batch index: 1, test loss: 0.427824, acc: 0.875\n",
      "==>>> epoch: 548, batch index: 6, train loss: 2.209335\n",
      "==>>> epoch: 548, batch index: 1, test loss: 0.428322, acc: 0.875\n",
      "==>>> epoch: 549, batch index: 6, train loss: 2.195988\n",
      "==>>> epoch: 549, batch index: 1, test loss: 0.415878, acc: 0.900\n",
      "==>>> epoch: 550, batch index: 6, train loss: 2.285344\n",
      "==>>> epoch: 550, batch index: 1, test loss: 0.421473, acc: 0.900\n",
      "==>>> epoch: 551, batch index: 6, train loss: 2.223127\n",
      "==>>> epoch: 551, batch index: 1, test loss: 0.430155, acc: 0.875\n",
      "==>>> epoch: 552, batch index: 6, train loss: 2.241931\n",
      "==>>> epoch: 552, batch index: 1, test loss: 0.432932, acc: 0.875\n",
      "==>>> epoch: 553, batch index: 6, train loss: 2.181703\n",
      "==>>> epoch: 553, batch index: 1, test loss: 0.406083, acc: 0.900\n",
      "==>>> epoch: 554, batch index: 6, train loss: 2.182251\n",
      "==>>> epoch: 554, batch index: 1, test loss: 0.431395, acc: 0.875\n",
      "==>>> epoch: 555, batch index: 6, train loss: 2.207138\n",
      "==>>> epoch: 555, batch index: 1, test loss: 0.411927, acc: 0.900\n",
      "==>>> epoch: 556, batch index: 6, train loss: 2.210815\n",
      "==>>> epoch: 556, batch index: 1, test loss: 0.425972, acc: 0.875\n",
      "==>>> epoch: 557, batch index: 6, train loss: 2.195538\n",
      "==>>> epoch: 557, batch index: 1, test loss: 0.435225, acc: 0.875\n",
      "==>>> epoch: 558, batch index: 6, train loss: 2.190045\n",
      "==>>> epoch: 558, batch index: 1, test loss: 0.436850, acc: 0.875\n",
      "==>>> epoch: 559, batch index: 6, train loss: 2.163331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 559, batch index: 1, test loss: 0.392837, acc: 0.925\n",
      "==>>> epoch: 560, batch index: 6, train loss: 2.275227\n",
      "==>>> epoch: 560, batch index: 1, test loss: 0.398495, acc: 0.925\n",
      "==>>> epoch: 561, batch index: 6, train loss: 2.222604\n",
      "==>>> epoch: 561, batch index: 1, test loss: 0.402055, acc: 0.925\n",
      "==>>> epoch: 562, batch index: 6, train loss: 2.221125\n",
      "==>>> epoch: 562, batch index: 1, test loss: 0.421920, acc: 0.900\n",
      "==>>> epoch: 563, batch index: 6, train loss: 2.204078\n",
      "==>>> epoch: 563, batch index: 1, test loss: 0.420299, acc: 0.900\n",
      "==>>> epoch: 564, batch index: 6, train loss: 2.215148\n",
      "==>>> epoch: 564, batch index: 1, test loss: 0.421424, acc: 0.900\n",
      "==>>> epoch: 565, batch index: 6, train loss: 2.245944\n",
      "==>>> epoch: 565, batch index: 1, test loss: 0.401527, acc: 0.925\n",
      "==>>> epoch: 566, batch index: 6, train loss: 2.175193\n",
      "==>>> epoch: 566, batch index: 1, test loss: 0.395942, acc: 0.925\n",
      "==>>> epoch: 567, batch index: 6, train loss: 2.209549\n",
      "==>>> epoch: 567, batch index: 1, test loss: 0.428472, acc: 0.875\n",
      "==>>> epoch: 568, batch index: 6, train loss: 2.198182\n",
      "==>>> epoch: 568, batch index: 1, test loss: 0.421675, acc: 0.900\n",
      "==>>> epoch: 569, batch index: 6, train loss: 2.222791\n",
      "==>>> epoch: 569, batch index: 1, test loss: 0.416001, acc: 0.900\n",
      "==>>> epoch: 570, batch index: 6, train loss: 2.218796\n",
      "==>>> epoch: 570, batch index: 1, test loss: 0.412044, acc: 0.900\n",
      "==>>> epoch: 571, batch index: 6, train loss: 2.240390\n",
      "==>>> epoch: 571, batch index: 1, test loss: 0.420141, acc: 0.900\n",
      "==>>> epoch: 572, batch index: 6, train loss: 2.222717\n",
      "==>>> epoch: 572, batch index: 1, test loss: 0.423829, acc: 0.900\n",
      "==>>> epoch: 573, batch index: 6, train loss: 2.221294\n",
      "==>>> epoch: 573, batch index: 1, test loss: 0.423381, acc: 0.900\n",
      "==>>> epoch: 574, batch index: 6, train loss: 2.248525\n",
      "==>>> epoch: 574, batch index: 1, test loss: 0.426907, acc: 0.900\n",
      "==>>> epoch: 575, batch index: 6, train loss: 2.217725\n",
      "==>>> epoch: 575, batch index: 1, test loss: 0.433205, acc: 0.875\n",
      "==>>> epoch: 576, batch index: 6, train loss: 2.193869\n",
      "==>>> epoch: 576, batch index: 1, test loss: 0.423051, acc: 0.900\n",
      "==>>> epoch: 577, batch index: 6, train loss: 2.199120\n",
      "==>>> epoch: 577, batch index: 1, test loss: 0.415852, acc: 0.900\n",
      "==>>> epoch: 578, batch index: 6, train loss: 2.256039\n",
      "==>>> epoch: 578, batch index: 1, test loss: 0.424065, acc: 0.900\n",
      "==>>> epoch: 579, batch index: 6, train loss: 2.223782\n",
      "==>>> epoch: 579, batch index: 1, test loss: 0.413181, acc: 0.900\n",
      "==>>> epoch: 580, batch index: 6, train loss: 2.270588\n",
      "==>>> epoch: 580, batch index: 1, test loss: 0.440064, acc: 0.875\n",
      "==>>> epoch: 581, batch index: 6, train loss: 2.203013\n",
      "==>>> epoch: 581, batch index: 1, test loss: 0.423003, acc: 0.900\n",
      "==>>> epoch: 582, batch index: 6, train loss: 2.172630\n",
      "==>>> epoch: 582, batch index: 1, test loss: 0.412616, acc: 0.900\n",
      "==>>> epoch: 583, batch index: 6, train loss: 2.199378\n",
      "==>>> epoch: 583, batch index: 1, test loss: 0.417132, acc: 0.900\n",
      "==>>> epoch: 584, batch index: 6, train loss: 2.227548\n",
      "==>>> epoch: 584, batch index: 1, test loss: 0.426580, acc: 0.875\n",
      "==>>> epoch: 585, batch index: 6, train loss: 2.193551\n",
      "==>>> epoch: 585, batch index: 1, test loss: 0.417002, acc: 0.900\n",
      "==>>> epoch: 586, batch index: 6, train loss: 2.206463\n",
      "==>>> epoch: 586, batch index: 1, test loss: 0.418300, acc: 0.900\n",
      "==>>> epoch: 587, batch index: 6, train loss: 2.209047\n",
      "==>>> epoch: 587, batch index: 1, test loss: 0.425025, acc: 0.900\n",
      "==>>> epoch: 588, batch index: 6, train loss: 2.238332\n",
      "==>>> epoch: 588, batch index: 1, test loss: 0.452553, acc: 0.850\n",
      "==>>> epoch: 589, batch index: 6, train loss: 2.222239\n",
      "==>>> epoch: 589, batch index: 1, test loss: 0.434717, acc: 0.875\n",
      "==>>> epoch: 590, batch index: 6, train loss: 2.191970\n",
      "==>>> epoch: 590, batch index: 1, test loss: 0.422975, acc: 0.900\n",
      "==>>> epoch: 591, batch index: 6, train loss: 2.166494\n",
      "==>>> epoch: 591, batch index: 1, test loss: 0.415377, acc: 0.900\n",
      "==>>> epoch: 592, batch index: 6, train loss: 2.177250\n",
      "==>>> epoch: 592, batch index: 1, test loss: 0.420785, acc: 0.900\n",
      "==>>> epoch: 593, batch index: 6, train loss: 2.211627\n",
      "==>>> epoch: 593, batch index: 1, test loss: 0.403175, acc: 0.925\n",
      "==>>> epoch: 594, batch index: 6, train loss: 2.197781\n",
      "==>>> epoch: 594, batch index: 1, test loss: 0.424310, acc: 0.900\n",
      "==>>> epoch: 595, batch index: 6, train loss: 2.200284\n",
      "==>>> epoch: 595, batch index: 1, test loss: 0.435242, acc: 0.875\n",
      "==>>> epoch: 596, batch index: 6, train loss: 2.182505\n",
      "==>>> epoch: 596, batch index: 1, test loss: 0.424365, acc: 0.900\n",
      "==>>> epoch: 597, batch index: 6, train loss: 2.216172\n",
      "==>>> epoch: 597, batch index: 1, test loss: 0.430923, acc: 0.875\n",
      "==>>> epoch: 598, batch index: 6, train loss: 2.213407\n",
      "==>>> epoch: 598, batch index: 1, test loss: 0.448735, acc: 0.850\n",
      "==>>> epoch: 599, batch index: 6, train loss: 2.213094\n",
      "==>>> epoch: 599, batch index: 1, test loss: 0.422105, acc: 0.900\n",
      "==>>> epoch: 600, batch index: 6, train loss: 2.199957\n",
      "==>>> epoch: 600, batch index: 1, test loss: 0.423825, acc: 0.900\n",
      "==>>> epoch: 601, batch index: 6, train loss: 2.205646\n",
      "==>>> epoch: 601, batch index: 1, test loss: 0.413757, acc: 0.900\n",
      "==>>> epoch: 602, batch index: 6, train loss: 2.180731\n",
      "==>>> epoch: 602, batch index: 1, test loss: 0.409916, acc: 0.900\n",
      "==>>> epoch: 603, batch index: 6, train loss: 2.211387\n",
      "==>>> epoch: 603, batch index: 1, test loss: 0.419556, acc: 0.900\n",
      "==>>> epoch: 604, batch index: 6, train loss: 2.191341\n",
      "==>>> epoch: 604, batch index: 1, test loss: 0.430603, acc: 0.875\n",
      "==>>> epoch: 605, batch index: 6, train loss: 2.232387\n",
      "==>>> epoch: 605, batch index: 1, test loss: 0.425040, acc: 0.900\n",
      "==>>> epoch: 606, batch index: 6, train loss: 2.213207\n",
      "==>>> epoch: 606, batch index: 1, test loss: 0.437746, acc: 0.875\n",
      "==>>> epoch: 607, batch index: 6, train loss: 2.227633\n",
      "==>>> epoch: 607, batch index: 1, test loss: 0.417283, acc: 0.900\n",
      "==>>> epoch: 608, batch index: 6, train loss: 2.199373\n",
      "==>>> epoch: 608, batch index: 1, test loss: 0.418344, acc: 0.900\n",
      "==>>> epoch: 609, batch index: 6, train loss: 2.202942\n",
      "==>>> epoch: 609, batch index: 1, test loss: 0.429526, acc: 0.875\n",
      "==>>> epoch: 610, batch index: 6, train loss: 2.197980\n",
      "==>>> epoch: 610, batch index: 1, test loss: 0.437119, acc: 0.875\n",
      "==>>> epoch: 611, batch index: 6, train loss: 2.218764\n",
      "==>>> epoch: 611, batch index: 1, test loss: 0.423503, acc: 0.900\n",
      "==>>> epoch: 612, batch index: 6, train loss: 2.212367\n",
      "==>>> epoch: 612, batch index: 1, test loss: 0.421808, acc: 0.900\n",
      "==>>> epoch: 613, batch index: 6, train loss: 2.211079\n",
      "==>>> epoch: 613, batch index: 1, test loss: 0.436063, acc: 0.875\n",
      "==>>> epoch: 614, batch index: 6, train loss: 2.193727\n",
      "==>>> epoch: 614, batch index: 1, test loss: 0.419889, acc: 0.900\n",
      "==>>> epoch: 615, batch index: 6, train loss: 2.192905\n",
      "==>>> epoch: 615, batch index: 1, test loss: 0.421576, acc: 0.900\n",
      "==>>> epoch: 616, batch index: 6, train loss: 2.220149\n",
      "==>>> epoch: 616, batch index: 1, test loss: 0.426402, acc: 0.875\n",
      "==>>> epoch: 617, batch index: 6, train loss: 2.207230\n",
      "==>>> epoch: 617, batch index: 1, test loss: 0.418045, acc: 0.900\n",
      "==>>> epoch: 618, batch index: 6, train loss: 2.250222\n",
      "==>>> epoch: 618, batch index: 1, test loss: 0.433986, acc: 0.875\n",
      "==>>> epoch: 619, batch index: 6, train loss: 2.183995\n",
      "==>>> epoch: 619, batch index: 1, test loss: 0.411119, acc: 0.900\n",
      "==>>> epoch: 620, batch index: 6, train loss: 2.218017\n",
      "==>>> epoch: 620, batch index: 1, test loss: 0.423271, acc: 0.900\n",
      "==>>> epoch: 621, batch index: 6, train loss: 2.168384\n",
      "==>>> epoch: 621, batch index: 1, test loss: 0.402391, acc: 0.925\n",
      "==>>> epoch: 622, batch index: 6, train loss: 2.178666\n",
      "==>>> epoch: 622, batch index: 1, test loss: 0.430391, acc: 0.875\n",
      "==>>> epoch: 623, batch index: 6, train loss: 2.188676\n",
      "==>>> epoch: 623, batch index: 1, test loss: 0.408316, acc: 0.900\n",
      "==>>> epoch: 624, batch index: 6, train loss: 2.220954\n",
      "==>>> epoch: 624, batch index: 1, test loss: 0.422404, acc: 0.900\n",
      "==>>> epoch: 625, batch index: 6, train loss: 2.208189\n",
      "==>>> epoch: 625, batch index: 1, test loss: 0.441606, acc: 0.875\n",
      "==>>> epoch: 626, batch index: 6, train loss: 2.188815\n",
      "==>>> epoch: 626, batch index: 1, test loss: 0.428495, acc: 0.875\n",
      "==>>> epoch: 627, batch index: 6, train loss: 2.187213\n",
      "==>>> epoch: 627, batch index: 1, test loss: 0.415007, acc: 0.900\n",
      "==>>> epoch: 628, batch index: 6, train loss: 2.156692\n",
      "==>>> epoch: 628, batch index: 1, test loss: 0.434978, acc: 0.875\n",
      "==>>> epoch: 629, batch index: 6, train loss: 2.177330\n",
      "==>>> epoch: 629, batch index: 1, test loss: 0.424729, acc: 0.875\n",
      "==>>> epoch: 630, batch index: 6, train loss: 2.174736\n",
      "==>>> epoch: 630, batch index: 1, test loss: 0.429117, acc: 0.875\n",
      "==>>> epoch: 631, batch index: 6, train loss: 2.217166\n",
      "==>>> epoch: 631, batch index: 1, test loss: 0.427023, acc: 0.875\n",
      "==>>> epoch: 632, batch index: 6, train loss: 2.180537\n",
      "==>>> epoch: 632, batch index: 1, test loss: 0.416313, acc: 0.900\n",
      "==>>> epoch: 633, batch index: 6, train loss: 2.185606\n",
      "==>>> epoch: 633, batch index: 1, test loss: 0.438156, acc: 0.875\n",
      "==>>> epoch: 634, batch index: 6, train loss: 2.175542\n",
      "==>>> epoch: 634, batch index: 1, test loss: 0.416612, acc: 0.900\n",
      "==>>> epoch: 635, batch index: 6, train loss: 2.164000\n",
      "==>>> epoch: 635, batch index: 1, test loss: 0.400609, acc: 0.925\n",
      "==>>> epoch: 636, batch index: 6, train loss: 2.177546\n",
      "==>>> epoch: 636, batch index: 1, test loss: 0.437205, acc: 0.875\n",
      "==>>> epoch: 637, batch index: 6, train loss: 2.256962\n",
      "==>>> epoch: 637, batch index: 1, test loss: 0.427457, acc: 0.900\n",
      "==>>> epoch: 638, batch index: 6, train loss: 2.196370\n",
      "==>>> epoch: 638, batch index: 1, test loss: 0.423398, acc: 0.900\n",
      "==>>> epoch: 639, batch index: 6, train loss: 2.223509\n",
      "==>>> epoch: 639, batch index: 1, test loss: 0.434723, acc: 0.875\n",
      "==>>> epoch: 640, batch index: 6, train loss: 2.221276\n",
      "==>>> epoch: 640, batch index: 1, test loss: 0.438686, acc: 0.875\n",
      "==>>> epoch: 641, batch index: 6, train loss: 2.195956\n",
      "==>>> epoch: 641, batch index: 1, test loss: 0.425491, acc: 0.875\n",
      "==>>> epoch: 642, batch index: 6, train loss: 2.180267\n",
      "==>>> epoch: 642, batch index: 1, test loss: 0.413017, acc: 0.900\n",
      "==>>> epoch: 643, batch index: 6, train loss: 2.148846\n",
      "==>>> epoch: 643, batch index: 1, test loss: 0.442449, acc: 0.875\n",
      "==>>> epoch: 644, batch index: 6, train loss: 2.195710\n",
      "==>>> epoch: 644, batch index: 1, test loss: 0.445512, acc: 0.875\n",
      "==>>> epoch: 645, batch index: 6, train loss: 2.191473\n",
      "==>>> epoch: 645, batch index: 1, test loss: 0.439303, acc: 0.875\n",
      "==>>> epoch: 646, batch index: 6, train loss: 2.168121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 646, batch index: 1, test loss: 0.421407, acc: 0.900\n",
      "==>>> epoch: 647, batch index: 6, train loss: 2.217253\n",
      "==>>> epoch: 647, batch index: 1, test loss: 0.433890, acc: 0.875\n",
      "==>>> epoch: 648, batch index: 6, train loss: 2.237900\n",
      "==>>> epoch: 648, batch index: 1, test loss: 0.427568, acc: 0.875\n",
      "==>>> epoch: 649, batch index: 6, train loss: 2.175895\n",
      "==>>> epoch: 649, batch index: 1, test loss: 0.417437, acc: 0.900\n",
      "==>>> epoch: 650, batch index: 6, train loss: 2.229316\n",
      "==>>> epoch: 650, batch index: 1, test loss: 0.404745, acc: 0.900\n",
      "==>>> epoch: 651, batch index: 6, train loss: 2.268131\n",
      "==>>> epoch: 651, batch index: 1, test loss: 0.412131, acc: 0.900\n",
      "==>>> epoch: 652, batch index: 6, train loss: 2.238049\n",
      "==>>> epoch: 652, batch index: 1, test loss: 0.443774, acc: 0.875\n",
      "==>>> epoch: 653, batch index: 6, train loss: 2.195491\n",
      "==>>> epoch: 653, batch index: 1, test loss: 0.430953, acc: 0.875\n",
      "==>>> epoch: 654, batch index: 6, train loss: 2.167320\n",
      "==>>> epoch: 654, batch index: 1, test loss: 0.414985, acc: 0.900\n",
      "==>>> epoch: 655, batch index: 6, train loss: 2.194656\n",
      "==>>> epoch: 655, batch index: 1, test loss: 0.436967, acc: 0.875\n",
      "==>>> epoch: 656, batch index: 6, train loss: 2.249188\n",
      "==>>> epoch: 656, batch index: 1, test loss: 0.435220, acc: 0.875\n",
      "==>>> epoch: 657, batch index: 6, train loss: 2.210043\n",
      "==>>> epoch: 657, batch index: 1, test loss: 0.407146, acc: 0.900\n",
      "==>>> epoch: 658, batch index: 6, train loss: 2.188226\n",
      "==>>> epoch: 658, batch index: 1, test loss: 0.411274, acc: 0.900\n",
      "==>>> epoch: 659, batch index: 6, train loss: 2.209461\n",
      "==>>> epoch: 659, batch index: 1, test loss: 0.405133, acc: 0.900\n",
      "==>>> epoch: 660, batch index: 6, train loss: 2.153553\n",
      "==>>> epoch: 660, batch index: 1, test loss: 0.431268, acc: 0.875\n",
      "==>>> epoch: 661, batch index: 6, train loss: 2.186324\n",
      "==>>> epoch: 661, batch index: 1, test loss: 0.425567, acc: 0.875\n",
      "==>>> epoch: 662, batch index: 6, train loss: 2.214860\n",
      "==>>> epoch: 662, batch index: 1, test loss: 0.430053, acc: 0.875\n",
      "==>>> epoch: 663, batch index: 6, train loss: 2.201481\n",
      "==>>> epoch: 663, batch index: 1, test loss: 0.422923, acc: 0.900\n",
      "==>>> epoch: 664, batch index: 6, train loss: 2.219767\n",
      "==>>> epoch: 664, batch index: 1, test loss: 0.442598, acc: 0.875\n",
      "==>>> epoch: 665, batch index: 6, train loss: 2.176303\n",
      "==>>> epoch: 665, batch index: 1, test loss: 0.421056, acc: 0.900\n",
      "==>>> epoch: 666, batch index: 6, train loss: 2.202317\n",
      "==>>> epoch: 666, batch index: 1, test loss: 0.425832, acc: 0.875\n",
      "==>>> epoch: 667, batch index: 6, train loss: 2.211481\n",
      "==>>> epoch: 667, batch index: 1, test loss: 0.442444, acc: 0.875\n",
      "==>>> epoch: 668, batch index: 6, train loss: 2.200791\n",
      "==>>> epoch: 668, batch index: 1, test loss: 0.436316, acc: 0.875\n",
      "==>>> epoch: 669, batch index: 6, train loss: 2.215605\n",
      "==>>> epoch: 669, batch index: 1, test loss: 0.433447, acc: 0.875\n",
      "==>>> epoch: 670, batch index: 6, train loss: 2.216096\n",
      "==>>> epoch: 670, batch index: 1, test loss: 0.436298, acc: 0.875\n",
      "==>>> epoch: 671, batch index: 6, train loss: 2.212184\n",
      "==>>> epoch: 671, batch index: 1, test loss: 0.404951, acc: 0.900\n",
      "==>>> epoch: 672, batch index: 6, train loss: 2.240941\n",
      "==>>> epoch: 672, batch index: 1, test loss: 0.430456, acc: 0.875\n",
      "==>>> epoch: 673, batch index: 6, train loss: 2.168251\n",
      "==>>> epoch: 673, batch index: 1, test loss: 0.407235, acc: 0.900\n",
      "==>>> epoch: 674, batch index: 6, train loss: 2.241472\n",
      "==>>> epoch: 674, batch index: 1, test loss: 0.433180, acc: 0.875\n",
      "==>>> epoch: 675, batch index: 6, train loss: 2.168164\n",
      "==>>> epoch: 675, batch index: 1, test loss: 0.409638, acc: 0.900\n",
      "==>>> epoch: 676, batch index: 6, train loss: 2.167917\n",
      "==>>> epoch: 676, batch index: 1, test loss: 0.432702, acc: 0.875\n",
      "==>>> epoch: 677, batch index: 6, train loss: 2.194174\n",
      "==>>> epoch: 677, batch index: 1, test loss: 0.427795, acc: 0.875\n",
      "==>>> epoch: 678, batch index: 6, train loss: 2.188970\n",
      "==>>> epoch: 678, batch index: 1, test loss: 0.418852, acc: 0.900\n",
      "==>>> epoch: 679, batch index: 6, train loss: 2.226214\n",
      "==>>> epoch: 679, batch index: 1, test loss: 0.428580, acc: 0.875\n",
      "==>>> epoch: 680, batch index: 6, train loss: 2.227483\n",
      "==>>> epoch: 680, batch index: 1, test loss: 0.426559, acc: 0.875\n",
      "==>>> epoch: 681, batch index: 6, train loss: 2.246684\n",
      "==>>> epoch: 681, batch index: 1, test loss: 0.428832, acc: 0.875\n",
      "==>>> epoch: 682, batch index: 6, train loss: 2.162635\n",
      "==>>> epoch: 682, batch index: 1, test loss: 0.397882, acc: 0.925\n",
      "==>>> epoch: 683, batch index: 6, train loss: 2.199559\n",
      "==>>> epoch: 683, batch index: 1, test loss: 0.415739, acc: 0.900\n",
      "==>>> epoch: 684, batch index: 6, train loss: 2.244757\n",
      "==>>> epoch: 684, batch index: 1, test loss: 0.430890, acc: 0.875\n",
      "==>>> epoch: 685, batch index: 6, train loss: 2.205341\n",
      "==>>> epoch: 685, batch index: 1, test loss: 0.425920, acc: 0.875\n",
      "==>>> epoch: 686, batch index: 6, train loss: 2.199400\n",
      "==>>> epoch: 686, batch index: 1, test loss: 0.428307, acc: 0.875\n",
      "==>>> epoch: 687, batch index: 6, train loss: 2.199191\n",
      "==>>> epoch: 687, batch index: 1, test loss: 0.436128, acc: 0.875\n",
      "==>>> epoch: 688, batch index: 6, train loss: 2.179660\n",
      "==>>> epoch: 688, batch index: 1, test loss: 0.417305, acc: 0.900\n",
      "==>>> epoch: 689, batch index: 6, train loss: 2.176013\n",
      "==>>> epoch: 689, batch index: 1, test loss: 0.429064, acc: 0.875\n",
      "==>>> epoch: 690, batch index: 6, train loss: 2.159646\n",
      "==>>> epoch: 690, batch index: 1, test loss: 0.400282, acc: 0.925\n",
      "==>>> epoch: 691, batch index: 6, train loss: 2.177806\n",
      "==>>> epoch: 691, batch index: 1, test loss: 0.432908, acc: 0.875\n",
      "==>>> epoch: 692, batch index: 6, train loss: 2.188359\n",
      "==>>> epoch: 692, batch index: 1, test loss: 0.409664, acc: 0.900\n",
      "==>>> epoch: 693, batch index: 6, train loss: 2.190805\n",
      "==>>> epoch: 693, batch index: 1, test loss: 0.398643, acc: 0.925\n",
      "==>>> epoch: 694, batch index: 6, train loss: 2.211777\n",
      "==>>> epoch: 694, batch index: 1, test loss: 0.422530, acc: 0.875\n",
      "==>>> epoch: 695, batch index: 6, train loss: 2.192014\n",
      "==>>> epoch: 695, batch index: 1, test loss: 0.450690, acc: 0.850\n",
      "==>>> epoch: 696, batch index: 6, train loss: 2.197868\n",
      "==>>> epoch: 696, batch index: 1, test loss: 0.421788, acc: 0.900\n",
      "==>>> epoch: 697, batch index: 6, train loss: 2.172214\n",
      "==>>> epoch: 697, batch index: 1, test loss: 0.431191, acc: 0.875\n",
      "==>>> epoch: 698, batch index: 6, train loss: 2.264403\n",
      "==>>> epoch: 698, batch index: 1, test loss: 0.450761, acc: 0.850\n",
      "==>>> epoch: 699, batch index: 6, train loss: 2.269410\n",
      "==>>> epoch: 699, batch index: 1, test loss: 0.385576, acc: 0.925\n",
      "==>>> epoch: 700, batch index: 6, train loss: 2.222485\n",
      "==>>> epoch: 700, batch index: 1, test loss: 0.418488, acc: 0.900\n",
      "==>>> epoch: 701, batch index: 6, train loss: 2.162318\n",
      "==>>> epoch: 701, batch index: 1, test loss: 0.400141, acc: 0.925\n",
      "==>>> epoch: 702, batch index: 6, train loss: 2.259409\n",
      "==>>> epoch: 702, batch index: 1, test loss: 0.417220, acc: 0.900\n",
      "==>>> epoch: 703, batch index: 6, train loss: 2.168991\n",
      "==>>> epoch: 703, batch index: 1, test loss: 0.403544, acc: 0.900\n",
      "==>>> epoch: 704, batch index: 6, train loss: 2.280265\n",
      "==>>> epoch: 704, batch index: 1, test loss: 0.418653, acc: 0.900\n",
      "==>>> epoch: 705, batch index: 6, train loss: 2.161401\n",
      "==>>> epoch: 705, batch index: 1, test loss: 0.423660, acc: 0.900\n",
      "==>>> epoch: 706, batch index: 6, train loss: 2.168826\n",
      "==>>> epoch: 706, batch index: 1, test loss: 0.413210, acc: 0.900\n",
      "==>>> epoch: 707, batch index: 6, train loss: 2.166299\n",
      "==>>> epoch: 707, batch index: 1, test loss: 0.433164, acc: 0.875\n",
      "==>>> epoch: 708, batch index: 6, train loss: 2.198695\n",
      "==>>> epoch: 708, batch index: 1, test loss: 0.434858, acc: 0.875\n",
      "==>>> epoch: 709, batch index: 6, train loss: 2.188619\n",
      "==>>> epoch: 709, batch index: 1, test loss: 0.426617, acc: 0.875\n",
      "==>>> epoch: 710, batch index: 6, train loss: 2.174170\n",
      "==>>> epoch: 710, batch index: 1, test loss: 0.406498, acc: 0.900\n",
      "==>>> epoch: 711, batch index: 6, train loss: 2.180859\n",
      "==>>> epoch: 711, batch index: 1, test loss: 0.432173, acc: 0.875\n",
      "==>>> epoch: 712, batch index: 6, train loss: 2.167750\n",
      "==>>> epoch: 712, batch index: 1, test loss: 0.414720, acc: 0.900\n",
      "==>>> epoch: 713, batch index: 6, train loss: 2.209671\n",
      "==>>> epoch: 713, batch index: 1, test loss: 0.408890, acc: 0.900\n",
      "==>>> epoch: 714, batch index: 6, train loss: 2.186848\n",
      "==>>> epoch: 714, batch index: 1, test loss: 0.431586, acc: 0.875\n",
      "==>>> epoch: 715, batch index: 6, train loss: 2.188874\n",
      "==>>> epoch: 715, batch index: 1, test loss: 0.435308, acc: 0.875\n",
      "==>>> epoch: 716, batch index: 6, train loss: 2.261877\n",
      "==>>> epoch: 716, batch index: 1, test loss: 0.455864, acc: 0.850\n",
      "==>>> epoch: 717, batch index: 6, train loss: 2.219451\n",
      "==>>> epoch: 717, batch index: 1, test loss: 0.424192, acc: 0.900\n",
      "==>>> epoch: 718, batch index: 6, train loss: 2.211645\n",
      "==>>> epoch: 718, batch index: 1, test loss: 0.429780, acc: 0.875\n",
      "==>>> epoch: 719, batch index: 6, train loss: 2.205866\n",
      "==>>> epoch: 719, batch index: 1, test loss: 0.427600, acc: 0.875\n",
      "==>>> epoch: 720, batch index: 6, train loss: 2.183884\n",
      "==>>> epoch: 720, batch index: 1, test loss: 0.416300, acc: 0.900\n",
      "==>>> epoch: 721, batch index: 6, train loss: 2.218296\n",
      "==>>> epoch: 721, batch index: 1, test loss: 0.408987, acc: 0.900\n",
      "==>>> epoch: 722, batch index: 6, train loss: 2.167085\n",
      "==>>> epoch: 722, batch index: 1, test loss: 0.438746, acc: 0.875\n",
      "==>>> epoch: 723, batch index: 6, train loss: 2.196124\n",
      "==>>> epoch: 723, batch index: 1, test loss: 0.437124, acc: 0.875\n",
      "==>>> epoch: 724, batch index: 6, train loss: 2.173294\n",
      "==>>> epoch: 724, batch index: 1, test loss: 0.417659, acc: 0.900\n",
      "==>>> epoch: 725, batch index: 6, train loss: 2.170727\n",
      "==>>> epoch: 725, batch index: 1, test loss: 0.440344, acc: 0.875\n",
      "==>>> epoch: 726, batch index: 6, train loss: 2.177697\n",
      "==>>> epoch: 726, batch index: 1, test loss: 0.439139, acc: 0.875\n",
      "==>>> epoch: 727, batch index: 6, train loss: 2.242882\n",
      "==>>> epoch: 727, batch index: 1, test loss: 0.432105, acc: 0.875\n",
      "==>>> epoch: 728, batch index: 6, train loss: 2.192786\n",
      "==>>> epoch: 728, batch index: 1, test loss: 0.452792, acc: 0.850\n",
      "==>>> epoch: 729, batch index: 6, train loss: 2.184617\n",
      "==>>> epoch: 729, batch index: 1, test loss: 0.409657, acc: 0.900\n",
      "==>>> epoch: 730, batch index: 6, train loss: 2.213499\n",
      "==>>> epoch: 730, batch index: 1, test loss: 0.432894, acc: 0.875\n",
      "==>>> epoch: 731, batch index: 6, train loss: 2.207610\n",
      "==>>> epoch: 731, batch index: 1, test loss: 0.429621, acc: 0.875\n",
      "==>>> epoch: 732, batch index: 6, train loss: 2.187109\n",
      "==>>> epoch: 732, batch index: 1, test loss: 0.422626, acc: 0.900\n",
      "==>>> epoch: 733, batch index: 6, train loss: 2.265992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 733, batch index: 1, test loss: 0.434063, acc: 0.875\n",
      "==>>> epoch: 734, batch index: 6, train loss: 2.213226\n",
      "==>>> epoch: 734, batch index: 1, test loss: 0.430789, acc: 0.875\n",
      "==>>> epoch: 735, batch index: 6, train loss: 2.173786\n",
      "==>>> epoch: 735, batch index: 1, test loss: 0.414704, acc: 0.900\n",
      "==>>> epoch: 736, batch index: 6, train loss: 2.179838\n",
      "==>>> epoch: 736, batch index: 1, test loss: 0.415018, acc: 0.900\n",
      "==>>> epoch: 737, batch index: 6, train loss: 2.215031\n",
      "==>>> epoch: 737, batch index: 1, test loss: 0.441113, acc: 0.875\n",
      "==>>> epoch: 738, batch index: 6, train loss: 2.185405\n",
      "==>>> epoch: 738, batch index: 1, test loss: 0.412470, acc: 0.900\n",
      "==>>> epoch: 739, batch index: 6, train loss: 2.159321\n",
      "==>>> epoch: 739, batch index: 1, test loss: 0.435037, acc: 0.875\n",
      "==>>> epoch: 740, batch index: 6, train loss: 2.251487\n",
      "==>>> epoch: 740, batch index: 1, test loss: 0.461324, acc: 0.850\n",
      "==>>> epoch: 741, batch index: 6, train loss: 2.224949\n",
      "==>>> epoch: 741, batch index: 1, test loss: 0.426975, acc: 0.875\n",
      "==>>> epoch: 742, batch index: 6, train loss: 2.186074\n",
      "==>>> epoch: 742, batch index: 1, test loss: 0.407917, acc: 0.900\n",
      "==>>> epoch: 743, batch index: 6, train loss: 2.210587\n",
      "==>>> epoch: 743, batch index: 1, test loss: 0.404576, acc: 0.900\n",
      "==>>> epoch: 744, batch index: 6, train loss: 2.227401\n",
      "==>>> epoch: 744, batch index: 1, test loss: 0.426737, acc: 0.875\n",
      "==>>> epoch: 745, batch index: 6, train loss: 2.173313\n",
      "==>>> epoch: 745, batch index: 1, test loss: 0.414287, acc: 0.900\n",
      "==>>> epoch: 746, batch index: 6, train loss: 2.152405\n",
      "==>>> epoch: 746, batch index: 1, test loss: 0.432126, acc: 0.875\n",
      "==>>> epoch: 747, batch index: 6, train loss: 2.196628\n",
      "==>>> epoch: 747, batch index: 1, test loss: 0.439777, acc: 0.875\n",
      "==>>> epoch: 748, batch index: 6, train loss: 2.196193\n",
      "==>>> epoch: 748, batch index: 1, test loss: 0.430801, acc: 0.875\n",
      "==>>> epoch: 749, batch index: 6, train loss: 2.177886\n",
      "==>>> epoch: 749, batch index: 1, test loss: 0.408657, acc: 0.900\n",
      "==>>> epoch: 750, batch index: 6, train loss: 2.166122\n",
      "==>>> epoch: 750, batch index: 1, test loss: 0.409517, acc: 0.900\n",
      "==>>> epoch: 751, batch index: 6, train loss: 2.189895\n",
      "==>>> epoch: 751, batch index: 1, test loss: 0.411643, acc: 0.900\n",
      "==>>> epoch: 752, batch index: 6, train loss: 2.219393\n",
      "==>>> epoch: 752, batch index: 1, test loss: 0.452338, acc: 0.850\n",
      "==>>> epoch: 753, batch index: 6, train loss: 2.214598\n",
      "==>>> epoch: 753, batch index: 1, test loss: 0.414255, acc: 0.900\n",
      "==>>> epoch: 754, batch index: 6, train loss: 2.227865\n",
      "==>>> epoch: 754, batch index: 1, test loss: 0.446757, acc: 0.850\n",
      "==>>> epoch: 755, batch index: 6, train loss: 2.189914\n",
      "==>>> epoch: 755, batch index: 1, test loss: 0.439399, acc: 0.875\n",
      "==>>> epoch: 756, batch index: 6, train loss: 2.173028\n",
      "==>>> epoch: 756, batch index: 1, test loss: 0.413429, acc: 0.900\n",
      "==>>> epoch: 757, batch index: 6, train loss: 2.170011\n",
      "==>>> epoch: 757, batch index: 1, test loss: 0.434800, acc: 0.875\n",
      "==>>> epoch: 758, batch index: 6, train loss: 2.203093\n",
      "==>>> epoch: 758, batch index: 1, test loss: 0.423086, acc: 0.900\n",
      "==>>> epoch: 759, batch index: 6, train loss: 2.217762\n",
      "==>>> epoch: 759, batch index: 1, test loss: 0.413940, acc: 0.900\n",
      "==>>> epoch: 760, batch index: 6, train loss: 2.218980\n",
      "==>>> epoch: 760, batch index: 1, test loss: 0.428507, acc: 0.875\n",
      "==>>> epoch: 761, batch index: 6, train loss: 2.215414\n",
      "==>>> epoch: 761, batch index: 1, test loss: 0.421834, acc: 0.900\n",
      "==>>> epoch: 762, batch index: 6, train loss: 2.242979\n",
      "==>>> epoch: 762, batch index: 1, test loss: 0.439639, acc: 0.875\n",
      "==>>> epoch: 763, batch index: 6, train loss: 2.201245\n",
      "==>>> epoch: 763, batch index: 1, test loss: 0.429582, acc: 0.875\n",
      "==>>> epoch: 764, batch index: 6, train loss: 2.175312\n",
      "==>>> epoch: 764, batch index: 1, test loss: 0.412400, acc: 0.900\n",
      "==>>> epoch: 765, batch index: 6, train loss: 2.169101\n",
      "==>>> epoch: 765, batch index: 1, test loss: 0.445486, acc: 0.875\n",
      "==>>> epoch: 766, batch index: 6, train loss: 2.182736\n",
      "==>>> epoch: 766, batch index: 1, test loss: 0.426872, acc: 0.875\n",
      "==>>> epoch: 767, batch index: 6, train loss: 2.186414\n",
      "==>>> epoch: 767, batch index: 1, test loss: 0.423919, acc: 0.875\n",
      "==>>> epoch: 768, batch index: 6, train loss: 2.191212\n",
      "==>>> epoch: 768, batch index: 1, test loss: 0.417277, acc: 0.900\n",
      "==>>> epoch: 769, batch index: 6, train loss: 2.169793\n",
      "==>>> epoch: 769, batch index: 1, test loss: 0.427736, acc: 0.875\n",
      "==>>> epoch: 770, batch index: 6, train loss: 2.188225\n",
      "==>>> epoch: 770, batch index: 1, test loss: 0.432976, acc: 0.875\n",
      "==>>> epoch: 771, batch index: 6, train loss: 2.214189\n",
      "==>>> epoch: 771, batch index: 1, test loss: 0.441761, acc: 0.875\n",
      "==>>> epoch: 772, batch index: 6, train loss: 2.201821\n",
      "==>>> epoch: 772, batch index: 1, test loss: 0.423482, acc: 0.900\n",
      "==>>> epoch: 773, batch index: 6, train loss: 2.171270\n",
      "==>>> epoch: 773, batch index: 1, test loss: 0.410201, acc: 0.900\n",
      "==>>> epoch: 774, batch index: 6, train loss: 2.144412\n",
      "==>>> epoch: 774, batch index: 1, test loss: 0.432824, acc: 0.875\n",
      "==>>> epoch: 775, batch index: 6, train loss: 2.200470\n",
      "==>>> epoch: 775, batch index: 1, test loss: 0.431105, acc: 0.875\n",
      "==>>> epoch: 776, batch index: 6, train loss: 2.183759\n",
      "==>>> epoch: 776, batch index: 1, test loss: 0.411678, acc: 0.900\n",
      "==>>> epoch: 777, batch index: 6, train loss: 2.149477\n",
      "==>>> epoch: 777, batch index: 1, test loss: 0.435138, acc: 0.875\n",
      "==>>> epoch: 778, batch index: 6, train loss: 2.210378\n",
      "==>>> epoch: 778, batch index: 1, test loss: 0.437862, acc: 0.875\n",
      "==>>> epoch: 779, batch index: 6, train loss: 2.170706\n",
      "==>>> epoch: 779, batch index: 1, test loss: 0.405430, acc: 0.900\n",
      "==>>> epoch: 780, batch index: 6, train loss: 2.248820\n",
      "==>>> epoch: 780, batch index: 1, test loss: 0.416596, acc: 0.900\n",
      "==>>> epoch: 781, batch index: 6, train loss: 2.141586\n",
      "==>>> epoch: 781, batch index: 1, test loss: 0.423090, acc: 0.875\n",
      "==>>> epoch: 782, batch index: 6, train loss: 2.157871\n",
      "==>>> epoch: 782, batch index: 1, test loss: 0.405853, acc: 0.900\n",
      "==>>> epoch: 783, batch index: 6, train loss: 2.231665\n",
      "==>>> epoch: 783, batch index: 1, test loss: 0.406727, acc: 0.900\n",
      "==>>> epoch: 784, batch index: 6, train loss: 2.176609\n",
      "==>>> epoch: 784, batch index: 1, test loss: 0.426304, acc: 0.875\n",
      "==>>> epoch: 785, batch index: 6, train loss: 2.152405\n",
      "==>>> epoch: 785, batch index: 1, test loss: 0.396157, acc: 0.925\n",
      "==>>> epoch: 786, batch index: 6, train loss: 2.173037\n",
      "==>>> epoch: 786, batch index: 1, test loss: 0.424789, acc: 0.900\n",
      "==>>> epoch: 787, batch index: 6, train loss: 2.207780\n",
      "==>>> epoch: 787, batch index: 1, test loss: 0.422669, acc: 0.900\n",
      "==>>> epoch: 788, batch index: 6, train loss: 2.184272\n",
      "==>>> epoch: 788, batch index: 1, test loss: 0.437959, acc: 0.875\n",
      "==>>> epoch: 789, batch index: 6, train loss: 2.212414\n",
      "==>>> epoch: 789, batch index: 1, test loss: 0.434802, acc: 0.875\n",
      "==>>> epoch: 790, batch index: 6, train loss: 2.216097\n",
      "==>>> epoch: 790, batch index: 1, test loss: 0.436215, acc: 0.875\n",
      "==>>> epoch: 791, batch index: 6, train loss: 2.197937\n",
      "==>>> epoch: 791, batch index: 1, test loss: 0.433132, acc: 0.875\n",
      "==>>> epoch: 792, batch index: 6, train loss: 2.176242\n",
      "==>>> epoch: 792, batch index: 1, test loss: 0.416526, acc: 0.900\n",
      "==>>> epoch: 793, batch index: 6, train loss: 2.148110\n",
      "==>>> epoch: 793, batch index: 1, test loss: 0.437692, acc: 0.875\n",
      "==>>> epoch: 794, batch index: 6, train loss: 2.161085\n",
      "==>>> epoch: 794, batch index: 1, test loss: 0.405242, acc: 0.900\n",
      "==>>> epoch: 795, batch index: 6, train loss: 2.152383\n",
      "==>>> epoch: 795, batch index: 1, test loss: 0.429788, acc: 0.875\n",
      "==>>> epoch: 796, batch index: 6, train loss: 2.220658\n",
      "==>>> epoch: 796, batch index: 1, test loss: 0.414395, acc: 0.900\n",
      "==>>> epoch: 797, batch index: 6, train loss: 2.207080\n",
      "==>>> epoch: 797, batch index: 1, test loss: 0.456403, acc: 0.825\n",
      "==>>> epoch: 798, batch index: 6, train loss: 2.237688\n",
      "==>>> epoch: 798, batch index: 1, test loss: 0.422097, acc: 0.900\n",
      "==>>> epoch: 799, batch index: 6, train loss: 2.170744\n",
      "==>>> epoch: 799, batch index: 1, test loss: 0.434783, acc: 0.875\n",
      "==>>> epoch: 800, batch index: 6, train loss: 2.157508\n",
      "==>>> epoch: 800, batch index: 1, test loss: 0.421001, acc: 0.900\n",
      "==>>> epoch: 801, batch index: 6, train loss: 2.154735\n",
      "==>>> epoch: 801, batch index: 1, test loss: 0.429467, acc: 0.875\n",
      "==>>> epoch: 802, batch index: 6, train loss: 2.157961\n",
      "==>>> epoch: 802, batch index: 1, test loss: 0.405821, acc: 0.900\n",
      "==>>> epoch: 803, batch index: 6, train loss: 2.151497\n",
      "==>>> epoch: 803, batch index: 1, test loss: 0.437661, acc: 0.875\n",
      "==>>> epoch: 804, batch index: 6, train loss: 2.154236\n",
      "==>>> epoch: 804, batch index: 1, test loss: 0.413807, acc: 0.900\n",
      "==>>> epoch: 805, batch index: 6, train loss: 2.263778\n",
      "==>>> epoch: 805, batch index: 1, test loss: 0.422684, acc: 0.900\n",
      "==>>> epoch: 806, batch index: 6, train loss: 2.184105\n",
      "==>>> epoch: 806, batch index: 1, test loss: 0.426196, acc: 0.875\n",
      "==>>> epoch: 807, batch index: 6, train loss: 2.180127\n",
      "==>>> epoch: 807, batch index: 1, test loss: 0.420006, acc: 0.900\n",
      "==>>> epoch: 808, batch index: 6, train loss: 2.179225\n",
      "==>>> epoch: 808, batch index: 1, test loss: 0.435999, acc: 0.875\n",
      "==>>> epoch: 809, batch index: 6, train loss: 2.221759\n",
      "==>>> epoch: 809, batch index: 1, test loss: 0.458590, acc: 0.850\n",
      "==>>> epoch: 810, batch index: 6, train loss: 2.188775\n",
      "==>>> epoch: 810, batch index: 1, test loss: 0.425738, acc: 0.875\n",
      "==>>> epoch: 811, batch index: 6, train loss: 2.190223\n",
      "==>>> epoch: 811, batch index: 1, test loss: 0.402801, acc: 0.900\n",
      "==>>> epoch: 812, batch index: 6, train loss: 2.156785\n",
      "==>>> epoch: 812, batch index: 1, test loss: 0.435614, acc: 0.875\n",
      "==>>> epoch: 813, batch index: 6, train loss: 2.162894\n",
      "==>>> epoch: 813, batch index: 1, test loss: 0.406513, acc: 0.900\n",
      "==>>> epoch: 814, batch index: 6, train loss: 2.259681\n",
      "==>>> epoch: 814, batch index: 1, test loss: 0.461197, acc: 0.850\n",
      "==>>> epoch: 815, batch index: 6, train loss: 2.286335\n",
      "==>>> epoch: 815, batch index: 1, test loss: 0.434806, acc: 0.875\n",
      "==>>> epoch: 816, batch index: 6, train loss: 2.186854\n",
      "==>>> epoch: 816, batch index: 1, test loss: 0.438012, acc: 0.875\n",
      "==>>> epoch: 817, batch index: 6, train loss: 2.164624\n",
      "==>>> epoch: 817, batch index: 1, test loss: 0.414424, acc: 0.900\n",
      "==>>> epoch: 818, batch index: 6, train loss: 2.231697\n",
      "==>>> epoch: 818, batch index: 1, test loss: 0.431631, acc: 0.875\n",
      "==>>> epoch: 819, batch index: 6, train loss: 2.192003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 819, batch index: 1, test loss: 0.429959, acc: 0.875\n",
      "==>>> epoch: 820, batch index: 6, train loss: 2.168510\n",
      "==>>> epoch: 820, batch index: 1, test loss: 0.419095, acc: 0.900\n",
      "==>>> epoch: 821, batch index: 6, train loss: 2.208456\n",
      "==>>> epoch: 821, batch index: 1, test loss: 0.394059, acc: 0.925\n",
      "==>>> epoch: 822, batch index: 6, train loss: 2.181974\n",
      "==>>> epoch: 822, batch index: 1, test loss: 0.403419, acc: 0.900\n",
      "==>>> epoch: 823, batch index: 6, train loss: 2.188056\n",
      "==>>> epoch: 823, batch index: 1, test loss: 0.450546, acc: 0.850\n",
      "==>>> epoch: 824, batch index: 6, train loss: 2.198767\n",
      "==>>> epoch: 824, batch index: 1, test loss: 0.416917, acc: 0.900\n",
      "==>>> epoch: 825, batch index: 6, train loss: 2.214999\n",
      "==>>> epoch: 825, batch index: 1, test loss: 0.411932, acc: 0.900\n",
      "==>>> epoch: 826, batch index: 6, train loss: 2.129937\n",
      "==>>> epoch: 826, batch index: 1, test loss: 0.440703, acc: 0.875\n",
      "==>>> epoch: 827, batch index: 6, train loss: 2.206379\n",
      "==>>> epoch: 827, batch index: 1, test loss: 0.416647, acc: 0.900\n",
      "==>>> epoch: 828, batch index: 6, train loss: 2.165592\n",
      "==>>> epoch: 828, batch index: 1, test loss: 0.427567, acc: 0.875\n",
      "==>>> epoch: 829, batch index: 6, train loss: 2.224327\n",
      "==>>> epoch: 829, batch index: 1, test loss: 0.453562, acc: 0.850\n",
      "==>>> epoch: 830, batch index: 6, train loss: 2.190958\n",
      "==>>> epoch: 830, batch index: 1, test loss: 0.404922, acc: 0.925\n",
      "==>>> epoch: 831, batch index: 6, train loss: 2.166403\n",
      "==>>> epoch: 831, batch index: 1, test loss: 0.448981, acc: 0.850\n",
      "==>>> epoch: 832, batch index: 6, train loss: 2.186708\n",
      "==>>> epoch: 832, batch index: 1, test loss: 0.438507, acc: 0.875\n",
      "==>>> epoch: 833, batch index: 6, train loss: 2.193940\n",
      "==>>> epoch: 833, batch index: 1, test loss: 0.440473, acc: 0.875\n",
      "==>>> epoch: 834, batch index: 6, train loss: 2.159854\n",
      "==>>> epoch: 834, batch index: 1, test loss: 0.424774, acc: 0.875\n",
      "==>>> epoch: 835, batch index: 6, train loss: 2.196500\n",
      "==>>> epoch: 835, batch index: 1, test loss: 0.398589, acc: 0.925\n",
      "==>>> epoch: 836, batch index: 6, train loss: 2.209254\n",
      "==>>> epoch: 836, batch index: 1, test loss: 0.409971, acc: 0.900\n",
      "==>>> epoch: 837, batch index: 6, train loss: 2.230460\n",
      "==>>> epoch: 837, batch index: 1, test loss: 0.426369, acc: 0.875\n",
      "==>>> epoch: 838, batch index: 6, train loss: 2.157075\n",
      "==>>> epoch: 838, batch index: 1, test loss: 0.412307, acc: 0.900\n",
      "==>>> epoch: 839, batch index: 6, train loss: 2.221776\n",
      "==>>> epoch: 839, batch index: 1, test loss: 0.427240, acc: 0.875\n",
      "==>>> epoch: 840, batch index: 6, train loss: 2.209806\n",
      "==>>> epoch: 840, batch index: 1, test loss: 0.434023, acc: 0.875\n",
      "==>>> epoch: 841, batch index: 6, train loss: 2.197563\n",
      "==>>> epoch: 841, batch index: 1, test loss: 0.436249, acc: 0.875\n",
      "==>>> epoch: 842, batch index: 6, train loss: 2.178851\n",
      "==>>> epoch: 842, batch index: 1, test loss: 0.443745, acc: 0.875\n",
      "==>>> epoch: 843, batch index: 6, train loss: 2.288485\n",
      "==>>> epoch: 843, batch index: 1, test loss: 0.427213, acc: 0.875\n",
      "==>>> epoch: 844, batch index: 6, train loss: 2.166648\n",
      "==>>> epoch: 844, batch index: 1, test loss: 0.415662, acc: 0.900\n",
      "==>>> epoch: 845, batch index: 6, train loss: 2.233028\n",
      "==>>> epoch: 845, batch index: 1, test loss: 0.423685, acc: 0.875\n",
      "==>>> epoch: 846, batch index: 6, train loss: 2.222938\n",
      "==>>> epoch: 846, batch index: 1, test loss: 0.435329, acc: 0.875\n",
      "==>>> epoch: 847, batch index: 6, train loss: 2.191634\n",
      "==>>> epoch: 847, batch index: 1, test loss: 0.439566, acc: 0.875\n",
      "==>>> epoch: 848, batch index: 6, train loss: 2.169760\n",
      "==>>> epoch: 848, batch index: 1, test loss: 0.412099, acc: 0.900\n",
      "==>>> epoch: 849, batch index: 6, train loss: 2.131260\n",
      "==>>> epoch: 849, batch index: 1, test loss: 0.433721, acc: 0.875\n",
      "==>>> epoch: 850, batch index: 6, train loss: 2.188790\n",
      "==>>> epoch: 850, batch index: 1, test loss: 0.437897, acc: 0.875\n",
      "==>>> epoch: 851, batch index: 6, train loss: 2.157401\n",
      "==>>> epoch: 851, batch index: 1, test loss: 0.409382, acc: 0.900\n",
      "==>>> epoch: 852, batch index: 6, train loss: 2.177652\n",
      "==>>> epoch: 852, batch index: 1, test loss: 0.403811, acc: 0.900\n",
      "==>>> epoch: 853, batch index: 6, train loss: 2.216189\n",
      "==>>> epoch: 853, batch index: 1, test loss: 0.450157, acc: 0.850\n",
      "==>>> epoch: 854, batch index: 6, train loss: 2.177893\n",
      "==>>> epoch: 854, batch index: 1, test loss: 0.431743, acc: 0.875\n",
      "==>>> epoch: 855, batch index: 6, train loss: 2.169182\n",
      "==>>> epoch: 855, batch index: 1, test loss: 0.407840, acc: 0.900\n",
      "==>>> epoch: 856, batch index: 6, train loss: 2.170256\n",
      "==>>> epoch: 856, batch index: 1, test loss: 0.434420, acc: 0.875\n",
      "==>>> epoch: 857, batch index: 6, train loss: 2.259064\n",
      "==>>> epoch: 857, batch index: 1, test loss: 0.435128, acc: 0.875\n",
      "==>>> epoch: 858, batch index: 6, train loss: 2.185309\n",
      "==>>> epoch: 858, batch index: 1, test loss: 0.400535, acc: 0.900\n",
      "==>>> epoch: 859, batch index: 6, train loss: 2.189428\n",
      "==>>> epoch: 859, batch index: 1, test loss: 0.461819, acc: 0.825\n",
      "==>>> epoch: 860, batch index: 6, train loss: 2.154034\n",
      "==>>> epoch: 860, batch index: 1, test loss: 0.399697, acc: 0.925\n",
      "==>>> epoch: 861, batch index: 6, train loss: 2.249669\n",
      "==>>> epoch: 861, batch index: 1, test loss: 0.419573, acc: 0.900\n",
      "==>>> epoch: 862, batch index: 6, train loss: 2.178062\n",
      "==>>> epoch: 862, batch index: 1, test loss: 0.404284, acc: 0.900\n",
      "==>>> epoch: 863, batch index: 6, train loss: 2.170268\n",
      "==>>> epoch: 863, batch index: 1, test loss: 0.394702, acc: 0.925\n",
      "==>>> epoch: 864, batch index: 6, train loss: 2.199678\n",
      "==>>> epoch: 864, batch index: 1, test loss: 0.403547, acc: 0.900\n",
      "==>>> epoch: 865, batch index: 6, train loss: 2.141980\n",
      "==>>> epoch: 865, batch index: 1, test loss: 0.427962, acc: 0.875\n",
      "==>>> epoch: 866, batch index: 6, train loss: 2.161272\n",
      "==>>> epoch: 866, batch index: 1, test loss: 0.402251, acc: 0.900\n",
      "==>>> epoch: 867, batch index: 6, train loss: 2.225387\n",
      "==>>> epoch: 867, batch index: 1, test loss: 0.415446, acc: 0.900\n",
      "==>>> epoch: 868, batch index: 6, train loss: 2.197408\n",
      "==>>> epoch: 868, batch index: 1, test loss: 0.449794, acc: 0.850\n",
      "==>>> epoch: 869, batch index: 6, train loss: 2.184893\n",
      "==>>> epoch: 869, batch index: 1, test loss: 0.442289, acc: 0.875\n",
      "==>>> epoch: 870, batch index: 6, train loss: 2.265406\n",
      "==>>> epoch: 870, batch index: 1, test loss: 0.426256, acc: 0.900\n",
      "==>>> epoch: 871, batch index: 6, train loss: 2.199442\n",
      "==>>> epoch: 871, batch index: 1, test loss: 0.433696, acc: 0.875\n",
      "==>>> epoch: 872, batch index: 6, train loss: 2.192951\n",
      "==>>> epoch: 872, batch index: 1, test loss: 0.440957, acc: 0.875\n",
      "==>>> epoch: 873, batch index: 6, train loss: 2.158275\n",
      "==>>> epoch: 873, batch index: 1, test loss: 0.425209, acc: 0.875\n",
      "==>>> epoch: 874, batch index: 6, train loss: 2.214370\n",
      "==>>> epoch: 874, batch index: 1, test loss: 0.438147, acc: 0.875\n",
      "==>>> epoch: 875, batch index: 6, train loss: 2.201208\n",
      "==>>> epoch: 875, batch index: 1, test loss: 0.431772, acc: 0.875\n",
      "==>>> epoch: 876, batch index: 6, train loss: 2.201910\n",
      "==>>> epoch: 876, batch index: 1, test loss: 0.436914, acc: 0.875\n",
      "==>>> epoch: 877, batch index: 6, train loss: 2.147390\n",
      "==>>> epoch: 877, batch index: 1, test loss: 0.411290, acc: 0.900\n",
      "==>>> epoch: 878, batch index: 6, train loss: 2.180643\n",
      "==>>> epoch: 878, batch index: 1, test loss: 0.404736, acc: 0.900\n",
      "==>>> epoch: 879, batch index: 6, train loss: 2.291075\n",
      "==>>> epoch: 879, batch index: 1, test loss: 0.448334, acc: 0.875\n",
      "==>>> epoch: 880, batch index: 6, train loss: 2.170768\n",
      "==>>> epoch: 880, batch index: 1, test loss: 0.410220, acc: 0.900\n",
      "==>>> epoch: 881, batch index: 6, train loss: 2.384339\n",
      "==>>> epoch: 881, batch index: 1, test loss: 0.423104, acc: 0.900\n",
      "==>>> epoch: 882, batch index: 6, train loss: 2.219985\n",
      "==>>> epoch: 882, batch index: 1, test loss: 0.441216, acc: 0.850\n",
      "==>>> epoch: 883, batch index: 6, train loss: 2.190553\n",
      "==>>> epoch: 883, batch index: 1, test loss: 0.442083, acc: 0.850\n",
      "==>>> epoch: 884, batch index: 6, train loss: 2.176716\n",
      "==>>> epoch: 884, batch index: 1, test loss: 0.412938, acc: 0.900\n",
      "==>>> epoch: 885, batch index: 6, train loss: 2.153301\n",
      "==>>> epoch: 885, batch index: 1, test loss: 0.402765, acc: 0.925\n",
      "==>>> epoch: 886, batch index: 6, train loss: 2.187224\n",
      "==>>> epoch: 886, batch index: 1, test loss: 0.464151, acc: 0.850\n",
      "==>>> epoch: 887, batch index: 6, train loss: 2.211871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 887, batch index: 1, test loss: 0.429551, acc: 0.875\n",
      "==>>> epoch: 888, batch index: 6, train loss: 2.176307\n",
      "==>>> epoch: 888, batch index: 1, test loss: 0.430035, acc: 0.875\n",
      "==>>> epoch: 889, batch index: 6, train loss: 2.188425\n",
      "==>>> epoch: 889, batch index: 1, test loss: 0.432331, acc: 0.875\n",
      "==>>> epoch: 890, batch index: 6, train loss: 2.173768\n",
      "==>>> epoch: 890, batch index: 1, test loss: 0.422495, acc: 0.900\n",
      "==>>> epoch: 891, batch index: 6, train loss: 2.162387\n",
      "==>>> epoch: 891, batch index: 1, test loss: 0.403780, acc: 0.900\n",
      "==>>> epoch: 892, batch index: 6, train loss: 2.242061\n",
      "==>>> epoch: 892, batch index: 1, test loss: 0.425809, acc: 0.875\n",
      "==>>> epoch: 893, batch index: 6, train loss: 2.201379\n",
      "==>>> epoch: 893, batch index: 1, test loss: 0.421014, acc: 0.900\n",
      "==>>> epoch: 894, batch index: 6, train loss: 2.128363\n",
      "==>>> epoch: 894, batch index: 1, test loss: 0.406529, acc: 0.900\n",
      "==>>> epoch: 895, batch index: 6, train loss: 2.229839\n",
      "==>>> epoch: 895, batch index: 1, test loss: 0.426549, acc: 0.875\n",
      "==>>> epoch: 896, batch index: 6, train loss: 2.233570\n",
      "==>>> epoch: 896, batch index: 1, test loss: 0.456141, acc: 0.850\n",
      "==>>> epoch: 897, batch index: 6, train loss: 2.213625\n",
      "==>>> epoch: 897, batch index: 1, test loss: 0.440718, acc: 0.875\n",
      "==>>> epoch: 898, batch index: 6, train loss: 2.185308\n",
      "==>>> epoch: 898, batch index: 1, test loss: 0.424160, acc: 0.875\n",
      "==>>> epoch: 899, batch index: 6, train loss: 2.173295\n",
      "==>>> epoch: 899, batch index: 1, test loss: 0.406059, acc: 0.900\n",
      "==>>> epoch: 900, batch index: 6, train loss: 2.140035\n",
      "==>>> epoch: 900, batch index: 1, test loss: 0.437833, acc: 0.875\n",
      "==>>> epoch: 901, batch index: 6, train loss: 2.168429\n",
      "==>>> epoch: 901, batch index: 1, test loss: 0.413172, acc: 0.900\n",
      "==>>> epoch: 902, batch index: 6, train loss: 2.168617\n",
      "==>>> epoch: 902, batch index: 1, test loss: 0.400922, acc: 0.900\n",
      "==>>> epoch: 903, batch index: 6, train loss: 2.269702\n",
      "==>>> epoch: 903, batch index: 1, test loss: 0.429842, acc: 0.875\n",
      "==>>> epoch: 904, batch index: 6, train loss: 2.244379\n",
      "==>>> epoch: 904, batch index: 1, test loss: 0.444269, acc: 0.850\n",
      "==>>> epoch: 905, batch index: 6, train loss: 2.183422\n",
      "==>>> epoch: 905, batch index: 1, test loss: 0.437653, acc: 0.875\n",
      "==>>> epoch: 906, batch index: 6, train loss: 2.157175\n",
      "==>>> epoch: 906, batch index: 1, test loss: 0.408100, acc: 0.900\n",
      "==>>> epoch: 907, batch index: 6, train loss: 2.151891\n",
      "==>>> epoch: 907, batch index: 1, test loss: 0.433663, acc: 0.875\n",
      "==>>> epoch: 908, batch index: 6, train loss: 2.157222\n",
      "==>>> epoch: 908, batch index: 1, test loss: 0.411849, acc: 0.900\n",
      "==>>> epoch: 909, batch index: 6, train loss: 2.186398\n",
      "==>>> epoch: 909, batch index: 1, test loss: 0.412399, acc: 0.900\n",
      "==>>> epoch: 910, batch index: 6, train loss: 2.236369\n",
      "==>>> epoch: 910, batch index: 1, test loss: 0.424689, acc: 0.875\n",
      "==>>> epoch: 911, batch index: 6, train loss: 2.192661\n",
      "==>>> epoch: 911, batch index: 1, test loss: 0.432149, acc: 0.875\n",
      "==>>> epoch: 912, batch index: 6, train loss: 2.149802\n",
      "==>>> epoch: 912, batch index: 1, test loss: 0.405983, acc: 0.900\n",
      "==>>> epoch: 913, batch index: 6, train loss: 2.177271\n",
      "==>>> epoch: 913, batch index: 1, test loss: 0.467229, acc: 0.825\n",
      "==>>> epoch: 914, batch index: 6, train loss: 2.202679\n",
      "==>>> epoch: 914, batch index: 1, test loss: 0.443088, acc: 0.875\n",
      "==>>> epoch: 915, batch index: 6, train loss: 2.172149\n",
      "==>>> epoch: 915, batch index: 1, test loss: 0.409007, acc: 0.900\n",
      "==>>> epoch: 916, batch index: 6, train loss: 2.205997\n",
      "==>>> epoch: 916, batch index: 1, test loss: 0.422966, acc: 0.900\n",
      "==>>> epoch: 917, batch index: 6, train loss: 2.163440\n",
      "==>>> epoch: 917, batch index: 1, test loss: 0.408478, acc: 0.900\n",
      "==>>> epoch: 918, batch index: 6, train loss: 2.155848\n",
      "==>>> epoch: 918, batch index: 1, test loss: 0.399428, acc: 0.925\n",
      "==>>> epoch: 919, batch index: 6, train loss: 2.143941\n",
      "==>>> epoch: 919, batch index: 1, test loss: 0.446560, acc: 0.850\n",
      "==>>> epoch: 920, batch index: 6, train loss: 2.159789\n",
      "==>>> epoch: 920, batch index: 1, test loss: 0.416436, acc: 0.900\n",
      "==>>> epoch: 921, batch index: 6, train loss: 2.176527\n",
      "==>>> epoch: 921, batch index: 1, test loss: 0.401149, acc: 0.900\n",
      "==>>> epoch: 922, batch index: 6, train loss: 2.212076\n",
      "==>>> epoch: 922, batch index: 1, test loss: 0.425937, acc: 0.875\n",
      "==>>> epoch: 923, batch index: 6, train loss: 2.147678\n",
      "==>>> epoch: 923, batch index: 1, test loss: 0.423908, acc: 0.875\n",
      "==>>> epoch: 924, batch index: 6, train loss: 2.144085\n",
      "==>>> epoch: 924, batch index: 1, test loss: 0.397374, acc: 0.925\n",
      "==>>> epoch: 925, batch index: 6, train loss: 2.135254\n",
      "==>>> epoch: 925, batch index: 1, test loss: 0.445450, acc: 0.875\n",
      "==>>> epoch: 926, batch index: 6, train loss: 2.164325\n",
      "==>>> epoch: 926, batch index: 1, test loss: 0.394423, acc: 0.925\n",
      "==>>> epoch: 927, batch index: 6, train loss: 2.199115\n",
      "==>>> epoch: 927, batch index: 1, test loss: 0.451524, acc: 0.850\n",
      "==>>> epoch: 928, batch index: 6, train loss: 2.214010\n",
      "==>>> epoch: 928, batch index: 1, test loss: 0.438694, acc: 0.875\n",
      "==>>> epoch: 929, batch index: 6, train loss: 2.171600\n",
      "==>>> epoch: 929, batch index: 1, test loss: 0.422808, acc: 0.900\n",
      "==>>> epoch: 930, batch index: 6, train loss: 2.142087\n",
      "==>>> epoch: 930, batch index: 1, test loss: 0.402624, acc: 0.900\n",
      "==>>> epoch: 931, batch index: 6, train loss: 2.157874\n",
      "==>>> epoch: 931, batch index: 1, test loss: 0.439784, acc: 0.875\n",
      "==>>> epoch: 932, batch index: 6, train loss: 2.176352\n",
      "==>>> epoch: 932, batch index: 1, test loss: 0.438382, acc: 0.875\n",
      "==>>> epoch: 933, batch index: 6, train loss: 2.168690\n",
      "==>>> epoch: 933, batch index: 1, test loss: 0.417156, acc: 0.900\n",
      "==>>> epoch: 934, batch index: 6, train loss: 2.200671\n",
      "==>>> epoch: 934, batch index: 1, test loss: 0.421950, acc: 0.900\n",
      "==>>> epoch: 935, batch index: 6, train loss: 2.197383\n",
      "==>>> epoch: 935, batch index: 1, test loss: 0.427667, acc: 0.875\n",
      "==>>> epoch: 936, batch index: 6, train loss: 2.201734\n",
      "==>>> epoch: 936, batch index: 1, test loss: 0.438929, acc: 0.875\n",
      "==>>> epoch: 937, batch index: 6, train loss: 2.174469\n",
      "==>>> epoch: 937, batch index: 1, test loss: 0.448259, acc: 0.850\n",
      "==>>> epoch: 938, batch index: 6, train loss: 2.226009\n",
      "==>>> epoch: 938, batch index: 1, test loss: 0.467414, acc: 0.850\n",
      "==>>> epoch: 939, batch index: 6, train loss: 2.175536\n",
      "==>>> epoch: 939, batch index: 1, test loss: 0.398198, acc: 0.925\n",
      "==>>> epoch: 940, batch index: 6, train loss: 2.236217\n",
      "==>>> epoch: 940, batch index: 1, test loss: 0.424440, acc: 0.900\n",
      "==>>> epoch: 941, batch index: 6, train loss: 2.148207\n",
      "==>>> epoch: 941, batch index: 1, test loss: 0.398492, acc: 0.925\n",
      "==>>> epoch: 942, batch index: 6, train loss: 2.181493\n",
      "==>>> epoch: 942, batch index: 1, test loss: 0.458235, acc: 0.850\n",
      "==>>> epoch: 943, batch index: 6, train loss: 2.204327\n",
      "==>>> epoch: 943, batch index: 1, test loss: 0.417165, acc: 0.900\n",
      "==>>> epoch: 944, batch index: 6, train loss: 2.177756\n",
      "==>>> epoch: 944, batch index: 1, test loss: 0.417604, acc: 0.900\n",
      "==>>> epoch: 945, batch index: 6, train loss: 2.167019\n",
      "==>>> epoch: 945, batch index: 1, test loss: 0.403818, acc: 0.900\n",
      "==>>> epoch: 946, batch index: 6, train loss: 2.143070\n",
      "==>>> epoch: 946, batch index: 1, test loss: 0.432240, acc: 0.875\n",
      "==>>> epoch: 947, batch index: 6, train loss: 2.192622\n",
      "==>>> epoch: 947, batch index: 1, test loss: 0.432583, acc: 0.875\n",
      "==>>> epoch: 948, batch index: 6, train loss: 2.185179\n",
      "==>>> epoch: 948, batch index: 1, test loss: 0.439849, acc: 0.875\n",
      "==>>> epoch: 949, batch index: 6, train loss: 2.161040\n",
      "==>>> epoch: 949, batch index: 1, test loss: 0.413647, acc: 0.900\n",
      "==>>> epoch: 950, batch index: 6, train loss: 2.219883\n",
      "==>>> epoch: 950, batch index: 1, test loss: 0.414091, acc: 0.900\n",
      "==>>> epoch: 951, batch index: 6, train loss: 2.145234\n",
      "==>>> epoch: 951, batch index: 1, test loss: 0.430958, acc: 0.875\n",
      "==>>> epoch: 952, batch index: 6, train loss: 2.175163\n",
      "==>>> epoch: 952, batch index: 1, test loss: 0.418875, acc: 0.900\n",
      "==>>> epoch: 953, batch index: 6, train loss: 2.239246\n",
      "==>>> epoch: 953, batch index: 1, test loss: 0.452067, acc: 0.850\n",
      "==>>> epoch: 954, batch index: 6, train loss: 2.205474\n",
      "==>>> epoch: 954, batch index: 1, test loss: 0.423478, acc: 0.900\n",
      "==>>> epoch: 955, batch index: 6, train loss: 2.132074\n",
      "==>>> epoch: 955, batch index: 1, test loss: 0.401442, acc: 0.925\n",
      "==>>> epoch: 956, batch index: 6, train loss: 2.220080\n",
      "==>>> epoch: 956, batch index: 1, test loss: 0.417668, acc: 0.900\n",
      "==>>> epoch: 957, batch index: 6, train loss: 2.164597\n",
      "==>>> epoch: 957, batch index: 1, test loss: 0.398290, acc: 0.925\n",
      "==>>> epoch: 958, batch index: 6, train loss: 2.197174\n",
      "==>>> epoch: 958, batch index: 1, test loss: 0.396262, acc: 0.925\n",
      "==>>> epoch: 959, batch index: 6, train loss: 2.207581\n",
      "==>>> epoch: 959, batch index: 1, test loss: 0.404273, acc: 0.900\n",
      "==>>> epoch: 960, batch index: 6, train loss: 2.178055\n",
      "==>>> epoch: 960, batch index: 1, test loss: 0.398362, acc: 0.925\n",
      "==>>> epoch: 961, batch index: 6, train loss: 2.128072\n",
      "==>>> epoch: 961, batch index: 1, test loss: 0.436449, acc: 0.875\n",
      "==>>> epoch: 962, batch index: 6, train loss: 2.170795\n",
      "==>>> epoch: 962, batch index: 1, test loss: 0.407706, acc: 0.900\n",
      "==>>> epoch: 963, batch index: 6, train loss: 2.183396\n",
      "==>>> epoch: 963, batch index: 1, test loss: 0.405651, acc: 0.900\n",
      "==>>> epoch: 964, batch index: 6, train loss: 2.227058\n",
      "==>>> epoch: 964, batch index: 1, test loss: 0.414074, acc: 0.900\n",
      "==>>> epoch: 965, batch index: 6, train loss: 2.146520\n",
      "==>>> epoch: 965, batch index: 1, test loss: 0.435136, acc: 0.875\n",
      "==>>> epoch: 966, batch index: 6, train loss: 2.184847\n",
      "==>>> epoch: 966, batch index: 1, test loss: 0.442028, acc: 0.875\n",
      "==>>> epoch: 967, batch index: 6, train loss: 2.171876\n",
      "==>>> epoch: 967, batch index: 1, test loss: 0.420407, acc: 0.900\n",
      "==>>> epoch: 968, batch index: 6, train loss: 2.172592\n",
      "==>>> epoch: 968, batch index: 1, test loss: 0.429167, acc: 0.875\n",
      "==>>> epoch: 969, batch index: 6, train loss: 2.196952\n",
      "==>>> epoch: 969, batch index: 1, test loss: 0.428235, acc: 0.875\n",
      "==>>> epoch: 970, batch index: 6, train loss: 2.148573\n",
      "==>>> epoch: 970, batch index: 1, test loss: 0.402764, acc: 0.900\n",
      "==>>> epoch: 971, batch index: 6, train loss: 2.232469\n",
      "==>>> epoch: 971, batch index: 1, test loss: 0.459307, acc: 0.850\n",
      "==>>> epoch: 972, batch index: 6, train loss: 2.249018\n",
      "==>>> epoch: 972, batch index: 1, test loss: 0.459133, acc: 0.850\n",
      "==>>> epoch: 973, batch index: 6, train loss: 2.207842\n",
      "==>>> epoch: 973, batch index: 1, test loss: 0.435566, acc: 0.875\n",
      "==>>> epoch: 974, batch index: 6, train loss: 2.143096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 974, batch index: 1, test loss: 0.409748, acc: 0.900\n",
      "==>>> epoch: 975, batch index: 6, train loss: 2.222464\n",
      "==>>> epoch: 975, batch index: 1, test loss: 0.411100, acc: 0.900\n",
      "==>>> epoch: 976, batch index: 6, train loss: 2.173594\n",
      "==>>> epoch: 976, batch index: 1, test loss: 0.462526, acc: 0.850\n",
      "==>>> epoch: 977, batch index: 6, train loss: 2.180678\n",
      "==>>> epoch: 977, batch index: 1, test loss: 0.398191, acc: 0.925\n",
      "==>>> epoch: 978, batch index: 6, train loss: 2.246314\n",
      "==>>> epoch: 978, batch index: 1, test loss: 0.416609, acc: 0.900\n",
      "==>>> epoch: 979, batch index: 6, train loss: 2.142402\n",
      "==>>> epoch: 979, batch index: 1, test loss: 0.393782, acc: 0.925\n",
      "==>>> epoch: 980, batch index: 6, train loss: 2.188297\n",
      "==>>> epoch: 980, batch index: 1, test loss: 0.395322, acc: 0.925\n",
      "==>>> epoch: 981, batch index: 6, train loss: 2.156607\n",
      "==>>> epoch: 981, batch index: 1, test loss: 0.426183, acc: 0.900\n",
      "==>>> epoch: 982, batch index: 6, train loss: 2.128931\n",
      "==>>> epoch: 982, batch index: 1, test loss: 0.403008, acc: 0.925\n",
      "==>>> epoch: 983, batch index: 6, train loss: 2.176659\n",
      "==>>> epoch: 983, batch index: 1, test loss: 0.401190, acc: 0.925\n",
      "==>>> epoch: 984, batch index: 6, train loss: 2.135228\n",
      "==>>> epoch: 984, batch index: 1, test loss: 0.441781, acc: 0.875\n",
      "==>>> epoch: 985, batch index: 6, train loss: 2.163485\n",
      "==>>> epoch: 985, batch index: 1, test loss: 0.425500, acc: 0.875\n",
      "==>>> epoch: 986, batch index: 6, train loss: 2.164364\n",
      "==>>> epoch: 986, batch index: 1, test loss: 0.403339, acc: 0.925\n",
      "==>>> epoch: 987, batch index: 6, train loss: 2.232128\n",
      "==>>> epoch: 987, batch index: 1, test loss: 0.429511, acc: 0.875\n",
      "==>>> epoch: 988, batch index: 6, train loss: 2.211923\n",
      "==>>> epoch: 988, batch index: 1, test loss: 0.466088, acc: 0.825\n",
      "==>>> epoch: 989, batch index: 6, train loss: 2.201619\n",
      "==>>> epoch: 989, batch index: 1, test loss: 0.441065, acc: 0.875\n",
      "==>>> epoch: 990, batch index: 6, train loss: 2.190346\n",
      "==>>> epoch: 990, batch index: 1, test loss: 0.448695, acc: 0.850\n",
      "==>>> epoch: 991, batch index: 6, train loss: 2.218755\n",
      "==>>> epoch: 991, batch index: 1, test loss: 0.435558, acc: 0.875\n",
      "==>>> epoch: 992, batch index: 6, train loss: 2.152320\n",
      "==>>> epoch: 992, batch index: 1, test loss: 0.412936, acc: 0.900\n",
      "==>>> epoch: 993, batch index: 6, train loss: 2.116120\n",
      "==>>> epoch: 993, batch index: 1, test loss: 0.454117, acc: 0.850\n",
      "==>>> epoch: 994, batch index: 6, train loss: 2.167784\n",
      "==>>> epoch: 994, batch index: 1, test loss: 0.420782, acc: 0.900\n",
      "==>>> epoch: 995, batch index: 6, train loss: 2.134328\n",
      "==>>> epoch: 995, batch index: 1, test loss: 0.440119, acc: 0.875\n",
      "==>>> epoch: 996, batch index: 6, train loss: 2.142971\n",
      "==>>> epoch: 996, batch index: 1, test loss: 0.414697, acc: 0.900\n",
      "==>>> epoch: 997, batch index: 6, train loss: 2.158108\n",
      "==>>> epoch: 997, batch index: 1, test loss: 0.413062, acc: 0.900\n",
      "==>>> epoch: 998, batch index: 6, train loss: 2.144654\n",
      "==>>> epoch: 998, batch index: 1, test loss: 0.417017, acc: 0.900\n",
      "==>>> epoch: 999, batch index: 6, train loss: 2.144091\n",
      "==>>> epoch: 999, batch index: 1, test loss: 0.444712, acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    # trainning\n",
    "    total_loss = 0\n",
    "    model2.train()\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model2(x)\n",
    "        loss = criterion(out, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, total_loss))\n",
    "    # testing\n",
    "    correct_cnt, total_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model2(x)\n",
    "        \n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        total_loss += loss.item()\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, total_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "da140c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d1205f4370>]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAYGklEQVR4nO3df5Ac9Xnn8fcnLGh3Axv/kA6tAVv+gXP88PmCVLKd+CwudjgsUshycBCoErjDJdaEs1MVVYoUVb5DOtXZFyop+ywt2sIu46sUls8WQmfkA2OHECURYeXipwFbSBAkD2KxXUREWjih5/6YFh6vZnZnt3t6uqc/r6qt7Zlp5vvQ2n22++nn+x1FBGZm1vt+pdsBmJlZPpzwzcwqwgnfzKwinPDNzCrCCd/MrCL6uh3AdObPnx+LFi3qdhhmZqWxe/fuFyNiQbPXCp3wFy1axPj4eLfDMDMrDUnPtnrNJR0zs4pwwjczqwgnfDOzinDCNzOrCCd8M7OKcMI3s0zUDtVY9tVlPP/y890OxVpwwjezTKy/fz07/2kn6/5mXbdDsRZU5OWRlyxZEu7DNyu2gQ0DTB6dPOH5/r5+jtx4pAsRVZuk3RGxpNlrPsM3s1T2fnovV55/JYN9gwAM9g2y+j2r2feZfV2OzKZywjezVIZPG2Zo3hCTr03S39fP5GuTDM0bYuGpC7sdmk3hhG9mqR38l4OMLB5h1zW7GFk84hu3BeUavplZD3EN36xg3MJo3eCEb9YFbmG0bnBJxyxHbmG0TnNJx6wg3MJo3eSEb5YjtzBaNznhm+XMLYzWLa7hm5n1ENfwzcwsm4Qv6SuSXpD0WIvXL5T0kqSHkq/PZjGumZm1ry+j9/kq8CXga9Ps87cR8bsZjWdmZrOUyRl+RNwP/CyL9zIzs87Is4b/AUkPS/qOpPNa7SRpjaRxSeMTExM5hmdm1tvySvg/AN4WEe8F/iewrdWOETEWEUsiYsmCBQtyCs/MrPflkvAj4p8j4uVkewdwsqT5eYxtZmZ1uSR8SQslKdlemoz70zzGNjOzuqzaMm8H/gH4dUn7JV0jaUTSSLLLZcBjkh4GvgisiiLP+LKec8JyxLUaLFsGz3uWq1VHVl06V0TEcEScHBFnRsSXI+KWiLglef1LEXFeRLw3It4fEX+fxbhm7TphOeL162HnTljn5YmtOry0gvW0qcsRH/5vMHC0yY79/XDEyxNb+XlpBeuIMnxq09TliM//kwH+7t8tIgYG6jsMDsLq1bDPyxNb73PCtzkrw6c2TV2O+JnBV3jt1EH0yiv1s/rJSRgagoVenth6nxO+zdrAhgF0kxgdH+VYHGN0fBTdJAY2DHQ7tKamLkd88os/h5ER2LWr/t03bq0iXMO3WasdqrH2nrVse3Ibh48eZrBvkJXnrOTmi272B3mYdZlr+JYpf2qTWTk54duc+FObzGav240OLumYmeXkuruuY/PuzVy7+Fo2XbKpI2NMV9Jxwjcz67Cp80GO6+/r58iN2c7/cA3fzKyLps4HGewbZPV7VrPvM/nO/3DCNzPrsKI0Ojjhm5nloAiNDq7hm5n1ENfwzczMCd/MrCqc8M3MKsIJ38ysIpzwzcwqwgm/5Lq9NoeZlUdWH2L+FUkvSHqsxeuS9EVJeyQ9IumCLMa1cnwIiZkVQyZ9+JI+BLwMfC0izm/y+nLgPwPLgfcBX4iI9830vu7Dby3PtTnMrDw63ocfEfcDP5tmlxXU/xhEROwC3iBpOIuxq6ooa3OYWXnkVcM/A3iu4fH+5LkTSFojaVzS+MTERC7BlVFR1uawLqnVYNmyYn08YxFjsl9SuJu2ETEWEUsiYsmCBQu6HU6hFWFtDuuS9eth505YV6B7N0WMyX5JZmvpSFoEfLtFDX8zcF9E3J48fgq4MCJq072na/i9r3aoxqpvrWLLZVt8ddKOgQGYPPHeDf39cKRL926KGFOFFWEtne3AHybdOu8HXpop2Vs1uMtolvbuhSuvhMH6vRsGB2H1atjXxXs3RYzJmurL4k0k3Q5cCMyXtB/4L8DJABFxC7CDeofOHuAw8B+zGNfKa2qX0ej4KKPjo+4ymsnwMAwN1c+o+/vr34eGYGEXr46KGJM1lVWXzhURMRwRJ0fEmRHx5Yi4JUn2JN05fxQR74yI90SE6zQV5y6jFA4ehJER2LWr/r0IN0mLGJOdIJMzfLPZcpdRClu3/mJ748buxdGoiDHZCQrXpWPV4S4js3z5E6+sp7kLyKqmCF06Zl3hLiCzX/AZvvWktGsN+crAyspn+FY5abuAfGVgvchdOtaT5toF5PkB1st8hm89ay5dQJ4fYL3MZ/jWs7Ze/ove8I2XtNcb7vkB1st8hm82hecHWK9yl46ZWQ9xl46ZmTnhm5lVhRO+mWWidqjGsq8u8z2PAnPCN7NMeLJa8fmmrZmlknYZC8uWb9qaWcd4slp5OOGbWSqerFYemSR8SRdLekrSHkk3NHn9akkTkh5Kvj6ZxbhmVgyerFYOqWv4kk4CfgT8DrAfeBC4IiJ+2LDP1cCSiLh+Nu/tGr6Z2ex0uoa/FNgTEXsj4lXg68CKDN7XrHLc2midlEXCPwN4ruHx/uS5qX5P0iOSvinprFZvJmmNpHFJ4xMTExmEZ1Yebm20TsqipHMZcHFEfDJ5/AfA+xrLN5LeDLwcEa9Iuha4PCJ+e6b3dknHqsKtjZaVTpd0DgCNZ+xnJs+9LiJ+GhGvJA9vBRZnMK5Zz3Bro+Uhi4T/IHC2pLdLOgVYBWxv3EHScMPDS4EnMhjXrGe4tdHykDrhR8RR4HrgbuqJ/BsR8bikdZIuTXb7tKTHJT0MfBq4Ou24Zp3UjZunbm20TvPSCmZNXHfXdWzevZlrF1/Lpks2dTscs7ZNV8N3wjdr4JunVnZeS8esTb55ar3MCd+sgW+eWi9zwjebwjdPrVe5hm9m1kNcwzczMyd8M7OqcMI3M6sIJ3wzs4pwwjczqwgnfDOzinDCNzOrCCd8M7MC6eRKrU74ZmYF0smPufRMWzOzAshqpVbPtLVC6MaHipiVRR4rtTrhW246ealqVnZ5rNTal9k7mbUw9VJ1dHyU0fFRf6iI2RTHV2pds3gNY7vHqL1cy/T9M6nhS7oY+AJwEnBrRHxuyuvzgK8Bi4GfApdHxDMzva9r+L2hdqjG2nvWsu3JbRw+epjBvkFWnrOSmy+62evMm2WsozV8SScBG4GPAucCV0g6d8pu1wA/j4h3AX8JfD7tuFYe/lARs2LIooa/FNgTEXsj4lXg68CKKfusAG5Ltr8JfFiSMhjbSsIfKmLWfVnU8M8Anmt4vB94X6t9IuKopJeANwMvZjC+lcDWy7e+vr3xko1djMQsndqhGqu+tYotl20p3VVq4bp0JK2RNC5pfGJiotvhmJn9kjJ3m2WR8A8AZzU8PjN5ruk+kvqAX6N+8/YEETEWEUsiYsmCBQsyCM86zf31VgUDGwbQTWJ0fJRjcYzR8VF0kxjYMNDt0NqWRcJ/EDhb0tslnQKsArZP2Wc7cFWyfRnw/SjyFF+blTKf8Zi1K4+JUZ2Wuoaf1OSvB+6m3pb5lYh4XNI6YDwitgNfBv6XpD3Az6j/UbCSc3+9VUkvdJtlUsOPiB0R8e6IeGdEbEie+2yS7ImIyYj4RES8KyKWRsTeLMa17prNGY/LPtYLyt5t5pm2NmezOeNpLPtsumRTF6I1S6/s3WZO+JbKTFPBXfYxKw4vj2wd5WUVzPLl5ZGta3rhRpdZr3DCt44r+40us17hko6Z9bZaDVatgi1bYGHvX1m6pGNm1bV+PezcCes8MdAJ38x608AASDA6CseO1b9L9ecrygnfzHrT3r1w5ZUwWJ8YyOAgrF4N+8qzFELWnPDNrDxqNVi2DJ5v48b/8DAMDcHkJPT3178PDVWijt+KE76Zlcds6/EHD8LICOzaVf/ezh+KHuYuHTMrvoGB+hn6VP39cMQzthu5S8fMys31+Ew44ZtZ8bkenwknfDMrB9fjU/NqmWZWDlt/sTQxG8u3NHER+AzfzKwinPDNzCrCCd/MrCJSJXxJb5L0XUk/Tr6/scV+r0l6KPnanmZMMzObm7Rn+DcA34uIs4HvJY+bORIR/zb5ujTlmGZmNgdpE/4K4LZk+zbgYynfz8wKrHaoxrKvLvOH2JRU2oR/ekQc/9Tq54HTW+zXL2lc0i5JH5vuDSWtSfYdn5iYSBmemWVp/f3r2flPO1n3N15bvoxmXEtH0r1As+lsNwK3RcQbGvb9eUScUMeXdEZEHJD0DuD7wIcj4umZgvNaOmbFMLBhgMmjJ65l09/Xz5EbvZZNkaRaSyciPhIR5zf5uhM4KGk4GWQYeKHFexxIvu8F7gN+Y47/L1YAvqyvnr2f3suV51/JYF99LZvBvkFWv2c1+z7jtWzKJG1JZztwVbJ9FXDn1B0kvVHSvGR7PvBbwA9Tjmtd5Mv66hk+bZiheUNMvjZJf18/k69NMjRviIWnei2bMkm7tMLngG9IugZ4Fvh9AElLgJGI+CRwDrBZ0jHqf2A+FxFO+CU09bJ+dHyU0fFRX9ZXxMF/OcjI4hHWLF7D2O4xai/XZv6PrFC8Hr61rXaoxtp71rLtyW0cPnqYwb5BVp6zkpsvutlnemYF4fXwLRO+rM+f75dYlpzwbVaOX9bvumYXI4tHnIg6zPdLLEsu6ZgVkNsgba5c0jGbo26VVNwGaZ3ghG+uE0+jWyUV3y+xTnDCN9eJmxjYMIBuEqPjoxyLY4yOj6KbxMCGgdxi8P0Sy5pr+BXmOnFrbkG1snIN35pynbg1l1SsFznhV5iT2vRcUrFek3ZpBSs5T5dvbevlW1/f3njJxi5GYpYN1/DNzHqIa/hmZuaEb1Y0nhdhneKEb9YBaZK250VYp7iGb9YB1911HZt3b+baxdey6ZJNbf03nhdhWXAN3ywnaWboel6EdZoTvlmG0iTttuZF1GqwbBk87/q+zZ4TvlmG0k5mm3Gy1/r1sHMnrFvn5G+zlqqGL+kTwH+l/rm1SyOiacFd0sXAF4CTgFsj4nPtvL9r+FZGH9/ycYZPHf6lyWyNk7jmZGAAJk+s7wPwqU/BpvbuE1jvm66GnzbhnwMcAzYDa5slfEknAT8CfgfYDzwIXNHOB5k74ZslajVYuxa2bYPDh5vv098PR3xzt+o6dtM2Ip6IiKdm2G0psCci9kbEq8DXgRVpxjWrnOFhGBqqn+XPm1d/ri9ZGWVwEFavhn2+uWvTy6OGfwbwXMPj/clzTUlaI2lc0vjExETHgzMrjYMHYWQEHngAzjsPjh6tn9VPTtb/GCz0onc2vRkXT5N0L9DsJ+nGiLgz64AiYgwYg3pJJ+v3NyutrQ33Ad797voN2zVrYGysXvIxm8GMCT8iPpJyjAPAWQ2Pz0yeM7O5akz+G72Sp7Unj5LOg8DZkt4u6RRgFbA9h3HNzKxBqoQvaaWk/cAHgLsk3Z08/xZJOwAi4ihwPXA38ATwjYh4PF3Y0/PiU2ZmJ0rbpXNHRJwZEfMi4vSI+A/J8z+JiOUN++2IiHdHxDsjYkPaoGfixacsDz6xsLLpqcXTvPiU5WkuC6SZdVplFk/z4lOWhzQLpJl1U08lfH8ot+XBJxZWVj2V8KGNxafMUvKJhZXVjH34ZdO4SNXGS9yfbJ1x/MSicYE0s6LrqZu2NrPaoRqrvrWKLZdt8RmpWQ+qzE1bm5lbVs2qy2f4FeGWVeskXzkWh8/wzZ0l1lG+ciyHyiX8qs6OdGeJtWO2vx+ek1AulUv4VT4TccuqzWS2vx++ciyXytTwXcM2ay3N78envv0pxn4wxiknncKrr73qpSa6zDV8fCZiNp00vx++ciyPnpt41Ypr2Gatpfn98GTH8qjMGT74TMRsOv796H2VqeGbmVWBa/hWeFVtlzXLkxO+FUKV22XN8uKSjnWV22XNstWxko6kT0h6XNIxSU0HSPZ7RtKjkh6S5Axur3O7rFl+0rZlPgZ8HNjcxr7/PiJeTDme9Ri3y5rlJ9UZfkQ8ERFPZRWMVZPbAc3ykUkNX9J9wNqIaFqukbQP+DkQwOaIGJvmvdYAawDe+ta3Ln722WdTx2dmVhWpaviS7pX0WJOvFbOI4YMRcQHwUeCPJH2o1Y4RMRYRSyJiyYIFC2YxhFl5uS3V8jBjwo+Ij0TE+U2+7mx3kIg4kHx/AbgDWDr3kM16j9tSLQ8dX0tH0q8CvxIRh5LtiwD/VJtxYlvq6Pgoo+Ojbku1jkjblrlS0n7gA8Bdku5Onn+LpB3JbqcDOyU9DPwjcFdE/N8045r1CrelWp5SneFHxB3USzRTn/8JsDzZ3gu8N804Zr3KbamWJy+tYNZlbku1vHhpBTOzHuLVMs3MzAnfzKwqnPDNzCrCCd/MrCKc8M3MKsIJ38ysIpzwzcwqwgnfrMd5JU47zgnfrMd5JU47zjNtzXqUPyC+mjzT1qyCvBKnTeWEb9ajvBKnTeWEb9bDvBKnNXIN38ysh7iGb2ZmTvhmZlWR9jNt/1zSk5IekXSHpDe02O9iSU9J2iPphjRjmpnZ3KQ9w/8ucH5E/BvgR8CfTd1B0knARuCjwLnAFZLOTTmumZnNUqqEHxH3RMTR5OEu4Mwmuy0F9kTE3oh4Ffg6sCLNuGZmNntZ1vD/E/CdJs+fATzX8Hh/8lxTktZIGpc0PjExkWF4ZmbV1jfTDpLuBZrN1LgxIu5M9rkROAr8VdqAImIMGEved0LSs23+p/OBF9OO3yFFja2ocYFjm4uixgWObS7mGtfbWr0wY8KPiI9M97qkq4HfBT4czZv6DwBnNTw+M3luRhGxoJ39kjjGW/WedltRYytqXODY5qKocYFjm4tOxJW2S+di4E+BSyPicIvdHgTOlvR2SacAq4DtacY1M7PZS1vD/xJwGvBdSQ9JugVA0lsk7QBIbupeD9wNPAF8IyIeTzmumZnN0owlnelExLtaPP8TYHnD4x3AjjRjtWGsw++fRlFjK2pc4NjmoqhxgWObi8zjKvRaOmZmlh0vrWBmVhFO+GZmFVHahF/kdXwkfULS45KOSWrZViXpGUmPJje8O74O9Czi6sYxe5Ok70r6cfL9jS32ey05Xg9J6li310zHQNI8SVuS1x+QtKhTscwhtquTOSzHj9Mnc4rrK5JekPRYi9cl6YtJ3I9IuiCPuNqM7UJJLzUcs8/mFNdZkv5a0g+T383PNNknu+MWEaX8Ai4C+pLtzwOfb7LPScDTwDuAU4CHgXNziO0c4NeB+4Al0+z3DDA/x2M2Y1xdPGb/A7gh2b6h2b9n8trLOcQy4zEArgNuSbZXAVty+jdsJ7argS/l9XPVMO6HgAuAx1q8vpz6bHwB7wceKFBsFwLf7sIxGwYuSLZPo74m2dR/z8yOW2nP8KPA6/hExBMR8VSnx5mtNuPq1tpHK4Dbku3bgI/lMGYr7RyDxni/CXxYkgoSW1dExP3Az6bZZQXwtajbBbxB0nBBYuuKiKhFxA+S7UPUW9enLj2T2XErbcKfIpN1fLoggHsk7Za0ptvBJLp1zE6PiFqy/Txweov9+pO1lnZJ+liHYmnnGLy+T3Li8RLw5g7FM9vYAH4vufz/pqSzmrzeDUX/ffyApIclfUfSeXkPnpQFfwN4YMpLmR23VH34nZb3Oj5Zx9aGD0bEAUn/ivrktSeTM5Fux9UR08XW+CAiQlKrfuG3JcfsHcD3JT0aEU9nHWvJ/R/g9oh4RdK11K9EfrvLMRXdD6j/bL0saTmwDTg7r8ElnQp8C/jjiPjnTo1T6IQfXVzHJ21sbb7HgeT7C5LuoH65nirhZxBXV46ZpIOShiOillyuvtDiPY4fs72S7qN+RpR1wm/nGBzfZ7+kPuDXgJ9mHMecYouIxjhupX5/pAg69rOVVmOSjYgdkjZJmh8RHV9UTdLJ1JP9X0XE1ia7ZHbcSlvSUcnX8ZH0q5JOO75N/SZ00w6CnHXrmG0Hrkq2rwJOuBqR9EZJ85Lt+cBvAT/sQCztHIPGeC8Dvt/ipCP32KbUdy+lXhcugu3AHyZdJ+8HXmoo43WVpIXH78FIWko9N3b8D3gy5peBJyLiL1rslt1xy/uudFZfwB7qda2Hkq/jHRNvAXY07Lec+p3vp6mXNfKIbSX1OtsrwEHg7qmxUe+yeDj5ejyP2NqJq4vH7M3A94AfA/cCb0qeXwLcmmz/JvBocsweBa7pYDwnHANgHfUTDIB+4H8nP4f/CLwjj+PUZmz/PfmZehj4a+Bf5xTX7UAN+H/Jz9k1wAgwkrwu6p9+93Ty79eyg60LsV3fcMx2Ab+ZU1wfpH4v75GGXLa8U8fNSyuYmVVEaUs6ZmY2O074ZmYV4YRvZlYRTvhmZhXhhG9mVhFO+GZmFeGEb2ZWEf8fIxt3JCsjQd4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boolarray = np.array([x == y for x,y in zip(Y_test, pred_label)])\n",
    "plt.plot(X_test[boolarray, 0], X_test[boolarray, 1], \"*g\")\n",
    "plt.plot(X_test[boolarray == 0, 0], X_test[boolarray == 0, 1], \"*r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "e80b582d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d12057dd90>]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[pred_label == 1, 0], X_test[pred_label == 1, 1], \"*r\")\n",
    "plt.plot(X_test[pred_label == 0, 0], X_test[pred_label == 0, 1], \"*b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0068293b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d120529880>]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVx0lEQVR4nO3df6zddX3H8dfLQn/gQNDeQaFoQYsTnEx204kaZQMdNIZSlaRoImyYAkrc/mgWFhK3YMx0I1vmaAsNEutioI6VUrWugkgYyYqcmhZaaqVUDa0XegGDM62wynt/nO/Fw73n3Hvu/X7P9+fzkZzc7znn4/m8/fbeN5/z+b4/n68jQgCA+ntd0QEAAPJBwgeAhiDhA0BDkPABoCFI+ADQEMcUHcBk5s+fH4sWLSo6DACojO3btz8XEUPd3it1wl+0aJFarVbRYQBAZdj+ea/3mNIBgIYg4QNAQ5DwAaAhSPgA0BAkfABoCBI+gEyMjEgf/KD0zDNFR4JeSPgAMvGFL0gPPyzddFPRkaAXEj6AVObNk2xp7VrplVfaP+326ygXEj6AVPbvlz7xCem449rPjztO+uQnpZ/+tNi4MBEJH0AqCxZIJ5wg/eY30ty57Z8nnCCdckrRkWE8Ej6A1J59Vrr2WmnbtvZPLtyWU6n30gFQDRs3/u549eri4sDkGOEDBaCEEUUg4QMFoIQRRSDhAzmihBFFIuEDOaKEEUUi4QM5ooQRRSLhAzmjhBFFoSwTyBkljCgKI3wAaIhMEr7tO2wfsr2rx/sX2H7R9o7k8fks+gUA9C+rKZ2vSbpF0tcnafPfEfGRjPoDAExTJiP8iHhI0gtZfBYAYDDynMM/3/ZO29+1fU6vRrZX2m7Zbo2OjuYYHgDUW14J/0eS3hIR50r6N0mbejWMiHURMRwRw0NDQzmFBwD1l0vCj4hfRcSvk+Mtko61PT+PvgEAbbkkfNun2HZyvCTp9/k8+gYAtGVVlnmnpP+R9HbbB2xfbfta29cmTT4uaZftnZK+ImlFREQWfQP9mLAdMfsTo4EyKcuMiCumeP8Wtcs2gUJ0bke8Zk23F4D6c5kH2sPDw9FqtYoOAxU2b157g7Lx5uqIjui4jhfmSkeO5BcYMCC2t0fEcLf32FoBM1aFWZEJ2xHPe0WfXPSwfjovqQxmf2I0CAkfM1aFuzZN2I74pdfphON+q1Ne+jn7E6NxSPiYtqrdtWnCdsS/nM3+xGgk5vAxbSMj0qpV0qZN0uHD7VmR5culm29moAwUjTl8ZIq7NgHVRMLHjHDXJmD6ii504I5XmBHu2gRMX9HLPxjhA8CAlaXQgYQPAAM2YT1IQcs/SPgAMGBlKXQg4QNADspQ6MBFWwDIQRkKHRjhA0BDkPABoCFI+ADQECR8AGgIEj4ANAQJv+KK3psDQHVkdRPzO2wfsr2rx/u2/RXb+2w/Zvu8LPpFNW5CAqAcshrhf03SxZO8f4mkxcljpaS1GfXbWGXZmwNAdWSS8CPiIUkvTNJkmaSvR9s2SSfaXpBF301Vlr05AFRHXnP4p0l6uuP5geS1CWyvtN2y3RodHc0luCoqy94cKEgZL96UMSa8Ruku2kbEuogYjojhoaGhosMptTLszYGClPHiTRljwmtkdk9b24skfTsi3tnlvdskPRgRdybP90q6ICJGJvtM7mnbACMj0ooV0oYNfD3px7x57a9z482dKx05kn88UjljarAy3NN2s6RPJdU675H04lTJHg3BqHB6ynjxpowxoatMdsu0faekCyTNt31A0t9JOlaSIuJWSVskLZW0T9JhSX+RRb+osPGjwrVr2w9GhZMr48WbMsaErrKq0rkiIhZExLERsTAivhoRtybJXkl1zmcj4q0R8YcRwTxN0zEqnLkyXrwpY0yYgP3wUQxGhTNXho3VxytjTJigdFU6aBBGhUCuGOGjOHmMCqkCAl7FCB/1RhUQ8CoSPuop7WZDrBpFDZHwUU9pq4D4ZoAaIuGjnmZaBcQ2pKgxEj7qayZVQKwPQI1RpYP6mkkVEOsDUGOM8IHxWB+AmmKED4zHqlHUFCN8AGgIEj4ANAQJH0A2WKxWeiR8ANlgsVrpkfABpMNitcog4QNIh8VqlUHCB5AOi9UqI5OEb/ti23tt77N9Q5f3r7I9antH8vh0Fv0CKAkWq1VC6oVXtmdJWi3pQ5IOSHrU9uaIeGJc0w0RcX3a/gCUEIvVKiGLEf4SSfsiYn9EvCzpLknLMvhcoHkobcQAZZHwT5P0dMfzA8lr433M9mO277Z9eq8Ps73Sdst2a3R0NIPwgAqhtBEDlNdF229JWhQR75J0n6T1vRpGxLqIGI6I4aGhoZzCAwpGaSNykEXCPyipc8S+MHntVRHxfES8lDy9XdIfZ9AvUB+UNiIHWST8RyUttn2G7dmSVkja3NnA9oKOp5dK2pNBv0B9UNqIHKRO+BFxVNL1kraqnci/GRG7bd9k+9Kk2eds77a9U9LnJF2Vtl9goIq4eEppIwbMEVF0DD0NDw9Hq9UqOgw00Wc+I912m3TNNdKaNUVHA/TN9vaIGO72HittgU5cPEWNkfCBTlw8RY2R8IFOXDxFjZHwgfG4eIqa4ibmwHjsC4OaYoQPAA1BwgeAhiDhA0BDkPABoCFI+ADQECR8AGgIEj4ANAQJHwBKZJAbtZLwAaBEBnmXSxI+AJRAHhu1kvCRnyJuKgJURB4btZLwkZ9BflcFKi6PjVpJ+Bg8bioC9GXQG7VmkvBtX2x7r+19tm/o8v4c2xuS9x+xvSiLflER3FQE6MvGje0NWs89t/2zc+PWLKRO+LZnSVot6RJJZ0u6wvbZ45pdLemXEfE2Sf8i6ctp+0WFcFMRoBSyGOEvkbQvIvZHxMuS7pK0bFybZZLWJ8d3S7rQtjPoG1XBTUWAwmVxA5TTJD3d8fyApD/p1SYijtp+UdKbJD2XQf+oAm4qgpoYGZFWrJA2bKjel9TSXbS1vdJ2y3ZrdHS06HAA4DWqXGyWRcI/KOn0jucLk9e6trF9jKQ3SHq+24dFxLqIGI6I4aGhoQzCw8BRX48GqEOxWRYJ/1FJi22fYXu2pBWSNo9rs1nSlcnxxyU9EBGRQd8ogyoPeYA+1aHYLPUcfjInf72krZJmSbojInbbvklSKyI2S/qqpH+3vU/SC2r/RwFVN29eu+JmzNq17cfcudKRI8XFBQxAHYrNMpnDj4gtEXFWRLw1Ir6YvPb5JNkrIn4TEZdHxNsiYklE7M+iXxRsOkMepn1QA1UvNsuiSgdNNZ0hT+e0z5o1+ccKZKDqxWalq9JBxUw15KnDlS6gJhjhI52phjz790urVkmbNkmHD7enfZYvl26+ObcQAbQxwsdg1eFKF1ATJHwMXtWvdAE1wZQOBq/qV7pQbVXeCyFjjPAB1BsLA19FwgdQT1SITUDCB1BPddgLIWMkfADVMZ0V21SITUDCB1Ad052Pp0LsNVzmTSuHh4ej1WoVHQaAoo3fqG8MG/VNYHt7RAx3e48RPoDyYz4+EyR8AOXHfHwmSPgAqoH5+NRYaQugGlixnRojfABoCBI+ADQECR8AGiJVwrf9Rtv32X4y+XlSj3a/tb0jeWxO0ycAYGbSjvBvkPT9iFgs6fvJ826ORMQfJY9LU/YJAJiBtAl/maT1yfF6SZel/DwAZTadvWxQOmkT/skRMZIcPyPp5B7t5tpu2d5m+7LJPtD2yqRta3R0NGV4ADLF3vKVNuVeOrbvl9RtOduNktZHxIkdbX8ZERPm8W2fFhEHbZ8p6QFJF0bEU1MFx146QEmwl01lpNpLJyIuioh3dnncK+lZ2wuSThZIOtTjMw4mP/dLelDSu2f4/wVlwNf65mEvm1pIO6WzWdKVyfGVku4d38D2SbbnJMfzJb1P0hMp+0WR+FrfPOxlUwtpE/6XJH3I9pOSLkqey/aw7duTNu+Q1LK9U9IPJH0pIkj4VcQt45qNvWwqj/3w0b+REWnVKmnTJunw4fbX+uXLpZtvZqQHlAT74SMbfK3PH9dLkCESPqaHr/X54noJMsSUDlBGlEFihpjSAWaqqCkVyiAxACR8ME88maKmVLheggEg4YN54m7KUILK9RJkjDn8JmOeuDdKUFFRzOGjO+aJe2NKBTVEwm8yktrkmFJBzRxTdAAo2FhSW7lSWreuPZWBto0bf3e8enVxcQAZIeE3HUkNaAymdACgIUj4QNmwLgIDQsIHBiFN0mZdBAaEhA8MwkySdhkWe6HWSPhAltIkbdZFYMBI+ECW0iTtftZFML+PFEj4QJbSLmabarFX51QRyR/TlGovHduXS/p7te9buyQium58Y/tiSf8qaZak2yPiS/18PnvpoJI++tF24u9czNa53mEmeu17JEnXXSetWZPu81Ebk+2lkzbhv0PSK5Juk7SqW8K3PUvSTyR9SNIBSY9KuqKfG5mT8IHE+M3cumHTO2iAm6dFxJ6I2DtFsyWS9kXE/oh4WdJdkpal6RdonM6pojlz2q8dkyyU5+Iu+pTHHP5pkp7ueH4gea0r2yttt2y3RkdHBx4cUBlj8/uPPCKdc4509Cib3mFaptxLx/b9krr9Jt0YEfdmHVBErJO0TmpP6WT9+UBldV4HOOus9gVbNr3DNEyZ8CPiopR9HJR0esfzhclrAGaKTe8wA3lM6TwqabHtM2zPlrRC0uYc+gUAdEiV8G0vt31A0vmSvmN7a/L6qba3SFJEHJV0vaStkvZI+mZE7E4X9uQoTwaAidJW6dwTEQsjYk5EnBwRf568/ouIWNrRbktEnBURb42IL6YNeirsPYVcMLJAxdRqpS17TyFXjCxQMbVK+Ow9hVwwskBF1Srhc09u5IKRBSqqVglfmnrvKSA1RhaoqNrdxJzyZORibGTBwidUSO0SPiY3MiKtWCFt2MCANBVGFqig2k3pYHIUlgDNRcJvCApLMEgsSagGEn5DUFiCQeKbYzU0LuE3dSRCYQn6Md2/D745VkvjEn6TRyKUrGIq0/374JtjtaS6xeGgZXmLw163BOWucEC6v4/rrmtXps6eLb38snTNNdxit0gDu8VhlTASAXpL8/fBN8fqaEwdPnPYQG9p/j5YklAdjRnhS4xEgMnw91F/jZnDB4AmYA4fpdfUclkgTyR8lEKTy2WBvJDwUSgW7gD5SXsT88tt77b9iu2uc0ZJu5/Zftz2DttMyuNVlMsC+UlblrlL0kcl3dZH2z+NiOdS9oeaoVwWyE+qEX5E7ImIvVkFg2aiHBDIR14Lr0LS92yHpNsiYl2vhrZXSlopSW9+85tzCg9FYuEOkI8pR/i277e9q8tj2TT6eX9EnCfpEkmftf2BXg0jYl1EDEfE8NDQ0DS6AKqLslTkYcoRfkRclLaTiDiY/Dxk+x5JSyQ9lPZzgbroLEtl4zEMysDLMm2/3vbxY8eSPqz2xV6g8ShLRZ7SlmUut31A0vmSvmN7a/L6qba3JM1OlvSw7Z2SfijpOxHxX2n6BeqCslTkKdVF24i4R9I9XV7/haSlyfF+Seem6QeoK8pSkSdW2gIFoywVeWnMfvhAWVGWirwwwgeAhiDhA0BDkPABoCFI+ADQECR8AGgIEj4ANAQJHwAagoQP1Bw7cWIMCR+oOW4QjzEkfKCm2IkT45HwgZpiJ06MR8IHaoqdODEeCR+oMXbiRCd2ywRqjJ040YkRPgA0BAkfABoi7T1t/8n2j20/Zvse2yf2aHex7b2299m+IU2fAICZSTvCv0/SOyPiXZJ+IulvxzewPUvSakmXSDpb0hW2z07ZLwBgmlIl/Ij4XkQcTZ5uk7SwS7MlkvZFxP6IeFnSXZKWpekXADB9Wc7h/6Wk73Z5/TRJT3c8P5C81pXtlbZbtlujo6MZhgcAzTZlWabt+yV1W6pxY0Tcm7S5UdJRSd9IG1BErJO0LvncUds/7/N/Ol/Sc2n7H5CyxlbWuCRim4myxiUR20zMNK639HpjyoQfERdN9r7tqyR9RNKFERFdmhyUdHrH84XJa1OKiKF+2iVxtCJiuN/2eSprbGWNSyK2mShrXBKxzcQg4kpbpXOxpL+RdGlEHO7R7FFJi22fYXu2pBWSNqfpFwAwfWnn8G+RdLyk+2zvsH2rJNk+1fYWSUou6l4vaaukPZK+GRG7U/YLAJimVFsrRMTberz+C0lLO55vkbQlTV99WDfgz0+jrLGVNS6J2GairHFJxDYTmcfl7tPuAIC6YWsFAGgIEj4ANERlE36Z9/Gxfbnt3bZfsd2zrMr2z2w/nlzwbpUoriLO2Rtt32f7yeTnST3a/TY5XztsD6zaa6pzYHuO7Q3J+4/YXjSoWGYQ21XJGpax8/TpnOK6w/Yh27t6vG/bX0nifsz2eXnE1WdsF9h+seOcfT6nuE63/QPbTyR/m3/VpU125y0iKvmQ9GFJxyTHX5b05S5tZkl6StKZkmZL2inp7Bxie4ekt0t6UNLwJO1+Jml+judsyrgKPGf/KOmG5PiGbv+eyXu/ziGWKc+BpM9IujU5XiFpQ07/hv3EdpWkW/L6vero9wOSzpO0q8f7S9VejW9J75H0SIliu0DStws4ZwsknZccH6/2nmTj/z0zO2+VHeFHiffxiYg9EbF30P1MV59xFbX30TJJ65Pj9ZIuy6HPXvo5B53x3i3pQtsuSWyFiIiHJL0wSZNlkr4ebdsknWh7QUliK0REjETEj5Lj/1W7dH381jOZnbfKJvxxMtnHpwAh6Xu2t9teWXQwiaLO2ckRMZIcPyPp5B7t5iZ7LW2zfdmAYunnHLzaJhl4vCjpTQOKZ7qxSdLHkq//d9s+vcv7RSj73+P5tnfa/q7tc/LuPJkWfLekR8a9ldl5K/UtDvPexyfr2Prw/og4aPv31V689uNkJFJ0XAMxWWydTyIibPeqF35Lcs7OlPSA7ccj4qmsY624b0m6MyJesn2N2t9E/qzgmMruR2r/bv3a9lJJmyQtzqtz278n6T8l/XVE/GpQ/ZQ64UeB+/ikja3PzziY/Dxk+x61v66nSvgZxFXIObP9rO0FETGSfF091OMzxs7ZftsPqj0iyjrh93MOxtocsH2MpDdIej7jOGYUW0R0xnG72tdHymBgv1tpdSbZiNhie43t+REx8E3VbB+rdrL/RkRs7NIks/NW2SkdV3wfH9uvt3382LHaF6G7VhDkrKhztlnSlcnxlZImfBuxfZLtOcnxfEnvk/TEAGLp5xx0xvtxSQ/0GHTkHtu4+d1L1Z4XLoPNkj6VVJ28R9KLHdN4hbJ9ytg1GNtL1M6NA/8PeNLnVyXtiYh/7tEsu/OW91XprB6S9qk9r7UjeYxVTJwqaUtHu6VqX/l+Su1pjTxiW672PNtLkp6VtHV8bGpXWexMHrvziK2fuAo8Z2+S9H1JT0q6X9Ibk9eHJd2eHL9X0uPJOXtc0tUDjGfCOZB0k9oDDEmaK+k/kt/DH0o6M4/z1Gds/5D8Tu2U9ANJf5BTXHdKGpH0f8nv2dWSrpV0bfK+1b773VPJv1/PCrYCYru+45xtk/TenOJ6v9rX8h7ryGVLB3Xe2FoBABqislM6AIDpIeEDQEOQ8AGgIUj4ANAQJHwAaAgSPgA0BAkfABri/wH1lzZoj/4otwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], \"*r\")\n",
    "plt.plot(X_test[Y_test == 0, 0], X_test[Y_test == 0, 1], \"*b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278d8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
