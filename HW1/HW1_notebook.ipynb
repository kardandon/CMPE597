{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c46a73",
   "metadata": {},
   "source": [
    "### Izzet Emre Kucukkaya\n",
    "# CMPE597 HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec4710",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a758e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from CNN_Class import Network_Model, Conv2D_Layer, MaxPooling2D_Layer, FC_Layer, default_optimizer, cross_entropy, Softmax_Layer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a70276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "dataset = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "X_train = dataset[0][0]\n",
    "Y_train = dataset[0][1]\n",
    "X_test = dataset[1][0]\n",
    "Y_test = dataset[1][1]\n",
    "\n",
    "X_train = np.array(X_train.astype(\"float32\") / 255) - 0.5\n",
    "X_test = np.array(X_test.astype(\"float32\") / 255) - 0.5\n",
    "\n",
    "X_train = np.expand_dims(X_train, 1)\n",
    "X_test = np.expand_dims(X_test, 1)\n",
    "\n",
    "#train_set = [[x,y] for x, y in zip(X_train, Y_train)]\n",
    "#test_set = [[x,y] for x, y in zip(X_test, Y_test)]\n",
    "#print(test_set)\n",
    "\n",
    "\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "x, y = train_set[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "print(train_set.data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6914933",
   "metadata": {},
   "source": [
    "### Using Own Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network_Model(cross_entropy, default_optimizer)\n",
    "model.add(Conv2D_Layer(channels=4, stride=1, kernel_size=5, activation=\"relu\"))\n",
    "model.add(MaxPooling2D_Layer(size=2))\n",
    "model.add(Conv2D_Layer(channels=8, stride=1, kernel_size=5, activation=\"relu\"))\n",
    "model.add(MaxPooling2D_Layer(size=2))\n",
    "model.add(FC_Layer(innode=784, outnode=128, activation=\"relu\"))\n",
    "model.add(FC_Layer(innode=128, outnode=10, activation=None))\n",
    "model.add(Softmax_Layer())\n",
    "model.train(X_train, Y_train, X_test, Y_test, epochs=5, lr=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52e2b64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==> Epoch 0\n",
      "loss: 0.915794822652423 accuracy: 72.41166666666666 val_loss: 0.5447461418365205 val_accuracy: 82.28 \n",
      "\n",
      " ==> Epoch 1\n",
      "loss: 0.507893183005401 accuracy: 84.565 val_loss: 0.45554378301760473 val_accuracy: 85.96000000000001 \n",
      "\n",
      " ==> Epoch 2\n",
      "loss: 0.43738835383505115 accuracy: 87.10166666666666 val_loss: 0.40009723801150765 val_accuracy: 88.36 \n",
      "\n",
      " ==> Epoch 3\n",
      "loss: 0.3959499268772749 accuracy: 88.69500000000001 val_loss: 0.3703774111705928 val_accuracy: 89.51 \n",
      "\n",
      " ==> Epoch 4\n",
      "loss: 0.3718569841044408 accuracy: 89.62 val_loss: 0.3514857076954318 val_accuracy: 90.25 \n"
     ]
    }
   ],
   "source": [
    "model = Network_Model(cross_entropy, default_optimizer)\n",
    "#model.add(Conv2D_Layer(channels=4, stride=1, kernel_size=5, activation=\"relu\"))\n",
    "#model.add(MaxPooling2D_Layer(size=2))\n",
    "#model.add(Conv2D_Layer(channels=8, stride=1, kernel_size=5, activation=\"relu\"))\n",
    "#model.add(MaxPooling2D_Layer(size=2))\n",
    "model.add(FC_Layer(innode=784, outnode=128, activation=\"relu\"))\n",
    "model.add(FC_Layer(innode=128, outnode=10, activation=None))\n",
    "model.add(Softmax_Layer())\n",
    "model.train(X_train, Y_train, X_test, Y_test, epochs=5, lr=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49f5152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Y = [(np.asarray(x), y) for x, y in test_set]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test))\n\u001b[1;32m----> 3\u001b[0m x,y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x,y)\n\u001b[0;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X_train[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:104\u001b[0m, in \u001b[0;36mNetwork_Model.eval\u001b[1;34m(self, X_test, Y_test)\u001b[0m\n\u001b[0;32m    102\u001b[0m loss, tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Y_test)):\n\u001b[1;32m--> 104\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(Y_test[i], output)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(output) \u001b[38;5;241m==\u001b[39m Y_test[i]:\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:16\u001b[0m, in \u001b[0;36mNetwork_Model.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 16\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:180\u001b[0m, in \u001b[0;36mConv2D_Layer.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_output \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 180\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReLU\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\numpy\\lib\\function_base.py:2304\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[0;32m   2302\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[1;32m-> 2304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\numpy\\lib\\function_base.py:2387\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m   2385\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m-> 2387\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2390\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:127\u001b[0m, in \u001b[0;36mReLU\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReLU\u001b[39m(x):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2791\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2677\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2792\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Y = [(np.asarray(x), y) for x, y in test_set]\n",
    "print(len(X_test))\n",
    "x,y = model.eval(X_test, Y_test)\n",
    "print(x,y)\n",
    "output = model.forward(X_train[0])\n",
    "print(output, Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3620766d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26196299 0.51155549 0.20483456 0.5286889  0.26470733 0.27759341\n",
      " 0.4393441  0.37026027 0.39098701 0.47111191 0.31117468 0.36628441\n",
      " 0.30842754 0.40174261 0.16201551 0.33715956 0.15713726 0.16156354\n",
      " 0.2732164  0.07819482 0.37020983 0.33792255 0.20407707 0.41528404\n",
      " 0.34058479 0.40788435 0.16451815 0.33657597 0.52788173 0.30613796\n",
      " 0.26222565 0.23505825 0.43628606 0.35245653 0.28335547 0.29485247\n",
      " 0.38437934 0.18201721 0.21252346 0.45360138 0.31635419 0.23005531\n",
      " 0.28487507 0.2875677  0.40293527 0.30722823 0.15212712 0.32656111\n",
      " 0.39317303 0.27055397 0.27368961 0.20279693 0.39912879 0.40488398\n",
      " 0.50533735 0.3682617  0.28810868 0.40871992 0.29683387 0.31612954\n",
      " 0.36023985 0.28342    0.09916922 0.31316584 0.25817676 0.31479278\n",
      " 0.28132231 0.19254478 0.36626594 0.27540002 0.46929745 0.36560387\n",
      " 0.1867702  0.515943   0.36336947 0.42680612 0.35720566 0.36693548\n",
      " 0.33425489 0.22588023 0.29967457 0.24790375 0.25956882 0.2822315\n",
      " 0.46644702 0.38483274 0.20778596 0.17293987 0.3785035  0.17589173\n",
      " 0.28238688 0.42696841 0.29895825 0.30587301 0.12090054 0.41658868\n",
      " 0.23070677 0.37984781 0.25739018 0.38402298]\n",
      "0\n",
      "[1.05750909 0.91754838 0.86590657 0.66232445 0.58776627 0.63106438\n",
      " 0.79452227 0.59390555 0.54099426 0.50842079 0.90351537 0.51991898\n",
      " 0.52632356 0.37539805 0.45843956 0.74745318 0.52812206 0.32733857\n",
      " 0.44187512 0.65106806 0.70014041 0.39107121 0.51126209 0.5369688\n",
      " 0.52148402 0.92096032 0.97722277 0.67229794 0.52186401 0.60266451\n",
      " 1.00716625 0.77125983 0.69139513 0.5701195  0.76755774 0.96941095\n",
      " 0.53062864 0.55660977 0.3693895  0.46667544 0.87575725 0.51304154\n",
      " 0.63758641 0.61530761 0.4641805  0.68724423 0.52384821 0.57616882\n",
      " 0.55766534 0.39447862 0.95623473 0.78437051 0.68023055 0.7900415\n",
      " 0.81748155 0.81027484 0.71180508 0.68408125 0.46279319 0.55030725\n",
      " 0.75848177 0.61724901 0.56915788 0.51954967 0.35457005 0.67366006\n",
      " 0.69416199 0.57650132 0.50181483 0.4825354  0.69772625 0.68774018\n",
      " 0.51953534 0.46799003 0.59081866 1.20613238 0.83799632 0.82433982\n",
      " 0.65549476 0.6820754  0.88308332 0.7430712  0.63298328 0.61349777\n",
      " 0.57105083 0.81038235 0.61359656 0.43370591 0.39191698 0.47791845\n",
      " 0.73056132 0.66209574 0.42465873 0.34348681 0.36223187 0.6671784\n",
      " 0.60143849 0.42337586 0.43831581 0.42590334 1.0050452  1.1533682\n",
      " 0.56659945 0.62253597 0.87271373 0.95666925 0.65105849 0.4727885\n",
      " 0.70871208 0.46827219 0.62831834 0.61686736 0.55808146 0.66123084\n",
      " 0.55344877 0.5665595  0.53290228 0.36646468 0.52579313 0.65914203\n",
      " 0.72636204 0.66358744 0.60563282 0.57042329 0.30764145 1.03464094\n",
      " 0.91511485 0.81464713 0.55449149 0.59859725 1.01794616 0.69007783\n",
      " 0.60688307 0.59212692 0.45885491 0.62826283 0.63828922 0.39569477\n",
      " 0.31811877 0.33080697 0.60803554 0.61299957 0.47529638 0.3918134\n",
      " 0.2994464  0.62637782 0.39723883 0.35960617 0.40211464 0.41097015\n",
      " 1.0705051  0.75006951 0.60776689 0.49003783 0.79979525 0.90371415\n",
      " 0.80786857 0.70833457 0.44431342 0.55667474 0.82860952 0.69954792\n",
      " 0.54861695 0.48650935 0.61844347 0.71750146 0.51984757 0.32006822\n",
      " 0.46127165 0.72357755 0.68399518 0.84583555 0.63312847 0.65292066\n",
      " 0.43951437 1.00192354 0.77831306 0.72657421 0.72072482 0.88404321\n",
      " 0.78487643 0.57785366 0.68566486 0.33788466 0.35438304 0.72651089\n",
      " 0.60392773 0.42028724 0.43098866 0.63497787 0.62173065 0.49026323\n",
      " 0.42006226 0.58649368 0.51465649 0.8312887  0.6138113  0.40697933\n",
      " 0.45526721 0.51022095]\n",
      "0\n",
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c957e3",
   "metadata": {},
   "source": [
    "### Using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923d432e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchNetwork(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PytorchNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(PytorchNetwork, self).__init__()\n",
    "\n",
    "      self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n",
    "      self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "      self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=5, stride=1)\n",
    "      self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "      self.fc1 = nn.Linear(128, 128)\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "      x = self.conv1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.pool1(x)\n",
    "    \n",
    "      x = self.conv2(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.pool2(x)\n",
    "      \n",
    "      x = torch.flatten(x, 1)\n",
    "\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      output = F.log_softmax(x, dim=1)\n",
    "      return output\n",
    "\n",
    "model = PytorchNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86fe1630",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# trainning\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      5\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      6\u001b[0m         x, target \u001b[38;5;241m=\u001b[39m Variable(x), Variable(target)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    169\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    169\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:180\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    # trainning\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        #optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, total_loss))\n",
    "    # testing\n",
    "    correct_cnt = 0\n",
    "    total_cnt = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        total_loss += loss.item()\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, total_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d9e36",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6059ebc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x230acb66400>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = np.asarray(np.load(\"./Data/X_test.npy\"), dtype=np.float32) / 4\n",
    "X_train = np.asarray(np.load(\"./Data/X_train.npy\"), dtype=np.float32) / 4\n",
    "Y_test = np.asarray(np.load(\"./Data/Y_test.npy\"))\n",
    "Y_train = np.asarray(np.load(\"./Data/Y_train.npy\"))\n",
    "# Linearly seperable\n",
    "\"\"\"\n",
    "X_train = np.random.randn(200,2)\n",
    "X_train[0:100,:] *= 0.1\n",
    "X_train[0:100,:] -= 0.25\n",
    "X_train[100:200,:] *= 0.1\n",
    "X_train[100:200,:] += 0.25\n",
    "Y_train = np.ones(200, dtype=np.int32)\n",
    "Y_train[0:100] = 0\n",
    "X_test = X_train\n",
    "Y_test = Y_train\n",
    "\"\"\"\n",
    "X_grid = np.zeros((10000,2), dtype=np.float32)\n",
    "\n",
    "for i in range(100):\n",
    "    X_grid[i*100:(i+1)*100, 0] = np.linspace(-0.5,0.5,100)\n",
    "    X_grid[i*100:(i+1)*100, 1] = -0.5 + i / 100\n",
    "        \n",
    "\n",
    "#train_set = np.array([[x,y] for x, y in zip(X_train, Y_train)])\n",
    "#test_set = np.array([[x,y] for x, y in zip(X_test, Y_test)])\n",
    "plt.plot(X_test[Y_test == 0, 0], X_test[Y_test == 0, 1], \"*r\")\n",
    "plt.plot(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], \"*b\")\n",
    "plt.plot(X_train[Y_train == 0, 0], X_train[Y_train == 0, 1], \"or\")\n",
    "plt.plot(X_train[Y_train == 1, 0], X_train[Y_train == 1, 1], \"ob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c728c6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==> Epoch 0\n",
      "loss: 0.7320553926156594 accuracy: 46.666666666666664 val_loss: 0.7477132395968071 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 1\n",
      "loss: 0.7353007176122824 accuracy: 45.0 val_loss: 0.7480753615242832 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 2\n",
      "loss: 0.7352745963889833 accuracy: 45.0 val_loss: 0.7478557192088058 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 3\n",
      "loss: 0.7352527550111032 accuracy: 45.0 val_loss: 0.7478272719888273 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 4\n",
      "loss: 0.7352109364313167 accuracy: 45.0 val_loss: 0.7478750060495957 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 5\n",
      "loss: 0.735216677749437 accuracy: 45.0 val_loss: 0.7478874921284815 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 6\n",
      "loss: 0.7352102283685299 accuracy: 45.0 val_loss: 0.7478725352015303 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 7\n",
      "loss: 0.7352031447947351 accuracy: 45.0 val_loss: 0.7478671454939828 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 8\n",
      "loss: 0.7352003565487266 accuracy: 45.0 val_loss: 0.7478649228222168 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 9\n",
      "loss: 0.7351989431409629 accuracy: 45.0 val_loss: 0.7478637534013831 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 10\n",
      "loss: 0.7351979916618208 accuracy: 45.0 val_loss: 0.7478635906971763 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 11\n",
      "loss: 0.735197594513655 accuracy: 45.0 val_loss: 0.7478637000649517 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 12\n",
      "loss: 0.7351989573610244 accuracy: 45.0 val_loss: 0.7478647963761436 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 13\n",
      "loss: 0.7352040398146075 accuracy: 45.0 val_loss: 0.7478659495907481 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 14\n",
      "loss: 0.7352081065196614 accuracy: 45.0 val_loss: 0.7478711583214473 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 15\n",
      "loss: 0.7352076451783369 accuracy: 45.0 val_loss: 0.7478772460539623 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 16\n",
      "loss: 0.7352082084224663 accuracy: 45.0 val_loss: 0.7478797551452437 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 17\n",
      "loss: 0.7352092692642065 accuracy: 45.0 val_loss: 0.747883046631545 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 18\n",
      "loss: 0.7352099093979739 accuracy: 45.0 val_loss: 0.7478864242380362 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 19\n",
      "loss: 0.7352101024308596 accuracy: 45.0 val_loss: 0.7478899295009097 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 20\n",
      "loss: 0.7352089019072812 accuracy: 45.0 val_loss: 0.7478935609327022 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 21\n",
      "loss: 0.7352084338104581 accuracy: 45.0 val_loss: 0.7478959928542547 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 22\n",
      "loss: 0.7352085397491762 accuracy: 45.0 val_loss: 0.7478981676906792 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 23\n",
      "loss: 0.7352089958711812 accuracy: 45.0 val_loss: 0.7479003982905428 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 24\n",
      "loss: 0.7352092578968652 accuracy: 45.0 val_loss: 0.747902811997289 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 25\n",
      "loss: 0.7352095610775315 accuracy: 45.0 val_loss: 0.7479053430630811 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 26\n",
      "loss: 0.7352100911928576 accuracy: 45.0 val_loss: 0.7479078176243892 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 27\n",
      "loss: 0.7352105758487767 accuracy: 45.0 val_loss: 0.7479104505738736 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 28\n",
      "loss: 0.735211269953019 accuracy: 45.0 val_loss: 0.7479129735124338 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 29\n",
      "loss: 0.7352107372610743 accuracy: 45.0 val_loss: 0.747917065284514 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 30\n",
      "loss: 0.7352112451028059 accuracy: 45.0 val_loss: 0.7479193916426913 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 31\n",
      "loss: 0.7352119675456051 accuracy: 45.0 val_loss: 0.7479218743461234 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 32\n",
      "loss: 0.735212846274825 accuracy: 45.0 val_loss: 0.7479243230021334 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 33\n",
      "loss: 0.7352123963202153 accuracy: 45.0 val_loss: 0.7479284049476177 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 34\n",
      "loss: 0.735213093103759 accuracy: 45.0 val_loss: 0.7479306882248205 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 35\n",
      "loss: 0.7352139822779555 accuracy: 45.0 val_loss: 0.7479330954624028 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 36\n",
      "loss: 0.7352147682615313 accuracy: 45.0 val_loss: 0.7479357213414215 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 37\n",
      "loss: 0.7352133775617334 accuracy: 45.0 val_loss: 0.7479401602560087 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 38\n",
      "loss: 0.7352122707967865 accuracy: 45.0 val_loss: 0.7479417296269093 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 39\n",
      "loss: 0.7352117938784718 accuracy: 45.0 val_loss: 0.7479428309473242 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 40\n",
      "loss: 0.7352132936905241 accuracy: 45.0 val_loss: 0.7479448425992288 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 41\n",
      "loss: 0.7352148889500867 accuracy: 45.0 val_loss: 0.7479477700125678 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 42\n",
      "loss: 0.735214374559124 accuracy: 45.0 val_loss: 0.7479511374254442 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 43\n",
      "loss: 0.7352139659035175 accuracy: 45.0 val_loss: 0.747952792630208 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 44\n",
      "loss: 0.7352142791643849 accuracy: 45.0 val_loss: 0.747954595102045 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 45\n",
      "loss: 0.7352150078196451 accuracy: 45.0 val_loss: 0.7479564911545042 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 46\n",
      "loss: 0.7352169419272501 accuracy: 45.0 val_loss: 0.7479592790651877 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 47\n",
      "loss: 0.7352168121332324 accuracy: 45.0 val_loss: 0.7479663777918999 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 48\n",
      "loss: 0.7352152401980507 accuracy: 45.0 val_loss: 0.7479672863256842 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 49\n",
      "loss: 0.7352156464386396 accuracy: 45.0 val_loss: 0.7479687872068349 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 50\n",
      "loss: 0.7352166592304079 accuracy: 45.0 val_loss: 0.7479705439489904 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 51\n",
      "loss: 0.7352177524909483 accuracy: 45.0 val_loss: 0.7479726322208585 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 52\n",
      "loss: 0.7352186242385571 accuracy: 45.0 val_loss: 0.7479754783795757 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 53\n",
      "loss: 0.7352194660612769 accuracy: 45.0 val_loss: 0.7479782159797963 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 54\n",
      "loss: 0.73522014695114 accuracy: 45.0 val_loss: 0.7479810891339025 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 55\n",
      "loss: 0.7352206437335973 accuracy: 45.0 val_loss: 0.7479840748968116 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 56\n",
      "loss: 0.7352212763903607 accuracy: 45.0 val_loss: 0.7479869645167946 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 57\n",
      "loss: 0.7352217359808466 accuracy: 45.0 val_loss: 0.7479899590483069 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 58\n",
      "loss: 0.7352223630628362 accuracy: 45.0 val_loss: 0.7479928392817806 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 59\n",
      "loss: 0.7352227804081067 accuracy: 45.0 val_loss: 0.7479958682335833 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 60\n",
      "loss: 0.7352219326610675 accuracy: 45.0 val_loss: 0.7480035739813022 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 61\n",
      "loss: 0.7352199002109787 accuracy: 45.0 val_loss: 0.7480027317691818 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 62\n",
      "loss: 0.735220595304714 accuracy: 45.0 val_loss: 0.7480033086277083 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 63\n",
      "loss: 0.7352207971908971 accuracy: 45.0 val_loss: 0.748005211240421 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 64\n",
      "loss: 0.7352207679509538 accuracy: 45.0 val_loss: 0.7480073948985035 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 65\n",
      "loss: 0.735220770132879 accuracy: 45.0 val_loss: 0.7480083956424207 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 66\n",
      "loss: 0.7352216603416537 accuracy: 45.0 val_loss: 0.7480109912517685 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 67\n",
      "loss: 0.7352207785958773 accuracy: 45.0 val_loss: 0.7480133716350743 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 68\n",
      "loss: 0.7352210182199147 accuracy: 45.0 val_loss: 0.7480149057122442 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 69\n",
      "loss: 0.7352222148100453 accuracy: 45.0 val_loss: 0.7480177832913542 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 70\n",
      "loss: 0.7352214793235531 accuracy: 45.0 val_loss: 0.7480203920227793 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 71\n",
      "loss: 0.7352208972587414 accuracy: 45.0 val_loss: 0.7480229907257997 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 72\n",
      "loss: 0.7352212917174352 accuracy: 45.0 val_loss: 0.7480247306365182 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 73\n",
      "loss: 0.7352224473957485 accuracy: 45.0 val_loss: 0.7480278507507053 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 74\n",
      "loss: 0.7352218144004987 accuracy: 45.0 val_loss: 0.7480305165034389 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 75\n",
      "loss: 0.7352213385926832 accuracy: 45.0 val_loss: 0.7480331022313438 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 76\n",
      "loss: 0.7352215946571086 accuracy: 45.0 val_loss: 0.7480350643238369 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 77\n",
      "loss: 0.7352229648860692 accuracy: 45.0 val_loss: 0.7480380660401299 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 78\n",
      "loss: 0.7352223735713442 accuracy: 45.0 val_loss: 0.7480406954530785 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 79\n",
      "loss: 0.7352217293596506 accuracy: 45.0 val_loss: 0.748043549608596 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7352221287015202 accuracy: 45.0 val_loss: 0.7480454496048102 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 81\n",
      "loss: 0.7352232951115486 accuracy: 45.0 val_loss: 0.7480486055088229 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 82\n",
      "loss: 0.7352172350476124 accuracy: 45.0 val_loss: 0.7480951118424886 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 83\n",
      "loss: 0.7351945996414985 accuracy: 45.0 val_loss: 0.7480388090889752 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 84\n",
      "loss: 0.735190245501047 accuracy: 45.0 val_loss: 0.7480506232950452 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 85\n",
      "loss: 0.7351832727295865 accuracy: 45.0 val_loss: 0.7480229281331966 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 86\n",
      "loss: 0.7351755305429262 accuracy: 45.0 val_loss: 0.7480179497960855 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 87\n",
      "loss: 0.7351724743700832 accuracy: 45.0 val_loss: 0.7480140571398024 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 88\n",
      "loss: 0.735171166403353 accuracy: 45.0 val_loss: 0.7480107102069772 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 89\n",
      "loss: 0.735170284657676 accuracy: 45.0 val_loss: 0.7480096057240198 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 90\n",
      "loss: 0.7351709955467534 accuracy: 45.0 val_loss: 0.7480088092746812 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 91\n",
      "loss: 0.7351729770947543 accuracy: 45.0 val_loss: 0.7480100348215266 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 92\n",
      "loss: 0.7351758968448435 accuracy: 45.0 val_loss: 0.7480114332965849 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 93\n",
      "loss: 0.7351758818404268 accuracy: 45.0 val_loss: 0.7480154451337475 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 94\n",
      "loss: 0.7351756414196517 accuracy: 45.0 val_loss: 0.748016144351236 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 95\n",
      "loss: 0.7351760898134159 accuracy: 45.0 val_loss: 0.7480182119349169 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 96\n",
      "loss: 0.7351762565295482 accuracy: 45.0 val_loss: 0.7480201248521989 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 97\n",
      "loss: 0.7351753949754777 accuracy: 45.0 val_loss: 0.7480224170142418 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 98\n",
      "loss: 0.7351752024458997 accuracy: 45.0 val_loss: 0.7480233489171527 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 99\n",
      "loss: 0.7351751691866396 accuracy: 45.0 val_loss: 0.7480244876038097 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 100\n",
      "loss: 0.7351753182020074 accuracy: 45.0 val_loss: 0.7480257200604926 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 101\n",
      "loss: 0.7351756550920712 accuracy: 45.0 val_loss: 0.748026699663918 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 102\n",
      "loss: 0.7351758570693853 accuracy: 45.0 val_loss: 0.7480279370810552 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 103\n",
      "loss: 0.7351761419055698 accuracy: 45.0 val_loss: 0.7480292622034224 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 104\n",
      "loss: 0.7351764450751326 accuracy: 45.0 val_loss: 0.7480305636529953 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 105\n",
      "loss: 0.7351767514267087 accuracy: 45.0 val_loss: 0.7480318663304312 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 106\n",
      "loss: 0.7351770939014128 accuracy: 45.0 val_loss: 0.7480331404492089 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 107\n",
      "loss: 0.7351774004593284 accuracy: 45.0 val_loss: 0.7480342925838428 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 108\n",
      "loss: 0.7351764641458748 accuracy: 45.0 val_loss: 0.7480342776872775 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 109\n",
      "loss: 0.7351750552365753 accuracy: 45.0 val_loss: 0.7480335492720832 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 110\n",
      "loss: 0.7351746891824905 accuracy: 45.0 val_loss: 0.7480332687406148 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 111\n",
      "loss: 0.7351750206878028 accuracy: 45.0 val_loss: 0.7480345523621275 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 112\n",
      "loss: 0.7351755888447393 accuracy: 45.0 val_loss: 0.7480351943239896 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 113\n",
      "loss: 0.7351762564522449 accuracy: 45.0 val_loss: 0.748035965037213 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 114\n",
      "loss: 0.7351766602541765 accuracy: 45.0 val_loss: 0.7480371180476617 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 115\n",
      "loss: 0.7351768657980808 accuracy: 45.0 val_loss: 0.7480383683942865 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 116\n",
      "loss: 0.7351769674963713 accuracy: 45.0 val_loss: 0.7480397730128093 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 117\n",
      "loss: 0.7351770360592954 accuracy: 45.0 val_loss: 0.7480411180519227 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 118\n",
      "loss: 0.7351771335820421 accuracy: 45.0 val_loss: 0.7480424276596106 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 119\n",
      "loss: 0.7351772484365895 accuracy: 45.0 val_loss: 0.748043708438161 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 120\n",
      "loss: 0.7351772970110417 accuracy: 45.0 val_loss: 0.7480448182437808 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 121\n",
      "loss: 0.7351773738220015 accuracy: 45.0 val_loss: 0.7480460865460719 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 122\n",
      "loss: 0.7351775366695256 accuracy: 45.0 val_loss: 0.7480474729621092 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 123\n",
      "loss: 0.7351776685273371 accuracy: 45.0 val_loss: 0.7480488015928268 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 124\n",
      "loss: 0.7351778622418991 accuracy: 45.0 val_loss: 0.7480501057277801 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 125\n",
      "loss: 0.7351780817128856 accuracy: 45.0 val_loss: 0.7480513968246749 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 126\n",
      "loss: 0.7351774347798458 accuracy: 45.0 val_loss: 0.7480549244100694 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 127\n",
      "loss: 0.7351760451175419 accuracy: 45.0 val_loss: 0.7480538475092765 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 128\n",
      "loss: 0.7351763597633697 accuracy: 45.0 val_loss: 0.7480535593077786 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 129\n",
      "loss: 0.7351770830791472 accuracy: 45.0 val_loss: 0.748052741571473 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 130\n",
      "loss: 0.7351782771649991 accuracy: 45.0 val_loss: 0.7480568420582803 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 131\n",
      "loss: 0.735177092281107 accuracy: 45.0 val_loss: 0.7480570775213515 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 132\n",
      "loss: 0.7351767360955324 accuracy: 45.0 val_loss: 0.748056939776369 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 133\n",
      "loss: 0.7351773026574561 accuracy: 45.0 val_loss: 0.748057873992807 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 134\n",
      "loss: 0.7351772182552431 accuracy: 45.0 val_loss: 0.7480581067937374 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 135\n",
      "loss: 0.7351778363763162 accuracy: 45.0 val_loss: 0.7480592803914738 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 136\n",
      "loss: 0.7351776113006192 accuracy: 45.0 val_loss: 0.7480602637060947 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 137\n",
      "loss: 0.7351777432202249 accuracy: 45.0 val_loss: 0.7480607415995465 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 138\n",
      "loss: 0.7351785175371641 accuracy: 45.0 val_loss: 0.7480622031425771 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 139\n",
      "loss: 0.7351783707481379 accuracy: 45.0 val_loss: 0.7480632950507184 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 140\n",
      "loss: 0.7351782005540082 accuracy: 45.0 val_loss: 0.7480644582994589 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 141\n",
      "loss: 0.7351782579740731 accuracy: 45.0 val_loss: 0.7480652143778468 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 142\n",
      "loss: 0.7351793254206085 accuracy: 45.0 val_loss: 0.7480665830321305 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 143\n",
      "loss: 0.7351789423823903 accuracy: 45.0 val_loss: 0.7480679399258735 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 144\n",
      "loss: 0.735178869966967 accuracy: 45.0 val_loss: 0.7480692767192197 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 145\n",
      "loss: 0.7351789180952469 accuracy: 45.0 val_loss: 0.7480701269129475 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 146\n",
      "loss: 0.7351798675888696 accuracy: 45.0 val_loss: 0.7480716711184077 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 147\n",
      "loss: 0.7351794983483815 accuracy: 45.0 val_loss: 0.7480730727933435 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 148\n",
      "loss: 0.7351794379207275 accuracy: 45.0 val_loss: 0.7480741198791363 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 149\n",
      "loss: 0.7351794501624487 accuracy: 45.0 val_loss: 0.7480754098790507 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 150\n",
      "loss: 0.7351799343448843 accuracy: 45.0 val_loss: 0.7480770192226888 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 151\n",
      "loss: 0.73518041159158 accuracy: 45.0 val_loss: 0.7480781601428326 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 152\n",
      "loss: 0.7351802845928428 accuracy: 45.0 val_loss: 0.7480793143665891 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 153\n",
      "loss: 0.7351801152958661 accuracy: 45.0 val_loss: 0.7480804651187974 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 154\n",
      "loss: 0.7351804472141443 accuracy: 45.0 val_loss: 0.7480816445240389 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 155\n",
      "loss: 0.7351805729805105 accuracy: 45.0 val_loss: 0.7480837568124649 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 156\n",
      "loss: 0.7351805505914959 accuracy: 45.0 val_loss: 0.7480842981895115 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 157\n",
      "loss: 0.7351807291286185 accuracy: 45.0 val_loss: 0.74808551228911 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 158\n",
      "loss: 0.7351813311492134 accuracy: 45.0 val_loss: 0.7480873508756323 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 159\n",
      "loss: 0.7351812392686911 accuracy: 45.0 val_loss: 0.7480880514264874 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 160\n",
      "loss: 0.7351818233883333 accuracy: 45.0 val_loss: 0.7480898682401953 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.735181909223916 accuracy: 45.0 val_loss: 0.7480913176551369 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 162\n",
      "loss: 0.7351821566046084 accuracy: 45.0 val_loss: 0.7480925438076601 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 163\n",
      "loss: 0.73518221712963 accuracy: 45.0 val_loss: 0.7480939014909653 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 164\n",
      "loss: 0.7351823494423169 accuracy: 45.0 val_loss: 0.7480947900940083 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 165\n",
      "loss: 0.7351839609157146 accuracy: 45.0 val_loss: 0.7480965864249305 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 166\n",
      "loss: 0.735183817762605 accuracy: 45.0 val_loss: 0.7480978836886077 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 167\n",
      "loss: 0.7351839139750714 accuracy: 45.0 val_loss: 0.7480990219799198 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 168\n",
      "loss: 0.7351838729748945 accuracy: 45.0 val_loss: 0.7481003340452428 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 169\n",
      "loss: 0.7351840830581348 accuracy: 45.0 val_loss: 0.7481014836864737 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 170\n",
      "loss: 0.735184131709893 accuracy: 45.0 val_loss: 0.748102804960828 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 171\n",
      "loss: 0.7351844142060233 accuracy: 45.0 val_loss: 0.7481039614620768 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 172\n",
      "loss: 0.7351848064462096 accuracy: 45.0 val_loss: 0.7481048089205578 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 173\n",
      "loss: 0.7351863988969917 accuracy: 45.0 val_loss: 0.7481067988569209 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 174\n",
      "loss: 0.7351864327000315 accuracy: 45.0 val_loss: 0.7481079612620242 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 175\n",
      "loss: 0.7351863143880105 accuracy: 45.0 val_loss: 0.7481092940762426 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 176\n",
      "loss: 0.7351864748753971 accuracy: 45.0 val_loss: 0.7481104569847722 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 177\n",
      "loss: 0.7351866859199807 accuracy: 45.0 val_loss: 0.7481116204818024 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 178\n",
      "loss: 0.7351867211391848 accuracy: 45.0 val_loss: 0.7481129681835368 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 179\n",
      "loss: 0.7351870126412543 accuracy: 45.0 val_loss: 0.7481141541855493 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 180\n",
      "loss: 0.7351872601888617 accuracy: 45.0 val_loss: 0.748115224435955 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 181\n",
      "loss: 0.7351889438841852 accuracy: 45.0 val_loss: 0.7481169114736401 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 182\n",
      "loss: 0.7351887444058737 accuracy: 45.0 val_loss: 0.7481179581324645 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 183\n",
      "loss: 0.7351886050259459 accuracy: 45.0 val_loss: 0.7481193779501005 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 184\n",
      "loss: 0.7351888051929065 accuracy: 45.0 val_loss: 0.7481197359817251 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 185\n",
      "loss: 0.7351885329524336 accuracy: 45.0 val_loss: 0.7481203273207485 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 186\n",
      "loss: 0.7351884268265886 accuracy: 45.0 val_loss: 0.7481210112447123 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 187\n",
      "loss: 0.7351899940225768 accuracy: 45.0 val_loss: 0.7481227371068893 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 188\n",
      "loss: 0.7351895922159761 accuracy: 45.0 val_loss: 0.7481237866150063 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 189\n",
      "loss: 0.7351891198088868 accuracy: 45.0 val_loss: 0.7481250309477567 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 190\n",
      "loss: 0.7351892487480111 accuracy: 45.0 val_loss: 0.7481271353202851 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 191\n",
      "loss: 0.7351909853019692 accuracy: 45.0 val_loss: 0.7481287197678785 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 192\n",
      "loss: 0.7351925634079421 accuracy: 45.0 val_loss: 0.7481310130521528 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 193\n",
      "loss: 0.7351924446855441 accuracy: 45.0 val_loss: 0.7481325945654491 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 194\n",
      "loss: 0.735192476311819 accuracy: 45.0 val_loss: 0.7481329334590171 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 195\n",
      "loss: 0.7351939678271059 accuracy: 45.0 val_loss: 0.7481354033864637 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 196\n",
      "loss: 0.7351934956154073 accuracy: 45.0 val_loss: 0.7481356588025921 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 197\n",
      "loss: 0.7351926247677428 accuracy: 45.0 val_loss: 0.7481359820704523 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 198\n",
      "loss: 0.7351923260706628 accuracy: 45.0 val_loss: 0.7481369279492438 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 199\n",
      "loss: 0.7351939483857414 accuracy: 45.0 val_loss: 0.7481387073407423 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 200\n",
      "loss: 0.7351935923665376 accuracy: 45.0 val_loss: 0.7481398895669455 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 201\n",
      "loss: 0.7351934511574552 accuracy: 45.0 val_loss: 0.7481459144183982 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 202\n",
      "loss: 0.7351914535933841 accuracy: 45.0 val_loss: 0.7481412577024695 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 203\n",
      "loss: 0.7351938690211784 accuracy: 45.0 val_loss: 0.7481484174852646 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 204\n",
      "loss: 0.735192459583963 accuracy: 45.0 val_loss: 0.7481434974969656 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 205\n",
      "loss: 0.7351950050959938 accuracy: 45.0 val_loss: 0.74814589730026 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 206\n",
      "loss: 0.7351949303485036 accuracy: 45.0 val_loss: 0.7481525921390126 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 207\n",
      "loss: 0.7351934037502031 accuracy: 45.0 val_loss: 0.7481475016162733 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 208\n",
      "loss: 0.7351961143781197 accuracy: 45.0 val_loss: 0.7481551283349404 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 209\n",
      "loss: 0.735194501946889 accuracy: 45.0 val_loss: 0.7481499646600586 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 210\n",
      "loss: 0.7351972337744043 accuracy: 45.0 val_loss: 0.7481530512863154 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 211\n",
      "loss: 0.7351972981285873 accuracy: 45.0 val_loss: 0.7481607964564273 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 212\n",
      "loss: 0.7351954259155362 accuracy: 45.0 val_loss: 0.7481546298612645 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 213\n",
      "loss: 0.7351979953216136 accuracy: 45.0 val_loss: 0.7481631829015333 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 214\n",
      "loss: 0.7351963041212666 accuracy: 45.0 val_loss: 0.7481625294557509 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 215\n",
      "loss: 0.7351964300862126 accuracy: 45.0 val_loss: 0.7481579568551482 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 216\n",
      "loss: 0.7351992526301087 accuracy: 45.0 val_loss: 0.7481668837796148 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 217\n",
      "loss: 0.7351974664696604 accuracy: 45.0 val_loss: 0.7481666337179338 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 218\n",
      "loss: 0.7352177094001191 accuracy: 45.0 val_loss: 0.7483443576387907 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 219\n",
      "loss: 0.7352840559241891 accuracy: 45.0 val_loss: 0.7481042437262321 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 220\n",
      "loss: 0.7352691472922199 accuracy: 45.0 val_loss: 0.7481049404526303 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 221\n",
      "loss: 0.7352695029407736 accuracy: 45.0 val_loss: 0.7481009089500682 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 222\n",
      "loss: 0.7352721611050576 accuracy: 45.0 val_loss: 0.748110356220833 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 223\n",
      "loss: 0.7352703148141615 accuracy: 45.0 val_loss: 0.7481101086672858 val_accuracy: 40.0 \n",
      "\n",
      " ==> Epoch 224\n",
      "accuracy: 0.4576271186440678 loss: 0.7293499718599543 iter:59  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(FC_Layer(innode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, outnode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Softmax_Layer())\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:83\u001b[0m, in \u001b[0;36mNetwork_Model.train\u001b[1;34m(self, X_train, Y_train, X_test, Y_test, epochs, lr, verbose)\u001b[0m\n\u001b[0;32m     81\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(tp\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(Y_train) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     82\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_train))\n\u001b[1;32m---> 83\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[0;32m     85\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_accuracy)\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:104\u001b[0m, in \u001b[0;36mNetwork_Model.eval\u001b[1;34m(self, X_test, Y_test)\u001b[0m\n\u001b[0;32m    102\u001b[0m loss, tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Y_test)):\n\u001b[1;32m--> 104\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(Y_test[i], output)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(output) \u001b[38;5;241m==\u001b[39m Y_test[i]:\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:16\u001b[0m, in \u001b[0;36mNetwork_Model.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 16\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\Documents\\Education\\Boun\\CMPE597\\HW1\\CNN_Class.py:251\u001b[0m, in \u001b[0;36mFC_Layer.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_shape \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    250\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m--> 251\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    254\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(ReLU)(out)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Network_Model(cross_entropy, default_optimizer)\n",
    "model.add(FC_Layer(innode=2, outnode=8, activation=\"relu\"))\n",
    "model.add(FC_Layer(innode=8, outnode=8, activation=\"relu\"))\n",
    "model.add(FC_Layer(innode=8, outnode=2, activation=None))\n",
    "model.add(Softmax_Layer())\n",
    "model.train(X_train, Y_train, X_test, Y_test, epochs=1000, lr=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465c3d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.982619384793512\n",
      "[-396.47753642    0.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_label = model.predict(X_test)\n",
    "output = model.forward(X_train[0])\n",
    "loss = model.loss(Y_train[0], output)\n",
    "grad = model.dloss(Y_train[0], output)\n",
    "print(loss)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d902ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=[(x,y) for x, y in zip(X_train, Y_train)],\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=[(x,y) for x, y in zip(X_test, Y_test)],\n",
    "                batch_size=len(Y_test),\n",
    "                shuffle=False)\n",
    "grid_loader = torch.utils.data.DataLoader(\n",
    "                dataset = [(x,0) for x in X_grid],\n",
    "                batch_size = len(X_grid)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7b0810d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchNetworkQ2(\n",
      "  (fc1): Linear(in_features=2, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PytorchNetworkQ2(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(PytorchNetworkQ2, self).__init__()\n",
    "    \n",
    "      self.fc1 = nn.Linear(2, 8)\n",
    "      self.fc2 = nn.Linear(8, 8)\n",
    "      self.fc3 = nn.Linear(8, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc2(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc3(x)\n",
    "        \n",
    "      return torch.sigmoid(x)\n",
    "\n",
    "model2 = PytorchNetworkQ2()\n",
    "print(model2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "585102f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 60, train loss: 0.696859\n",
      "==>>> epoch: 0, batch index: 1, test loss: 0.723227, test acc: 0.400\n",
      "==>>> epoch: 1, batch index: 60, train loss: 0.695231\n",
      "==>>> epoch: 1, batch index: 1, test loss: 0.726950, test acc: 0.400\n",
      "==>>> epoch: 2, batch index: 60, train loss: 0.693798\n",
      "==>>> epoch: 2, batch index: 1, test loss: 0.720197, test acc: 0.400\n",
      "==>>> epoch: 3, batch index: 60, train loss: 0.692038\n",
      "==>>> epoch: 3, batch index: 1, test loss: 0.726168, test acc: 0.400\n",
      "==>>> epoch: 4, batch index: 60, train loss: 0.689298\n",
      "==>>> epoch: 4, batch index: 1, test loss: 0.712309, test acc: 0.400\n",
      "==>>> epoch: 5, batch index: 60, train loss: 0.691294\n",
      "==>>> epoch: 5, batch index: 1, test loss: 0.719322, test acc: 0.400\n",
      "==>>> epoch: 6, batch index: 60, train loss: 0.690069\n",
      "==>>> epoch: 6, batch index: 1, test loss: 0.724271, test acc: 0.400\n",
      "==>>> epoch: 7, batch index: 60, train loss: 0.689386\n",
      "==>>> epoch: 7, batch index: 1, test loss: 0.727235, test acc: 0.400\n",
      "==>>> epoch: 8, batch index: 60, train loss: 0.688661\n",
      "==>>> epoch: 8, batch index: 1, test loss: 0.730838, test acc: 0.400\n",
      "==>>> epoch: 9, batch index: 60, train loss: 0.685515\n",
      "==>>> epoch: 9, batch index: 1, test loss: 0.742746, test acc: 0.400\n",
      "==>>> epoch: 10, batch index: 60, train loss: 0.684630\n",
      "==>>> epoch: 10, batch index: 1, test loss: 0.752067, test acc: 0.400\n",
      "==>>> epoch: 11, batch index: 60, train loss: 0.686993\n",
      "==>>> epoch: 11, batch index: 1, test loss: 0.726552, test acc: 0.400\n",
      "==>>> epoch: 12, batch index: 60, train loss: 0.684393\n",
      "==>>> epoch: 12, batch index: 1, test loss: 0.714352, test acc: 0.400\n",
      "==>>> epoch: 13, batch index: 60, train loss: 0.683782\n",
      "==>>> epoch: 13, batch index: 1, test loss: 0.713581, test acc: 0.400\n",
      "==>>> epoch: 14, batch index: 60, train loss: 0.680627\n",
      "==>>> epoch: 14, batch index: 1, test loss: 0.727179, test acc: 0.400\n",
      "==>>> epoch: 15, batch index: 60, train loss: 0.678469\n",
      "==>>> epoch: 15, batch index: 1, test loss: 0.722490, test acc: 0.400\n",
      "==>>> epoch: 16, batch index: 60, train loss: 0.673205\n",
      "==>>> epoch: 16, batch index: 1, test loss: 0.729613, test acc: 0.400\n",
      "==>>> epoch: 17, batch index: 60, train loss: 0.668027\n",
      "==>>> epoch: 17, batch index: 1, test loss: 0.742624, test acc: 0.400\n",
      "==>>> epoch: 18, batch index: 60, train loss: 0.665363\n",
      "==>>> epoch: 18, batch index: 1, test loss: 0.679154, test acc: 0.400\n",
      "==>>> epoch: 19, batch index: 60, train loss: 0.653941\n",
      "==>>> epoch: 19, batch index: 1, test loss: 0.688002, test acc: 0.400\n",
      "==>>> epoch: 20, batch index: 60, train loss: 0.649235\n",
      "==>>> epoch: 20, batch index: 1, test loss: 0.658103, test acc: 0.500\n",
      "==>>> epoch: 21, batch index: 60, train loss: 0.638369\n",
      "==>>> epoch: 21, batch index: 1, test loss: 0.638816, test acc: 0.575\n",
      "==>>> epoch: 22, batch index: 60, train loss: 0.625449\n",
      "==>>> epoch: 22, batch index: 1, test loss: 0.635425, test acc: 0.625\n",
      "==>>> epoch: 23, batch index: 60, train loss: 0.609814\n",
      "==>>> epoch: 23, batch index: 1, test loss: 0.663076, test acc: 0.450\n",
      "==>>> epoch: 24, batch index: 60, train loss: 0.622833\n",
      "==>>> epoch: 24, batch index: 1, test loss: 0.580396, test acc: 0.875\n",
      "==>>> epoch: 25, batch index: 60, train loss: 0.575390\n",
      "==>>> epoch: 25, batch index: 1, test loss: 0.747479, test acc: 0.400\n",
      "==>>> epoch: 26, batch index: 60, train loss: 0.570124\n",
      "==>>> epoch: 26, batch index: 1, test loss: 0.617988, test acc: 0.600\n",
      "==>>> epoch: 27, batch index: 60, train loss: 0.591682\n",
      "==>>> epoch: 27, batch index: 1, test loss: 0.686897, test acc: 0.575\n",
      "==>>> epoch: 28, batch index: 60, train loss: 0.568858\n",
      "==>>> epoch: 28, batch index: 1, test loss: 0.511952, test acc: 0.800\n",
      "==>>> epoch: 29, batch index: 60, train loss: 0.549017\n",
      "==>>> epoch: 29, batch index: 1, test loss: 0.513830, test acc: 0.850\n",
      "==>>> epoch: 30, batch index: 60, train loss: 0.547482\n",
      "==>>> epoch: 30, batch index: 1, test loss: 0.531350, test acc: 0.850\n",
      "==>>> epoch: 31, batch index: 60, train loss: 0.566682\n",
      "==>>> epoch: 31, batch index: 1, test loss: 0.632712, test acc: 0.650\n",
      "==>>> epoch: 32, batch index: 60, train loss: 0.543468\n",
      "==>>> epoch: 32, batch index: 1, test loss: 0.690305, test acc: 0.525\n",
      "==>>> epoch: 33, batch index: 60, train loss: 0.571199\n",
      "==>>> epoch: 33, batch index: 1, test loss: 0.677979, test acc: 0.600\n",
      "==>>> epoch: 34, batch index: 60, train loss: 0.552574\n",
      "==>>> epoch: 34, batch index: 1, test loss: 0.522619, test acc: 0.750\n",
      "==>>> epoch: 35, batch index: 60, train loss: 0.480193\n",
      "==>>> epoch: 35, batch index: 1, test loss: 0.626279, test acc: 0.700\n",
      "==>>> epoch: 36, batch index: 60, train loss: 0.529922\n",
      "==>>> epoch: 36, batch index: 1, test loss: 0.480910, test acc: 0.850\n",
      "==>>> epoch: 37, batch index: 60, train loss: 0.516177\n",
      "==>>> epoch: 37, batch index: 1, test loss: 0.523330, test acc: 0.775\n",
      "==>>> epoch: 38, batch index: 60, train loss: 0.493538\n",
      "==>>> epoch: 38, batch index: 1, test loss: 0.670656, test acc: 0.625\n",
      "==>>> epoch: 39, batch index: 60, train loss: 0.542435\n",
      "==>>> epoch: 39, batch index: 1, test loss: 0.541379, test acc: 0.750\n",
      "==>>> epoch: 40, batch index: 60, train loss: 0.487822\n",
      "==>>> epoch: 40, batch index: 1, test loss: 0.447395, test acc: 0.900\n",
      "==>>> epoch: 41, batch index: 60, train loss: 0.535881\n",
      "==>>> epoch: 41, batch index: 1, test loss: 0.457329, test acc: 0.850\n",
      "==>>> epoch: 42, batch index: 60, train loss: 0.481244\n",
      "==>>> epoch: 42, batch index: 1, test loss: 0.562332, test acc: 0.750\n",
      "==>>> epoch: 43, batch index: 60, train loss: 0.497511\n",
      "==>>> epoch: 43, batch index: 1, test loss: 0.490843, test acc: 0.825\n",
      "==>>> epoch: 44, batch index: 60, train loss: 0.507258\n",
      "==>>> epoch: 44, batch index: 1, test loss: 0.462456, test acc: 0.825\n",
      "==>>> epoch: 45, batch index: 60, train loss: 0.491294\n",
      "==>>> epoch: 45, batch index: 1, test loss: 0.509566, test acc: 0.750\n",
      "==>>> epoch: 46, batch index: 60, train loss: 0.542015\n",
      "==>>> epoch: 46, batch index: 1, test loss: 0.445765, test acc: 0.875\n",
      "==>>> epoch: 47, batch index: 60, train loss: 0.476553\n",
      "==>>> epoch: 47, batch index: 1, test loss: 0.617387, test acc: 0.650\n",
      "==>>> epoch: 48, batch index: 60, train loss: 0.509679\n",
      "==>>> epoch: 48, batch index: 1, test loss: 0.685792, test acc: 0.575\n",
      "==>>> epoch: 49, batch index: 60, train loss: 0.516379\n",
      "==>>> epoch: 49, batch index: 1, test loss: 0.611849, test acc: 0.675\n",
      "==>>> epoch: 50, batch index: 60, train loss: 0.471857\n",
      "==>>> epoch: 50, batch index: 1, test loss: 0.418635, test acc: 0.925\n",
      "==>>> epoch: 51, batch index: 60, train loss: 0.504526\n",
      "==>>> epoch: 51, batch index: 1, test loss: 0.563179, test acc: 0.725\n",
      "==>>> epoch: 52, batch index: 60, train loss: 0.480973\n",
      "==>>> epoch: 52, batch index: 1, test loss: 0.429462, test acc: 0.875\n",
      "==>>> epoch: 53, batch index: 60, train loss: 0.465380\n",
      "==>>> epoch: 53, batch index: 1, test loss: 0.498900, test acc: 0.800\n",
      "==>>> epoch: 54, batch index: 60, train loss: 0.501994\n",
      "==>>> epoch: 54, batch index: 1, test loss: 0.509700, test acc: 0.775\n",
      "==>>> epoch: 55, batch index: 60, train loss: 0.509436\n",
      "==>>> epoch: 55, batch index: 1, test loss: 0.676502, test acc: 0.625\n",
      "==>>> epoch: 56, batch index: 60, train loss: 0.520159\n",
      "==>>> epoch: 56, batch index: 1, test loss: 0.557199, test acc: 0.750\n",
      "==>>> epoch: 57, batch index: 60, train loss: 0.473055\n",
      "==>>> epoch: 57, batch index: 1, test loss: 0.444276, test acc: 0.875\n",
      "==>>> epoch: 58, batch index: 60, train loss: 0.494497\n",
      "==>>> epoch: 58, batch index: 1, test loss: 0.522747, test acc: 0.750\n",
      "==>>> epoch: 59, batch index: 60, train loss: 0.482271\n",
      "==>>> epoch: 59, batch index: 1, test loss: 0.434583, test acc: 0.900\n",
      "==>>> epoch: 60, batch index: 60, train loss: 0.459535\n",
      "==>>> epoch: 60, batch index: 1, test loss: 0.444088, test acc: 0.900\n",
      "==>>> epoch: 61, batch index: 60, train loss: 0.510383\n",
      "==>>> epoch: 61, batch index: 1, test loss: 0.608067, test acc: 0.675\n",
      "==>>> epoch: 62, batch index: 60, train loss: 0.456809\n",
      "==>>> epoch: 62, batch index: 1, test loss: 0.687097, test acc: 0.600\n",
      "==>>> epoch: 63, batch index: 60, train loss: 0.543140\n",
      "==>>> epoch: 63, batch index: 1, test loss: 0.465883, test acc: 0.850\n",
      "==>>> epoch: 64, batch index: 60, train loss: 0.466612\n",
      "==>>> epoch: 64, batch index: 1, test loss: 0.576630, test acc: 0.750\n",
      "==>>> epoch: 65, batch index: 60, train loss: 0.500149\n",
      "==>>> epoch: 65, batch index: 1, test loss: 0.439758, test acc: 0.900\n",
      "==>>> epoch: 66, batch index: 60, train loss: 0.444271\n",
      "==>>> epoch: 66, batch index: 1, test loss: 0.410839, test acc: 0.900\n",
      "==>>> epoch: 67, batch index: 60, train loss: 0.538967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 67, batch index: 1, test loss: 0.466714, test acc: 0.850\n",
      "==>>> epoch: 68, batch index: 60, train loss: 0.476647\n",
      "==>>> epoch: 68, batch index: 1, test loss: 0.505585, test acc: 0.825\n",
      "==>>> epoch: 69, batch index: 60, train loss: 0.477298\n",
      "==>>> epoch: 69, batch index: 1, test loss: 0.482477, test acc: 0.800\n",
      "==>>> epoch: 70, batch index: 60, train loss: 0.473109\n",
      "==>>> epoch: 70, batch index: 1, test loss: 0.445618, test acc: 0.875\n",
      "==>>> epoch: 71, batch index: 60, train loss: 0.466126\n",
      "==>>> epoch: 71, batch index: 1, test loss: 0.482620, test acc: 0.850\n",
      "==>>> epoch: 72, batch index: 60, train loss: 0.449260\n",
      "==>>> epoch: 72, batch index: 1, test loss: 0.435447, test acc: 0.900\n",
      "==>>> epoch: 73, batch index: 60, train loss: 0.446063\n",
      "==>>> epoch: 73, batch index: 1, test loss: 0.492336, test acc: 0.750\n",
      "==>>> epoch: 74, batch index: 60, train loss: 0.476271\n",
      "==>>> epoch: 74, batch index: 1, test loss: 0.441056, test acc: 0.875\n",
      "==>>> epoch: 75, batch index: 60, train loss: 0.491479\n",
      "==>>> epoch: 75, batch index: 1, test loss: 0.445987, test acc: 0.850\n",
      "==>>> epoch: 76, batch index: 60, train loss: 0.463896\n",
      "==>>> epoch: 76, batch index: 1, test loss: 0.450361, test acc: 0.850\n",
      "==>>> epoch: 77, batch index: 60, train loss: 0.493558\n",
      "==>>> epoch: 77, batch index: 1, test loss: 0.486515, test acc: 0.800\n",
      "==>>> epoch: 78, batch index: 60, train loss: 0.464684\n",
      "==>>> epoch: 78, batch index: 1, test loss: 0.480413, test acc: 0.825\n",
      "==>>> epoch: 79, batch index: 60, train loss: 0.436590\n",
      "==>>> epoch: 79, batch index: 1, test loss: 0.476490, test acc: 0.850\n",
      "==>>> epoch: 80, batch index: 60, train loss: 0.467167\n",
      "==>>> epoch: 80, batch index: 1, test loss: 0.464103, test acc: 0.850\n",
      "==>>> epoch: 81, batch index: 60, train loss: 0.462596\n",
      "==>>> epoch: 81, batch index: 1, test loss: 0.716680, test acc: 0.575\n",
      "==>>> epoch: 82, batch index: 60, train loss: 0.523778\n",
      "==>>> epoch: 82, batch index: 1, test loss: 0.613138, test acc: 0.675\n",
      "==>>> epoch: 83, batch index: 60, train loss: 0.505351\n",
      "==>>> epoch: 83, batch index: 1, test loss: 0.471655, test acc: 0.825\n",
      "==>>> epoch: 84, batch index: 60, train loss: 0.439263\n",
      "==>>> epoch: 84, batch index: 1, test loss: 0.434135, test acc: 0.900\n",
      "==>>> epoch: 85, batch index: 60, train loss: 0.487767\n",
      "==>>> epoch: 85, batch index: 1, test loss: 0.412383, test acc: 0.925\n",
      "==>>> epoch: 86, batch index: 60, train loss: 0.468539\n",
      "==>>> epoch: 86, batch index: 1, test loss: 0.564562, test acc: 0.750\n",
      "==>>> epoch: 87, batch index: 60, train loss: 0.459447\n",
      "==>>> epoch: 87, batch index: 1, test loss: 0.478622, test acc: 0.850\n",
      "==>>> epoch: 88, batch index: 60, train loss: 0.446978\n",
      "==>>> epoch: 88, batch index: 1, test loss: 0.476721, test acc: 0.850\n",
      "==>>> epoch: 89, batch index: 60, train loss: 0.466712\n",
      "==>>> epoch: 89, batch index: 1, test loss: 0.442336, test acc: 0.850\n",
      "==>>> epoch: 90, batch index: 60, train loss: 0.441281\n",
      "==>>> epoch: 90, batch index: 1, test loss: 0.412703, test acc: 0.900\n",
      "==>>> epoch: 91, batch index: 60, train loss: 0.481213\n",
      "==>>> epoch: 91, batch index: 1, test loss: 0.474561, test acc: 0.850\n",
      "==>>> epoch: 92, batch index: 60, train loss: 0.454234\n",
      "==>>> epoch: 92, batch index: 1, test loss: 0.431944, test acc: 0.875\n",
      "==>>> epoch: 93, batch index: 60, train loss: 0.440353\n",
      "==>>> epoch: 93, batch index: 1, test loss: 0.512439, test acc: 0.800\n",
      "==>>> epoch: 94, batch index: 60, train loss: 0.450434\n",
      "==>>> epoch: 94, batch index: 1, test loss: 0.526923, test acc: 0.800\n",
      "==>>> epoch: 95, batch index: 60, train loss: 0.483899\n",
      "==>>> epoch: 95, batch index: 1, test loss: 0.411161, test acc: 0.925\n",
      "==>>> epoch: 96, batch index: 60, train loss: 0.434856\n",
      "==>>> epoch: 96, batch index: 1, test loss: 0.573589, test acc: 0.750\n",
      "==>>> epoch: 97, batch index: 60, train loss: 0.485420\n",
      "==>>> epoch: 97, batch index: 1, test loss: 0.576421, test acc: 0.750\n",
      "==>>> epoch: 98, batch index: 60, train loss: 0.444856\n",
      "==>>> epoch: 98, batch index: 1, test loss: 0.459600, test acc: 0.850\n",
      "==>>> epoch: 99, batch index: 60, train loss: 0.425176\n",
      "==>>> epoch: 99, batch index: 1, test loss: 0.393250, test acc: 0.925\n",
      "==>>> epoch: 100, batch index: 60, train loss: 0.453679\n",
      "==>>> epoch: 100, batch index: 1, test loss: 0.491875, test acc: 0.775\n",
      "==>>> epoch: 101, batch index: 60, train loss: 0.430049\n",
      "==>>> epoch: 101, batch index: 1, test loss: 0.430262, test acc: 0.900\n",
      "==>>> epoch: 102, batch index: 60, train loss: 0.439815\n",
      "==>>> epoch: 102, batch index: 1, test loss: 0.517899, test acc: 0.800\n",
      "==>>> epoch: 103, batch index: 60, train loss: 0.437446\n",
      "==>>> epoch: 103, batch index: 1, test loss: 0.488232, test acc: 0.825\n",
      "==>>> epoch: 104, batch index: 60, train loss: 0.451667\n",
      "==>>> epoch: 104, batch index: 1, test loss: 0.414748, test acc: 0.900\n",
      "==>>> epoch: 105, batch index: 60, train loss: 0.461446\n",
      "==>>> epoch: 105, batch index: 1, test loss: 0.470396, test acc: 0.850\n",
      "==>>> epoch: 106, batch index: 60, train loss: 0.423343\n",
      "==>>> epoch: 106, batch index: 1, test loss: 0.440427, test acc: 0.900\n",
      "==>>> epoch: 107, batch index: 60, train loss: 0.436558\n",
      "==>>> epoch: 107, batch index: 1, test loss: 0.392104, test acc: 0.950\n",
      "==>>> epoch: 108, batch index: 60, train loss: 0.477342\n",
      "==>>> epoch: 108, batch index: 1, test loss: 0.426587, test acc: 0.875\n",
      "==>>> epoch: 109, batch index: 60, train loss: 0.450579\n",
      "==>>> epoch: 109, batch index: 1, test loss: 0.489502, test acc: 0.850\n",
      "==>>> epoch: 110, batch index: 60, train loss: 0.511227\n",
      "==>>> epoch: 110, batch index: 1, test loss: 0.490171, test acc: 0.825\n",
      "==>>> epoch: 111, batch index: 60, train loss: 0.451259\n",
      "==>>> epoch: 111, batch index: 1, test loss: 0.405866, test acc: 0.900\n",
      "==>>> epoch: 112, batch index: 60, train loss: 0.501909\n",
      "==>>> epoch: 112, batch index: 1, test loss: 0.502374, test acc: 0.800\n",
      "==>>> epoch: 113, batch index: 60, train loss: 0.455637\n",
      "==>>> epoch: 113, batch index: 1, test loss: 0.572053, test acc: 0.725\n",
      "==>>> epoch: 114, batch index: 60, train loss: 0.491562\n",
      "==>>> epoch: 114, batch index: 1, test loss: 0.451314, test acc: 0.850\n",
      "==>>> epoch: 115, batch index: 60, train loss: 0.421925\n",
      "==>>> epoch: 115, batch index: 1, test loss: 0.439568, test acc: 0.900\n",
      "==>>> epoch: 116, batch index: 60, train loss: 0.461117\n",
      "==>>> epoch: 116, batch index: 1, test loss: 0.456882, test acc: 0.875\n",
      "==>>> epoch: 117, batch index: 60, train loss: 0.442019\n",
      "==>>> epoch: 117, batch index: 1, test loss: 0.513080, test acc: 0.800\n",
      "==>>> epoch: 118, batch index: 60, train loss: 0.487081\n",
      "==>>> epoch: 118, batch index: 1, test loss: 0.465599, test acc: 0.825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, target)\n\u001b[0;32m      9\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\CMPE597\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    # trainning\n",
    "    total_loss = 0\n",
    "    model2.train()\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model2(x)\n",
    "        loss = criterion(out, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, total_loss/(batch_idx+1) / len(x)))\n",
    "    # testing\n",
    "    correct_cnt, total_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model2(x)\n",
    "        \n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        total_loss += loss.item()\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, test acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, total_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da140c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x230b15b9160>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pred_label = model.predict(X_grid)\n",
    "for i, (x, k) in enumerate(grid_loader):\n",
    "    pred_label_grid = model2(x)\n",
    "    _, pred_label_grid = torch.max(pred_label_grid.data, 1)\n",
    "boolarray = np.array([x == y for x,y in zip(Y_test, pred_label)])\n",
    "plt.plot(X_test[boolarray, 0], X_test[boolarray, 1], \"*g\")\n",
    "plt.plot(X_test[boolarray == 0, 0], X_test[boolarray == 0, 1], \"*r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e80b582d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x230b16eea00>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[pred_label == 1, 0], X_test[pred_label == 1, 1], \"*r\")\n",
    "plt.plot(X_test[pred_label == 0, 0], X_test[pred_label == 0, 1], \"*b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5737481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x230b164d820>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAd6ElEQVR4nO3df6xc5X3n8fd3rtdcbwJNCKx9i6G2G0uERlVaX7GptA1/QCQiEdiq2YQQNqDNyksdpErZKkJCTVZYK5FGbbNSoMVK/6DtaoGiXZVdNsomNCiKtCAuKrsKIQT30hQ7NnEjNkob88P2d/+4c+3nXs+ZOXPmOed8z5zPSxr5zJ3jmc85F8793MfPPGPujoiIzL9B2wFERKQZuuCLiPSELvgiIj2hC76ISE/ogi8i0hNb2g5Q5JJLLvFdu3a1HUNEpFOeffbZv3f3S0c9FvaCv2vXLlZWVtqOISLSKWb2g6LHNKQjItITuuCLiPSELvgiIj2hC76ISE/ogi8i0hNzecE/dgyuuQaOH4+xHTFTl/J1Kavy9SdrE/lyCzstcxYHD8K3vw333LN2v+3t+++Pl6lL+bqUVfn6k7WJfPffT1YWdXnk5eVln3Ye/rZt8PrrNQUSEWnB4iKcPFl+fzN71t2XRz02V0M6q6twyy2wZfh7i9narc3thQXYtWvtzyiZupSvS1mVrz9Zm8i3ZQt84hPw8stko4YvIhKYGn6B1VXYufPcT8goBsHPcvR8qehZlS+f6Fnrzme2dj3L2fDn6h9t9+yJ2fDPnGk7wXjR86WiZ1W+fKJnrTufOxw5Art3T9fwxwn+M3Q6avjVRM+Xip5V+fKJnlUNv2Vq+NVEz5eKnlX58omeVQ2/ZWr41UTPl4qeVfnyiZ5VDb9lavjVRM+Xip5V+fKJnlUNv2Vq+NVEz5eKnlX58omeVQ2/ZWr41UTPl4qeVfnyiZ5VDb9lavjVRM+Xip5V+fKJnlUNv2Vq+NVEz5eKnlX58omeVQ2/ZWr41UTPl4qeVfnyiZ5VDb9lavjVRM+Xip5V+fKJnlUNv2Vq+NVEz5eKnlX58ometYsNP0tkM7vezF40s8NmdteY/X7TzNzMRq7kNqs9e9Z+IkZbALTvTSWn6FmVL5/oWZts+LnMfME3swXgPuBDwFXAx83sqhH7XQj8NvD0rK9ZRA2/muj5UtGzKl8+0bP2teFfDRx291V3fxN4CLhpxH4HgS8AtY2yq+FXEz1fKnpW5csnetZeNnzgMuCV5P6R4dfOMrNfBS5398fHPZGZ7TezFTNbOXHixNRB1PCriZ4vFT2r8uUTPWtfG/5YZjYA/gD495P2dfdD7r7s7suXXnrp1K+lhl9N9Hyp6FmVL5/oWfva8I8Clyf3dw6/tu5C4L3Ak2b2t8D7gcfq+IdbNfxqoudLRc+qfPlEz9rFhp9jHv4zwF4z283ahf5m4Jb1B939J8Al6/fN7Engd9x9ug+sLUHz8KuJni8VPavy5RM9ay/n4bv7KeBO4GvAC8Aj7v68md1jZjfO+vzTUMOvJnq+VPSsypdP9KxdbPjm0Qa8h5aXl31lZbpfArZti9nwRUSqWlycruGb2bPuPnLIPPjP0Omo4VcTPV8qelblyyd61i42fK2l04C+j0XmFD2r8uUTPWsvx/AjUcOvJnq+VPSsypdP9Kxq+C1Tw68mer5U9KzKl0/0rGr4LVPDryZ6vlT0rMqXT/SsavgtU8OvJnq+VPSsypdP9Kxq+C1Tw68mer5U9Kyz5/PkzzLb04l+/lLRs3ax4Qc/pdPRWjrVRM+Xip519nxGelFf5OTI7bU/p2820c9fKnrWvq6lE4YafjXR86WiZy2fb1RjP8Pb+Cnf4Fp28zK7WeUp3j9y++tcx9v4KXBmxPOM3l7gFIMBLHC69N/Juz2d+fleV6Mx/Ak0hl9N9Hyp6FnL50ubvLHISd5kK5/kz7iWb7LKu8/uWbT9r/lzDrGfrZzkdS44+zxF22+ylSvPfIfv8Z6x++XentffRjSG3zI1/Gqi50tFz1quQY9u8nfwxxxne+nXepV/xh38ceFvAZu37+ABXhtcwh08UPrv5Nie/NtI8bmMrIsNX2vpiGTnLPJ60nY3br/JVv4dD3A/n247aGN+i/uHv428mTR/qNL8+0Zr6RRQw68mer5UnKyjx8gXB29wGw9OaNrTNfmsWjqBo34bOdf8R5fOON/r0dTwM1LDl/jSMfh+NvdZbONnvM62tmOEp4ZfQA2/muj5Us1knTTjpGgM/gGOD36+iYDVBfpmr7KHnfwdC7w1/MrG8hko6khdbPiapdOAvs82yKmZrOfPoNk822XUbJr7+PS5f5eMKtA3e4nj3MDjHGI/o2byBIo6kmbptEwNv5ro+VJ5s1abCz92DD76yQyWb31sf9R4frCo5+liw9cYvvScxuEj0Hh+MY3hF1DDryZ6vlS1rPXNhc8UsDlB862P51syJhY06lldbPgaw29A38cic6qWtfy7Wu/jzjYCNidovj2sntfwg0Y9S2P4LVPDryZ6vtT5WavOqKlpLnz0kxk03+YZO+vr/kSmht8yNfxqoudLnZ+16oyaGZt8+YCxBM2XzthZ+x4uhp/xpIbfMjX8aqLnK17tccYZNXWIfjID50vfjXsbf8ri4A2swiqbTeliw9csHekIZ8AZruQFvsd72MqbmlEzxzRr5xzN0imghl9Nu/nKj8FvXu2x1TVpiuibncUqv8jOwVE1fDX8Ymr4XbVxLnzfV5YUNfyUGn4BNfxq2lufpsIYvE7mbKLnG1LDV8OfSA0/Or2rVcpRwz9HDb+AGn41ba1PU2kMvl8nM7/o+YbWG/4Cp4ZfiVdMu9jwNQ+/AUGnPp+VN1/N72rt18nML3q+oSWOccOZxwpX0oxA8/BbpoZfTfl8Ad7VOj8nsx3R8yVeHSxxBw9M/GSstnSx4WcZwzez64H/BCwAX3H3ezc9/hng3wKngBPAv3H3H4x7To3hR6UZNdKsvo/nhxrDN7MF4D7gQ8BVwMfN7KpNu/01sOzuvww8CvzerK87ihp+NeXWpwnyrtbuncxYoudLDbOOWkkzgi42/Bxj+FcDh919FcDMHgJuAr67voO7fzPZ/yng1gyvex6N4VdTZn2aonH4RtanGR82FuXLZ5h11EqaEfR1DP8y4JXk/pHh14p8CvjqqAfMbL+ZrZjZyokTJ6YOooY/jenWpwnzrtaYJ/Mc5ctnU8Mv+uzbtvS14ZdmZrcCy8A1ox5390PAIVgbw5/2+dXwp7HW4h3jyjPf4Xu8p53VJacV82Seo3z5DLNO+uzbtvS14R8FLk/u7xx+bQMzuw64G7jR3d/I8LrnUcNfN2fr06SiN1TlyyfJOu6zb9vSxYY/8ywdM9sCfB+4lrUL/TPALe7+fLLPr7D2j7XXu/tLZZ5Xs3Rmpdk0Mn/6OGMn1Cwddz8F3Al8DXgBeMTdnzeze8zsxuFuXwTeDvyFmT1nZo/N+rqj9K/h92R9mlT0rMqXz4iskWbs9LLh10UNvyytTyP9oYY/mdbSaVm1JtDgu1o73vpCUb581PC7PUunbvM1S6fBz2rt4MyNsJQvnxFZI83J7+ssnTC62fADvKu1460vFOXLZ0zDjzAnv4sNX2P4IWgcXqSs3+J+DrGfMwyIMie/ThrDLxC54Z97J2vDq0uWDdgV0bMqXz4FWaPMyVfDz2j+Gr5rPrxIRn2ZsaOGX6Cdhj9+Ns0Cp1gcvMFtPNjO6pJlzEHrC0P58pmQte0ZO2r4GXWr4etdrSJNU8MfTQ2/Nl5iNs0DHB/8fFOBqpmj1tc65ctHDV8Nf5y2Gv4iJznJP23+hUV6TA1/NDX87NbH599iJ3/Hy+wev3v0VhU9Xyp6VuXLp2TDb2tOfhcbfoe++5Pt2bP2zrT6f2lZX0t+wIf5H+zg1fG7R393Y/R8qehZlS+fCVnX18l3BrSxTn6T77TNZa4u+M00fE/Wki85yyZ6q4qeLxU9q/LlUyJrm3Pyu9jwNYZfkcbtReKY5/F8jeEXqLfhTzlun4reqqLnS0XPqnz5TJG1jRk7XWz4Wi2ztCnH7VPRx02j50tFz6p8+UyRtY1VNLVaZsvyN/xRnwE7h+vNR8+Xip5V+fJRw1fDHyd/w19r9QP87Nrz1/LN6Z8mequKni8VPavy5aOGr4Y/Tt6GX2E2TpHorSp6vlT0rMqXjxq+Gv44eRu+8Y9cyA08PvtsnOitKnq+VPSsypePGr4a/jh5Gv4Ms3GKRG9V0fOlomdVvnzU8NXwx8nT8GeYjVMkequKni8VPavy5aOGr4Y/zuwNP+O4fSp6q4qeLxU9q/Llo4avhj/O7A0/47h9Knqrip4vFT2r8uWjhq+GP071hl/DuH0qequKni8VPavy5aOGr4Y/TvWGX8O4fSp6q4qeLxU9q/Llo4avhj9OtYZf07h9Knqrip4vFT2r8uWjhq+GP061hl/TuH0qequKni8VPavy5aOGr4Y/znQNv+Zx+1T0VhU9Xyp6VuXLRw1fDX+c6Rp+zeP2qeitKnq+VPSsypePGr4a/jjlG34D4/ap6K0qer5U9KzKl0+Fht/k59t2seFniWxm15vZi2Z22MzuGvH4BWb28PDxp81sV47X3az8Z9qeG7e/jzv5r3ykjjjnRG9V0fOlomdVvnymyNrG59v28jNtzWwBuA/4EHAV8HEzu2rTbp8CXnP3dwN/CHxh1tcdZXLDb3DcPhW9VUXPl4qeVfnymTJr059v28WGn2MM/2rgsLuvApjZQ8BNwHeTfW4C/sNw+1Hgy2ZmnvkDdSeP4Tc4bp+K3qqi50tFz6p8+UyZdf039aY+37avY/iXAa8k948MvzZyH3c/BfwEeNfmJzKz/Wa2YmYrJ06cmDrI+Ibf8Lh9Knqrip4vFT2r8uVTMWtTM3b62vCzcfdDwCGA5eXlqdv/+IbfwHz7ItFbVfR8qehZlS+filmbmrHT14Z/FLg8ub9z+LWR+5jZFuDngB9neO0NRjf8lsbtU9FbVfR8qehZlS8fNfyQs3SeAfaa2W4z2wrcDDy2aZ/HgNuG2x8B/ir3+D0UzdJpadw+Fb1VRc+Xip5V+fKZoeEf4YrhjJ369HKWznBM/k7ga8ALwCPu/ryZ3WNmNw53+xPgXWZ2GPgMcN7UzRzOb/gtjtunoreq6PlS0bMqXz5q+NkbvtVQtLNYXl72lZWVqf7Otm2jx/AXOdn8uL2ItKKpWTpNWVycbgzfzJ519+VRj3Xox/1k6w1/wU4BLY/bp6K3quj5UtGzKl8+avghx/DDWNqzjRuO/BHuxiIn2x23T0UfN42eLxU9q/LlozH8eGP4oayu8urOfdxhh3iK97c7bp+K3qqi50tFz6p8+ajhawx/rKJBfBHpDY3h92QMf4YPta1X9FYVPV8qelbly0cNf77faTuz6h9qW6/o46bR86WiZ1W+fPRO25DvtI1DDb+a6PlS0bMqXz5q+Gr4Y6nhVxM9Xyp6VuXLRw1fDX8sNfxqoudLRc+qfPmo4avhj6WGX030fKnoWZUvnwpZj7GDnRzhDAs1BNpIDb9tavjVRM+Xip5V+fKpkPUgv4sDb+OnavgjqOE3IXqrip4vFT2r8uUzRdbNc+//kQvrSLSBGn7b1PCriZ4vFT2r8uUzRdZV9nAL/5ktvDX8iqPPtD2fGn4Toreq6PlS0bMqXz4ls44et6+/9Knht00Nv5ro+VLRsypfPiWzNjlun1LDb9uw4R9jBzfzEA/zsfZXyoT4rSp6vlT0rMqXz4SsbYzbp9Tw2zZs+Af5HN/mX3APn2s70ZrorSp6vlT0rMqXz4SsbYzbp7rY8OdqtcyixTIHnOYol8Vo+yKSxbytillEq2UWWF2FW3Z+iy12GoAtvMVeXsSh3bYfvVVFz5eKnlX58inR8Jt4R20RNfyMcjb8lNq+SPc1+Y7atqnhFzh/ks65Mb1W2370VhU9Xyp6VuXLZ0zWtmbmpNTwM6qr4afW275jsWb1iMhIfRm3T6nhFxg/Db+47R/kd+ud1RO9VUXPl4qeVfnyGZG17Zk5KTX8jJpo+EXU/EXi6dO4fUoNv0D5N9qeawXGmbNjgLU1/+itKnq+VPSsypfPpqwRxu1TavgZtdnwi2iGj0jz+jhun1LDLzD7Ujrlmv/Uoreq6PlS0bMqXz7DrJHG7VNq+BlFbPgpjfOLNEMNXw1/pPoWy5xxhk/0VhU9Xyp6VuXLJ2n4bb6jtogafkbRG34RNX+RfPo6Myelhl+gmeXwp53h8/n4rSp6vlT0rMqXz2DAQT4XamZOqncN38wuBh4GdgF/C3zU3V/btM/7gD8CLgJOA//R3R+e9NxdbfhFLuB1/jlPq/GLlND3cftUpIZ/F/CEu+8Fnhje3+xnwCfd/ZeA64Evmdk7Znzdkdr9wKuC5m+n+cTgv3AzD8Vaoz/VsdYXmvJlscovcsvgobMr30aZmZPqYsPH3SvfgBeBpeH2EvBiib/zf4C9k/bbt2+fT2tx0X3tc2Li3wac8mNs9x+ywz/Ak36M7e2H0k23lm/r/z9cwMm2o4S5LS5Odx0EVoquq7P+jNru7seG28eB7eN2NrOrga3A3xQ8vt/MVsxs5cSJE1OHifqRttsGr7NQsEZ/7ev4lNGR1gfEz6p8M1n//+FjPMzOwVEsWKtPdbHhTxzDN7NvADtGPHQ38KC7vyPZ9zV3f2fB8ywBTwK3uftTk4LN2xh+WZrlI32kMftijY7hu/t17v7eEbe/BF4dXsjXL+g/KghwEfA4cHeZi31VURv+xibgw1tD6/hMHzC26FmVr5LN76Zd4DTbBrHb21w2/PGB7IvAj939XjO7C7jY3T+7aZ+twFeB/+7uXyr73H1t+EXU/GVeaa79eJFm6dwLfNDMXgKuG97HzJbN7CvDfT4KfAC43cyeG97eN+PrjtSNhl+kxeYftPWNFD2r8k2taBXMgFE36F3Dr5Mafjlq/tJVGrcvJ1LDD6XbDb9IA80/epVKRc+qfKVNWgUzUNSR1PAzUsOfjZq/RKeGX44afoH5bPhFMjb/6FUqFT2r8pU2aRXMQFFHUsPPSA2/Hmr+EoFm5pSnhl+gXw2/yLTN//Pxq1QqelblK6XM59MGiVpIDT8jNfxmpat5qv1LXTRuPz01/AJq+OMUNP8Rq3mGWN+nSIyTWUz5xprm82n7firV8CdQw6+Hxv0lB43bV6OGX0ANv5p0Nc8w6/sUiX4yla9QmXH7VN9PpRr+BGr4zVLzlzI0bj8bNfwCavjVFOfTyp5TU77zTDNun+r7qVTDn0ANPwY1f0mp4c9GDb+AGn410+fTyp6FlO88k95RW6Tvp1INfwI1/NjU/PtHM3Nmp4ZfQA2/mnz5tLKn8m007cycVN9PpRr+BGr43aTmP380bp+PGn4BNfxq6s/Xo5U9lQ+oPjMn1fdTqYY/gRr+fFHz7yaN2+elhl9ADb+a9vLN4cqeyjfTuH2q76dSDX8CNfx+0MqeMWncvh5q+AXU8KuJl6/DK3vGO5kb1Zgvx7h9qsenElDDn0gNXzTu3w6N29dHDb+AGn410fOlwq/sGf1k1pQv17h9qqen8iw1/AnU8KWImn89NG5fPzX8Amr41UTPlwq/smf0k5k5X+5x+1TPTuV51PAnUMOXaan5z0YNv35q+AXU8KuJni8VfmXP6CezhoZfZSXMMnp2Ks+jhj+BGr7kouY/mWbmNEMNv4AafjXR86XCr+wZ/WRmzFfHzJxUj07lSGr4E6jhS9363vzV6punhl9ADb+a6PlS4Vf2jH4yZ8y33ur38v1aZuak5vxUTlRHw8fdK9+Ai4GvAy8N/3znmH0vAo4AXy7z3Pv27fNpLS66g266NX8bcMqPsd0d/Ifs8A/w5Nn7Xbylx/BDdviAU21H6u1tcXG66yCwUnRdnfVn1F3AE+6+F3hieL/IQeBbM77eWGr41UTPl4q9sufnYTDgIJ+Ltb5PquQJTH+DabLVp6L/d9nHhv8isDTcXgJeLNhvH/AQcDtq+Lr17Lbe/qM0/83tXU0+9i1nw98y48+L7e5+bLh9HNi+eQczGwC/D9wKXDfuycxsP7Af4Iorrpg6zOoqXH01HD26dqqiGAzgTP5JDNlEz5eKl3X9PzRjgdNcPjjKK34Zp32BLbzFblY5zLvPtv311nw/n24n7mDAwTMbfwNJt521Jv8yuznFPzn7m8zaYMC5Y20oarDv9UZ15zODyy6DZ57J+KQlWvw3gO+MuN0E/L9N+7424u/fCXx2uH07avi66eYwuvnXuX0BJ1s/Zt2mvzXa8N29sJWb2atmtuTux8xsCfjRiN1+Dfh1MzsAvB3Yamb/4O7jxvsrUcOvJnq+VPSsG/Ot/0doG5ryuOZf5/at/DmnBhfwiP8rTvnChkxtNvki3fpe51dHw59pHr6ZfRH4sbvfa2Z3ARe7+2fH7H87sOzud056bs3DFxGJNQ//XuCDZvYSa+Pz9w5fcNnMvjLjc09Ns3SqiZ4vFT1ruXwOI2b81L29wGm2DbrTiObje12d3mk7gRq+iMybSA0/FDX8aqLnS0XPqnz5RM/axYY/67TMUPbsidnwI//DE8TPl4qeVfnyiZ617nzucOQI7N49XcMfJ/jP0Omo4VcTPV8qelblyyd6VjX8lqnhVxM9Xyp6VuXLJ3pWNfyWqeFXEz1fKnpW5csnelY1/Jap4VcTPV8qelblyyd6VjX8lqnhVxM9Xyp6VuXLJ3pWNfyWqeFXEz1fKnpW5csnelY1/Jap4VcTPV8qelblyyd6VjX8lqnhVxM9Xyp6VuXLJ3pWNfyWqeFXEz1fKnpW5csnelY1/Jap4VcTPV8qelblyyd6VjX8lqnhVxM9Xyp6VuXLJ3pWNfyWqeFXEz1fKnpW5csnelY1/JatN/yFhbaTnLOwsNYEImVKRc+Xip5V+fKJnrWJfAsL+Rv+XF3wl5bghhvWfjIuLp77epvb7nDllbEydSlfl7IqX3+yNpHPHT78Ydixg3zKfKB4G7cqH2Lu7v4bv+F+4ID7c8+57969dmtz+8AB96WlWJm6lK9LWZWvP1mbyHfgwNr1bFqM+RBz8zn6xCsRkb7rzSdeiYhIMV3wRUR6Qhd8EZGe0AVfRKQndMEXEekJXfBFRHoi7LRMMzsB/KDtHBVcAvx92yEapmPuBx1zN/yCu1866oGwF/yuMrOVojmw80rH3A865u7TkI6ISE/ogi8i0hO64Od3qO0ALdAx94OOueM0hi8i0hNq+CIiPaELvohIT+iCPyMzu9jMvm5mLw3/fOeYfS8ysyNm9uUmM+ZW5pjN7H1m9r/N7Hkz+79m9rE2ss7KzK43sxfN7LCZ3TXi8QvM7OHh40+b2a4WYmZT4ng/Y2bfHX5PnzCzX2gjZ06TjjnZ7zfNzM2ss9M0dcGf3V3AE+6+F3hieL/IQeBbjaSqV5lj/hnwSXf/JeB64Etm9o7mIs7OzBaA+4APAVcBHzezqzbt9ingNXd/N/CHwBeaTZlPyeP9a2DZ3X8ZeBT4vWZT5lXymDGzC4HfBp5uNmFeuuDP7ibgweH2g8C/HLWTme0DtgP/q5lYtZp4zO7+fXd/abj9Q+BHwMh3/wV2NXDY3Vfd/U3gIdaOPZWei0eBa83MGsyY08TjdfdvuvvPhnefAnY2nDG3Mt9jWCtrXwBebzJcbrrgz267ux8bbh9n7aK+gZkNgN8HfqfJYDWaeMwpM7sa2Ar8Td3BMrsMeCW5f2T4tZH7uPsp4CfAuxpJl1+Z4019CvhqrYnqN/GYzexXgcvd/fEmg9VhS9sBusDMvgGM+ijhu9M77u5mNmqe6wHgf7r7ka6UvwzHvP48S8CfAbe5+5m8KaUtZnYrsAxc03aWOg3L2h8At7ccJQtd8Etw9+uKHjOzV81syd2PDS9uPxqx268Bv25mB4C3A1vN7B/cfdx4f6syHDNmdhHwOHC3uz9VU9Q6HQUuT+7vHH5t1D5HzGwL8HPAj5uJl12Z48XMrmPtB/817v5GQ9nqMumYLwTeCzw5LGs7gMfM7EZ379yHbmtIZ3aPAbcNt28D/nLzDu7+CXe/wt13sTas86eRL/YlTDxmM9sK/DfWjvXRBrPl9Ayw18x2D4/nZtaOPZWei48Af+XdfTfjxOM1s18BHgBudPeRP+g7Zuwxu/tP3P0Sd981/P/3KdaOvXMXe9AFP4d7gQ+a2UvAdcP7mNmymX2l1WT1KXPMHwU+ANxuZs8Nb+9rJW1FwzH5O4GvAS8Aj7j782Z2j5ndONztT4B3mdlh4DOMn6UVWsnj/SJrv6X+xfB7uvkHYKeUPOa5oaUVRER6Qg1fRKQndMEXEekJXfBFRHpCF3wRkZ7QBV9EpCd0wRcR6Qld8EVEeuL/AykTGnEj5PYQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_grid[pred_label_grid == 1, 0], X_grid[pred_label_grid == 1, 1], \"*r\")\n",
    "plt.plot(X_grid[pred_label_grid == 0, 0], X_grid[pred_label_grid == 0, 1], \"*b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0068293b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x230acd7e0a0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAASc0lEQVR4nO3df4hd5Z3H8c8nkTQJmlbNoMEkHVmzf8TS2npJt3+0Fhoh/pM0bbeNWhpZl7QGoeBmMSD0j8iC1vTHQhNpsAupUIwNaQxosZpWitCIYytls2KTHTcYOyaxa4WSaIh+9497xr3O3Dv3zpxzz8/3C4a5597DnOckzGee85zv8xxHhAAA9Tev6AYAAPJB4ANAQxD4ANAQBD4ANASBDwANcVHRDehl6dKlMTo6WnQzAKBSXnjhhTciYqTbZ6UN/NHRUY2NjRXdDACoFNsnen3GkA4ANASBDwANQeADQEMQ+ADQEAQ+ADQEgQ8gExMT0g03SK+/XnRL0AuBDyAT994rPfustGNH0S1BLwQ+gFQWLZJs6cEHpffea3+32++jXAh8AKmMj0u33CItXtzeXrxYuvVW6ZVXim0XpiPwAaSybJm0ZIn09tvSwoXt70uWSFdeWXTLMBWBDyC1U6ekb31LOnKk/Z0bt+VU2rV0AFTHgQP//3rXruLagZnRwwcKQAkjikDgAwWghBFFIPCBHFHCiCJlEvi219l+2fZx29tn2O/LtsN2K4vjAlVDCSOKlDrwbc+XtEvSTZJWS7rZ9uou+10i6duSnkt7TKCqKGFEkbLo4a+RdDwixiPivKRHJG3ost+9ku6X9HYGxwQqixJGFCWLssyrJL3asX1S0qc7d7D9KUkrIuJx2//a6wfZ3iJpiyStXLkyg6YB5UMJI4oy9Ju2tudJ+r6kf+m3b0TsiYhWRLRGRro+gxcAMEdZBP5rklZ0bC9P3pt0iaSPSXrG9v9I+gdJh7hxCwD5yiLwn5e0yvbVthdI2iTp0OSHEfFWRCyNiNGIGJV0RNL6iBjL4NgAgAGlDvyIuCDpTklPSnpJ0qMRcdT2Dtvr0/58AEA2MllLJyKekPTElPe+02Pfz2dxTADA7DDTFgAagsAHgIYg8AGgIQh8NMK05YhZnxgNROCjEaYtR8z6xGggR0TRbeiq1WrF2Bil+khn0aL2AmVTLdQ5ndPijjcWSufO5dcwYEhsvxARXSe20sPHnFVhVGTacsSL3tOto8/qlUXXJm+wPjGag8DHnFVhVGTacsTvzNOSxe/qyndOsD4xGofAx6xV7alN05YjfnMB6xOjkRjDx6xNTEjbtkkHD0pnz7ZHRTZulHbupKMMFI0xfGSKpzYB1UTgY054ahMwe0UXOmSyeBqah6c2AbPXWeiwe3f+x6eHDwBDVpZCBwIfAIZs2nyQgqZ/EPgAMGRlKXQg8AEgB2UodOCmLQDkoAyFDvTwAaAhCHwAaAgCHwAagsAHgIYg8AGgIQj8iit6bQ4A1UHgV1wVHkICoBwI/Ioqy9ocAKqDwK+osqzNAaA6CPyKKsvaHChI2W7elK096IrAr7AyrM2BgpTt5k3Z2oOueKYtijMxIW3aJO3bx6XJoBYtal/OTbVwoXTuHO0Bz7RFSdErnL2y3bwpW3swIwIf+aPEaO7KdvOmbO3BjAh85I9eYTplu3lTtvagJ9bDR/7oFaZThoXVO5WtPeiJHj6KQa8QyF0mPXzb6yT9u6T5kh6KiPumfH6XpH+WdEHSGUn/FBEnsjg2KiqvXiGVQMD7Uvfwbc+XtEvSTZJWS7rZ9uopu/1BUisiPi5pv6Tvpj0uMBAqgYD3ZTGks0bS8YgYj4jzkh6RtKFzh4j4TUScTTaPSFqewXGB3tJUAjFrFDWVReBfJenVju2TyXu93C7pl90+sL3F9pjtsTNnzmTQNDRWmkogrgpQU7netLX9dUktSQ90+zwi9kREKyJaIyMjeTYNdTOXSiDmB6Dmsgj81ySt6Nhenrz3AbbXSrpH0vqIeCeD4wIzm20lEPMDUHNZVOk8L2mV7avVDvpNkm7p3MH2JyX9WNK6iDidwTGB/mZbCcT8ANRc6h5+RFyQdKekJyW9JOnRiDhqe4ft9cluD0i6WNLPbb9o+1Da4wJDwfwA1BirZQJAjbBaJgCAwAeApiDwAWSDCWulR+ADyAYT1kqPwAeQDhPWKoPAB5AOE9Yqg8AHkA4T1iqDwAeQHhPWKoFHHAJIj8ccVgI9fKAsKGvEkBH4QFlQ1oghI/CBolHWiJwQ+EDRKGtETgh8oGiUNSInBD7QTd43UClrRA4oywS66byBunv38I9HWSNyQA8f6MQNVNQYgQ904gYqaozABzpxAxU1RuADU3EDFTXFTVtgKm6goqbo4QNAQxD4ANAQBD4ANASBDwANQeADQEMQ+ADQEAQ+ADQEgQ8AJTLMhVoJfAAokWE+6ZLAB4ASyGOhVgIf+cn7oSJAheSxUCuBj/wM81oVqLg8Fmol8DF8PFQEGMiwF2rNJPBtr7P9su3jtrd3+fxDtvclnz9nezSL46IieKgIMJADB9oLtH7iE+3vnQu3ZiF14NueL2mXpJskrZZ0s+3VU3a7XdKbEXGNpB9Iuj/tcVEhPFQEKIUsevhrJB2PiPGIOC/pEUkbpuyzQdLe5PV+SV+w7QyOjargoSJA4bJ4AMpVkl7t2D4p6dO99omIC7bfknS5pDcyOD6qgIeKoCYmJqRNm6R9+6p3kVqqm7a2t9gesz125syZopsDANNUudgsi8B/TdKKju3lyXtd97F9kaQPS/rL1B8UEXsiohURrZGRkQyahlxQX48GqEOxWRaB/7ykVbavtr1A0iZJh6bsc0jS5uT1VyT9OiIig2OjDKrc5QEGVIdis9Rj+MmY/J2SnpQ0X9J/RMRR2zskjUXEIUk/kfSw7eOS/lftPwqoukWL2hU3kx58sP21cKF07lxx7QKGoA7FZpmM4UfEExHx9xHxdxHxb8l730nCXhHxdkT8Y0RcExFrImI8i+OiYIN2eRjyQU1UvdgsiyodNNWgXZ7OIZ/du4tpK5CBqheblapKBxU0U5enDne5gBqhh490ZuryjI9L27ZJBw9KZ8+2h3w2bpR27sy1iQDa6OFjeOpwlwuoEQIfw1X1u1xAjTCkg+Gq+l0uVF+V10LIGD18APXGxMD3EfgA6okqsWkIfAD1VIe1EDJG4AOojtnM2qZKbBoCH0B1zHY8niqxD3BZF61stVoxNjZWdDMAlMHUhfomsVDfNLZfiIhWt8/o4QMoP8bjM0HgAyg/xuMzQeADqAbG41Njpi2AamDWdmr08AGgIQh8AGgIAh8AGoLAB4CGIPABoCEIfACDm81aNigdAh/A4FhbvtIIfAD9sbZ8LRD4mD0u65uHtWxqgcDH7HFZ3zysZVMLBD4Gx2V9s7GWTeWxlg4GNz4ubdsmHTwonT3bvqzfuFHaubPoliEPrGVTefTwMTgu6/PH/RJkiMDH7HBZny/ulyBDPOIQKCMe6Yc54hGHwFwVNaRCGSSGgMAH48QzKWpIhfslGAICH4wTd1OGElTulyBjjOE3GePEvU1M9C5BpZeNEhvaGL7ty2w/ZftY8v3SLvtcZ/t3to/a/qPtr6U5JjLEOHFvDKmghtIO6WyXdDgiVkk6nGxPdVbSNyLiWknrJP3Q9kdSHhdZINRmxpAKaibtTNsNkj6fvN4r6RlJd3fuEBF/6nj9Z9unJY1I+mvKYyMLk6G2ZYu0Z097KANtzCxFzaQaw7f914j4SPLakt6c3O6x/xq1/zBcGxHvdfl8i6QtkrRy5crrT5w4Mee2AUATzTSG37eHb/tpSd2u8e/p3IiIsN3zr4ftZZIelrS5W9gnP2OPpD1S+6Ztv7YBAAbXN/AjYm2vz2yfsr0sIiaSQD/dY78lkh6XdE9EHJlza4EmmJiQNm2S9u3jfgoylfam7SFJm5PXmyU9NnUH2wsk/ULSTyNif8rjAdWQZjIb8yIwJGkD/z5JN9o+Jmltsi3bLdsPJft8VdLnJN1m+8Xk67qUxwXKbS6hXYbJXqg1Jl4BWUozmY3JXsgAi6cBeUkzmW2QeRGse4QUCHwgS2kns/Wb7DU5VHT33QQ/Zo0hHSBrX/pSO/g7J7N1TuKai15DRfPnSxcupPvZqJVUdfgAZmkYM3Qnnyf8s5998P13323f2GXBOwyAIR2gCiaHimxpXsevLQveYRYIfKAqTp2S7rijPWQktYOfBe8wCwQ+UBUHDrSHiN59V9q6Vfr971nFE7PCGD5QNaziiTmihw8ADVHLwGduCgBMV8vAZ+0p5IKeBSqmVoHP2lPIFT0LVEytAp9nciMX9CxQUbUKfJ7JjVzQs0BF1Srwpf5rTwGp0bNARdWuDp8SZeRismfRuUAaUHK1C3zMjMelZoSeBSqodkM6mBmFJUBzEfgNQWEJhokpCdVA4DcEhSUYJq4cq6Fxgd/UngiFJRjEbH8/uHKslsYFfpN7IpSsop/Z/n5w5VgtjXmmba9HgvJkOCDd78cdd7QrUxcskM6fl775TWn37uG0E/3N9EzbxvTw6YkAvaX5/eDKsToaU4fPGDbQW5rfD6YkVEdjevgSPRFgJvx+1F9jxvABoAkYw0fpNbVcFsgTgY9SaHK5LJAXAh+FYuIOkB8CH4WiXBbID4GPQlEuC+SHwEfhKAcE8tGYiVcoLybuAPmghw+UAGWpyEOqwLd9me2nbB9Lvl86w75LbJ+0/aM0xwTqiLJU5CFtD3+7pMMRsUrS4WS7l3sl/Tbl8YBaoSwVeUob+Bsk7U1e75X0xW472b5e0hWSfpXyeECtUJaKPKUN/CsiYiJ5/braof4BtudJ+p6kbf1+mO0ttsdsj505cyZl04DyoywVeeob+Laftv2fXb42dO4X7VXYuq3EtlXSExFxst+xImJPRLQiojUyMjLwSQBVRlkq8tK3LDMi1vb6zPYp28siYsL2Mkmnu+z2GUmftb1V0sWSFtj+W0TMNN4PNAZlqchL2jr8Q5I2S7ov+f7Y1B0i4tbJ17Zvk9Qi7AEgf2nH8O+TdKPtY5LWJtuy3bL9UNrGAQCywwNQAKBGeAAKAIDAB4CmIPABoCEIfABoCAIfqDlW4sQkAh+oOVbixCQCH6gpVuLEVAQ+UFOsxImpCHygpliJE1MR+ECNsRInOvEQc6DGWIkTnejhA0BDEPgA0BAEPgA0BIEPAA1B4ANAQxD4ANAQpX3ile0zkk4U3Y4BLZX0RtGNKADn3RxNPGepmuf90YgY6fZBaQO/SmyP9XqkWJ1x3s3RxHOW6nfeDOkAQEMQ+ADQEAR+NvYU3YCCcN7N0cRzlmp23ozhA0BD0MMHgIYg8AGgIQj8ObB9me2nbB9Lvl86w75LbJ+0/aM82zgMg5y37ets/872Udt/tP21Itqalu11tl+2fdz29i6ff8j2vuTz52yPFtDMzA1w3nfZ/q/k//aw7Y8W0c6s9Tvvjv2+bDtsV7JUk8Cfm+2SDkfEKkmHk+1e7pX021xaNXyDnPdZSd+IiGslrZP0Q9sfya+J6dmeL2mXpJskrZZ0s+3VU3a7XdKbEXGNpB9Iuj/fVmZvwPP+g6RWRHxc0n5J3823ldkb8Lxl+xJJ35b0XL4tzA6BPzcbJO1NXu+V9MVuO9m+XtIVkn6VT7OGru95R8SfIuJY8vrPkk5L6jrrr8TWSDoeEeMRcV7SI2qfe6fOf4v9kr5g2zm2cRj6nndE/CYiziabRyQtz7mNwzDI/7fU7rzdL+ntPBuXJQJ/bq6IiInk9etqh/oH2J4n6XuStuXZsCHre96dbK+RtEDSfw+7YRm7StKrHdsnk/e67hMRFyS9JenyXFo3PIOcd6fbJf1yqC3KR9/ztv0pSSsi4vE8G5Y1HnHYg+2nJXV73PM9nRsREba71bZulfRERJysUscvg/Oe/DnLJD0saXNEvJdtK1E021+X1JJ0Q9FtGbak8/Z9SbcV3JTUCPweImJtr89sn7K9LCImkmA73WW3z0j6rO2tki6WtMD23yJipvH+wmVw3rK9RNLjku6JiCNDauowvSZpRcf28uS9bvuctH2RpA9L+ks+zRuaQc5btteq3QG4ISLeyaltw9TvvC+R9DFJzySdtyslHbK9PiLGcmtlBhjSmZtDkjYnrzdLemzqDhFxa0SsjIhRtYd1flr2sB9A3/O2vUDSL9Q+3/05ti1Lz0taZfvq5Hw2qX3unTr/Lb4i6ddR/VmMfc/b9icl/VjS+ojo+ge/gmY874h4KyKWRsRo8vt8RO3zr1TYSwT+XN0n6UbbxyStTbZlu2X7oUJbNlyDnPdXJX1O0m22X0y+riuktXOUjMnfKelJSS9JejQijtreYXt9sttPJF1u+7ikuzRzpVYlDHjeD6h9xfrz5P926h/CyhnwvGuBpRUAoCHo4QNAQxD4ANAQBD4ANASBDwANQeADQEMQ+ADQEAQ+ADTE/wGmXI5iMI7Y+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], \"*r\")\n",
    "plt.plot(X_test[Y_test == 0, 0], X_test[Y_test == 0, 1], \"*b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278d8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
