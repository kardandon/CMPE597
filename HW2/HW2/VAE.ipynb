{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBg0p_iBRm0d"
   },
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "EPOCHS = 50\n",
    "\n",
    "FOLDER = \"./Data/\"\n",
    "\n",
    "MODEL_DIR = FOLDER +\"Vae_model_dict_50EPOCHS_v2\"\n",
    "DIGIT_SAVE_DIR = FOLDER + 'sample_digits.png'\n",
    "z_dim = 4\n",
    "RANDOM_VECTOR = FOLDER + \"random_vector.txt\"\n",
    "\n",
    "TRAIN_LOSS = FOLDER + \"TRAIN_LOSS_VAE.txt.npy\"\n",
    "TEST_LOSS = FOLDER + \"TEST_LOSS_VAE.txt.npy\"\n",
    "TRAIN_KLD = FOLDER + \"TRAIN_KLD_VAE.txt.npy\"\n",
    "TEST_KLD = FOLDER + \"TEST_KLD_VAE.txt.npy\"\n",
    "\n",
    "LOSS_PLOT = FOLDER + \"LOSS_PLOT.png\"\n",
    "KLD_PLOT = FOLDER + \"KLD_PLOT.png\"\n",
    "if not os.path.exists(RANDOM_VECTOR):\n",
    "    z = torch.randn(100, 4)\n",
    "    torch.save(z, RANDOM_VECTOR)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mdg5TLsWmJEv"
   },
   "outputs": [],
   "source": [
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, std):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + std - mu.pow(2) - std.exp())\n",
    "    return BCE + KLD, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMDb6vhQRror"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, loss_function):\n",
    "        super(VAE, self).__init__()\n",
    "        self.loss_function = loss_function\n",
    "        self.zdim = z_dim\n",
    "        # encoder part\n",
    "        self.lstm1 = nn.LSTM(input_size=x_dim, hidden_size=h_dim1)\n",
    "        self.fc31 = nn.Linear(h_dim1, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim1, z_dim)\n",
    "        # decoder part\n",
    "        self.tconv2d1 = nn.ConvTranspose2d(1, 32, 8)\n",
    "        self.tconv2d2 = nn.ConvTranspose2d(32, 16, 4)\n",
    "        self.tconv2d3 = nn.ConvTranspose2d(16, 8, 2)\n",
    "        self.fc2 = nn.Linear(h_dim2, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h, hidden = self.lstm1(x)\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = torch.reshape(z, (-1, 1, int(math.sqrt(self.zdim)), int(math.sqrt(self.zdim)),))\n",
    "        h = F.relu(self.tconv2d1(h))\n",
    "        #h = torch.nn.functional.dropout(h, p=0.8)\n",
    "        h = F.relu(self.tconv2d2(h))\n",
    "        #h = torch.nn.functional.dropout(h, p=0.8)\n",
    "        h = F.relu(self.tconv2d3(h))\n",
    "        #h = torch.nn.functional.dropout(h, p=0.8)\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "        return F.sigmoid(self.fc2(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "    \n",
    "    def train_data(self, epoch, train_loader):\n",
    "        self.train()\n",
    "        train_loss = 0\n",
    "        train_kld = 0\n",
    "        loss_history = []\n",
    "        KLD_history = []\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon_batch, mu, log_var = vae(data)\n",
    "            loss, KLD= self.loss_function(recon_batch, data, mu, log_var)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            train_kld += KLD.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "        \n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "        return train_loss / len(train_loader.dataset), train_kld / len(train_loader.dataset)\n",
    "    \n",
    "    def test_data(self, test_loader):\n",
    "        self.eval()\n",
    "        test_loss= 0\n",
    "        kld_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.to(device)\n",
    "                recon, mu, log_var = vae(data)\n",
    "                \n",
    "                # sum up batch loss\n",
    "                loss, kld = self.loss_function(recon, data, mu, log_var)\n",
    "                test_loss += loss.item()\n",
    "                kld_loss += kld.item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        kld_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "        return test_loss, kld_loss\n",
    "\n",
    "# build model\n",
    "\n",
    "vae = VAE(x_dim=784, h_dim1= 128, h_dim2=1352, z_dim=z_dim, loss_function=loss_function)\n",
    "vae = vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMZNASxaR6-6",
    "outputId": "0fa308c3-3e74-467d-d9f5-4e34d8ec94c6"
   },
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "train_kld_history = []\n",
    "test_loss_history = []\n",
    "test_kld_history = []\n",
    "for epoch in range(EPOCHS):\n",
    "    l,k = vae.train_data(epoch, train_loader)\n",
    "    train_loss_history.append(l)\n",
    "    train_kld_history.append(k)\n",
    "    l,k = vae.test_data(test_loader)\n",
    "    test_loss_history.append(l)\n",
    "    test_kld_history.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZjXBQZ8kXwe"
   },
   "outputs": [],
   "source": [
    "np.save(TRAIN_LOSS, np.array(train_loss_history))\n",
    "np.save(TEST_LOSS, np.array(test_loss_history))\n",
    "np.save(TRAIN_KLD, np.array(train_kld_history))\n",
    "np.save(TEST_KLD, np.array(test_kld_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5euh9OOrqLRR"
   },
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnUb5i21lG3H"
   },
   "outputs": [],
   "source": [
    "train_loss_history = np.load(TRAIN_LOSS)\n",
    "test_loss_history = np.load(TEST_LOSS)\n",
    "train_kld_history = np.load(TRAIN_KLD)\n",
    "test_kld_history = np.load(TEST_KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "lXCfZeFyeKwO",
    "outputId": "052796f8-af18-4c70-d715-3f2c967213db"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(range(1,EPOCHS+1), train_loss_history)\n",
    "plt.plot(range(1,EPOCHS+1), test_loss_history)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.savefig(LOSS_PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "iwmyPlLxe4lI",
    "outputId": "a9e3af78-387f-458c-a8a2-b0b996429a5b"
   },
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.plot(range(1,EPOCHS+1), train_kld_history)\n",
    "plt.plot(range(1,EPOCHS+1), test_kld_history)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Average kld\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.savefig(KLD_PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBGUKaR_R90s",
    "outputId": "6ac5af68-5e6f-4936-9473-3d74f1b20e1b"
   },
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=784, h_dim1= 128, h_dim2=1352, z_dim=z_dim, loss_function=loss_function)\n",
    "vae = vae.to(device)\n",
    "vae.load_state_dict(torch.load(MODEL_DIR, map_location=device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.load(RANDOM_VECTOR).to(device)\n",
    "    sample = vae.decoder(z).to(device)\n",
    "    save_image(sample.view(100, 1, 28, 28), DIGIT_SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
